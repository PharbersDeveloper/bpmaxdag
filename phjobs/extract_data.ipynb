{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"extract_data\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "extract_path = \"s3a://ph-stream/common/public/max_result/0.0.5/\"\n",
    "out_path = \"s3a://ph-stream/common/public/max_result/0.0.5/extract_data_out\"\n",
    "extract_file = \"Empty\"\n",
    "time_left = \"Empty\"\n",
    "time_right = \"Empty\"\n",
    "molecule = \"Empty\"\n",
    "molecule_sep = \"Empty\"\n",
    "atc = \"Empty\"\n",
    "project = \"Empty\"\n",
    "doi = \"Empty\"\n",
    "out_suffix = \"Empty\"\n",
    "data_type = \"max\"\n",
    "market_define = \"Empty\"\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write .extract_data in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "time_left = \"202001\"\n",
    "time_right = \"202010\"\n",
    "project = \"AZ\"\n",
    "doi = \"Respules_Market, HTN_Market\"\n",
    "out_suffix = \"test4\"\n",
    "market_define = \"AZ\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. 输入\n",
    "if data_type != \"max\" and data_type != \"raw\":\n",
    "    phlogger.error('wrong input: data_type, max or raw') \n",
    "    raise ValueError('wrong input: data_type, max or raw')\n",
    "\n",
    "if extract_file == \"Empty\":\n",
    "    path_for_extract_path = extract_path + \"/path_for_extract.csv\"\n",
    "else:\n",
    "    path_for_extract_path = extract_file\n",
    "\n",
    "if data_type == 'max':\n",
    "    extract_path = extract_path + '/max_standard'\n",
    "elif data_type == 'raw':\n",
    "    extract_path = extract_path + '/rawdata_standard'\n",
    "\n",
    "if out_suffix == \"Empty\":\n",
    "    raise ValueError('out_suffix: missing')\n",
    "\n",
    "# 提数条件\n",
    "time_left = int(time_left)\n",
    "time_right = int(time_right)\n",
    "\n",
    "if molecule == \"Empty\":\n",
    "    molecule = [] \n",
    "else:\n",
    "    if molecule_sep == \"Empty\": \n",
    "        molecule = molecule.replace(\" \",\"\").split(\",\")\n",
    "    else:\n",
    "        molecule = molecule.replace(\" \",\"\").split(molecule_sep)\n",
    "\n",
    "if atc == \"Empty\":\n",
    "    atc = []\n",
    "else:\n",
    "    atc = atc.replace(\" \",\"\").split(\",\")\n",
    "\n",
    "if len(set(len(i) for i in atc)) >1 :\n",
    "    raise ValueError('atc length diff')\n",
    "\n",
    "if project == \"Empty\":\n",
    "    project = []\n",
    "else:\n",
    "    project = project.replace(\" \",\"\").split(\",\")\n",
    "\n",
    "if doi == \"Empty\":\n",
    "    doi = []\n",
    "else:\n",
    "    doi = doi.replace(\" \",\"\").split(\",\")\n",
    "    if market_define == \"Empty\":\n",
    "        raise ValueError('没有定义 market_define')\n",
    "    if project == \"Empty\":\n",
    "        raise ValueError('没有指定 project')\n",
    "        \n",
    "# project_rank 文件\n",
    "project_rank_path =  max_path + \"/Common_files/extract_data_files/project_rank.csv\"\n",
    "\n",
    "# 满足率计算输入文件\n",
    "ims_mapping_path = max_path + \"/Common_files/extract_data_files/ims_mapping_202007.csv\"\n",
    "ims_sales_path = max_path + \"/Common_files/extract_data_files/cn_IMS_Sales_Fdata_202007.csv\"\n",
    "molecule_ACT_path = max_path  + \"/Common_files/extract_data_files/product_map_all_ATC.csv\"\n",
    "packID_ACT_map_path = max_path  + \"/Common_files/extract_data_files/packID_ATC_map.csv\"\n",
    "\n",
    "# packid市场名\n",
    "market_define_file_path = \"s3a://ph-stream/common/public/max_result/0.0.5/market_define.csv\"\n",
    "\n",
    "# b. 输出\n",
    "timenow = time.strftime(\"%Y-%m-%d\", time.localtime()).replace(\"-\", \"_\")\n",
    "if data_type == 'raw':\n",
    "    outdir = \"raw_out_\" + timenow + \"_\" + out_suffix\n",
    "elif data_type == 'max':\n",
    "    outdir = \"out_\" + timenow + \"_\" + out_suffix\n",
    "out_extract_data_path = out_path + \"/\" + outdir + \"/out_\" + timenow + \"_\" + out_suffix + '.csv'\n",
    "report_a_path = out_path + \"/\" + outdir + \"/report_a.csv\"\n",
    "report_b_path = out_path + \"/\" + outdir + \"/report_ATC.csv\"\n",
    "report_c_path = out_path + \"/\" + outdir + \"/report_c.csv\"\n",
    "report_d_path = out_path + \"/\" + outdir + \"/report_molecule.csv\"\n",
    "\n",
    "out_tmp_path = out_path + \"/\" + outdir + \"/out_tmp\"\n",
    "max_filter_raw_path = out_path + \"/\" + outdir + \"/max_filter_raw_tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ 数据执行 ==================\n",
    "'''\n",
    "raw 和 max 区别：\n",
    "1. raw：Sales，Units 不处理，就用原始的数据\n",
    "2. report A 去重依据：\n",
    "    max：月份数-项目排名\n",
    "    raw: 月份数-医院数-项目排名；增加了source来源统计\n",
    "3. 提数结果\n",
    "    raw：增加了医院ID，原始医院名，PHA，PHA医院名\n",
    "'''\n",
    "\n",
    "# 一. 文件准备\n",
    "\n",
    "# 1. 满足率文件准备\n",
    "# 通用名中英文对照\n",
    "molecule_ACT = spark.read.csv(molecule_ACT_path, header=True)\n",
    "packID_ACT_map = spark.read.csv(packID_ACT_map_path, header=True)\n",
    "molecule_ACT = molecule_ACT.select(\"MOLE_NAME_EN\", \"MOLE_NAME_CH\").distinct()\n",
    "packID_ACT_map = packID_ACT_map.select(\"MOLE_NAME_EN\", \"MOLE_NAME_CH\").distinct()\n",
    "molecule_name_map = molecule_ACT.union(packID_ACT_map).distinct()\n",
    "\n",
    "# ims mapping:ATC - Molecule - Pack_Id\n",
    "ims_mapping = spark.read.csv(ims_mapping_path, header=True)\n",
    "ims_mapping = ims_mapping.select(\"Pack_Id0\", \"ATC4_Code\", \"Molecule_Composition\").distinct() \\\n",
    "                    .withColumn(\"Pack_Id0\", ims_mapping.Pack_Id0.cast(IntegerType())) \\\n",
    "                    .withColumnRenamed(\"Pack_Id0\", \"PACK_ID\") \\\n",
    "                    .withColumnRenamed(\"ATC4_Code\", \"ATC\") \\\n",
    "                    .withColumnRenamed(\"Molecule_Composition\", \"MOLE_NAME_EN\")\n",
    "\n",
    "# 2019年全国的ims销售数据\n",
    "ims_sales = spark.read.csv(ims_sales_path, header=True)\n",
    "ims_sales = ims_sales.where(func.substring(ims_sales.Period_Code, 0, 4) == '2019') \\\n",
    "                    .where(ims_sales.Geography_id == 'CHT') \\\n",
    "                    .groupby(\"Pack_ID\").agg(func.sum(\"LC\").alias(\"Sales_ims\")) \\\n",
    "                    .withColumn(\"Pack_ID\", ims_sales.Pack_ID.cast(IntegerType())) \\\n",
    "                    .withColumnRenamed(\"Pack_ID\", \"PACK_ID\")\n",
    "\n",
    "ims_sales = ims_sales.join(ims_mapping, on=\"Pack_ID\", how=\"left\")\n",
    "ims_sales = ims_sales.join(molecule_name_map, on=\"MOLE_NAME_EN\", how=\"left\").distinct()\n",
    "\n",
    "if atc and max([len(i) for i in atc]) == 3:\n",
    "    ims_sales = ims_sales.withColumn(\"ATC\", func.substring(ims_sales.ATC, 0, 3)).distinct()\n",
    "elif atc and max([len(i) for i in atc]) == 4:\n",
    "    ims_sales = ims_sales.withColumn(\"ATC\", func.substring(ims_sales.ATC, 0, 4)).distinct()\n",
    "\n",
    "# 2. 项目排名文件\n",
    "project_rank = spark.read.csv(project_rank_path, header=True)\n",
    "project_rank = project_rank.withColumnRenamed(\"项目\", \"project\") \\\n",
    "                    .withColumn(\"排名\", project_rank[\"排名\"].cast(IntegerType())) \\\n",
    "                    .withColumnRenamed(\"排名\", \"project_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Market_definition_pack_level 市场名 packid水平\n",
    "if project and doi and market_define:\n",
    "    market_define_file = spark.read.csv(market_define_file_path, header=True)\n",
    "\n",
    "    market_define_file = market_define_file.withColumn('PACK_ID', market_define_file['PACK_ID'].cast(IntegerType())) \\\n",
    "                                .select('PACK_ID', 'Market', 'market_define')\n",
    "    \n",
    "    market_packid = market_define_file.where(market_define_file.market_define == market_define) \\\n",
    "                                    .where(market_define_file.Market.isin(doi)) \\\n",
    "                                    .select('PACK_ID', 'Market').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二. 根据 max_standard_brief_all 确定最终提数来源\n",
    "\n",
    "# 1. 根据path_for_extract， 合并项目brief，生成max_standard_brief_all\n",
    "path_for_extract = spark.read.csv(path_for_extract_path, header=True)\n",
    "# 如果指定了project，那么只读取该项目的brief文件即可        \n",
    "if project:\n",
    "    project_all = project\n",
    "else:\n",
    "    project_all = path_for_extract.toPandas()[\"project\"].tolist()\n",
    "\n",
    "if data_type == 'raw':\n",
    "    path_all_brief = [extract_path + '/' + i + \"_rawdata_standard_brief\" for i in project_all]\n",
    "elif data_type == 'max':\n",
    "    path_all_brief = [extract_path + '/' + i + \"_max_standard_brief\" for i in project_all]\n",
    "\n",
    "# \"project\", \"Date\", \"标准通用名\", \"ATC\", \"DOI\"  (\"PHA\", \"Source\")\n",
    "index = 0\n",
    "for eachpath in path_all_brief:\n",
    "    df = spark.read.parquet(eachpath)\n",
    "    if 'PACK_ID' not in df.columns:\n",
    "        df = df.withColumn('PACK_ID', func.lit(0))\n",
    "    if index ==0:\n",
    "        max_standard_brief_all = df\n",
    "    else:\n",
    "        max_standard_brief_all = max_standard_brief_all.union(df)\n",
    "    index += 1 \n",
    "\n",
    "# 2. 根据提数需求获取 max_filter_path_month\n",
    "# 筛选,获取符合条件的项目和月份\n",
    "max_filter_list = max_standard_brief_all.where((max_standard_brief_all.Date >= time_left) & (max_standard_brief_all.Date <= time_right))\n",
    "if project:\n",
    "    max_filter_list = max_filter_list.where(max_filter_list.project.isin(project))\n",
    "if doi:\n",
    "    max_filter_list = max_filter_list.join(market_packid, on='PACK_ID', how='inner')\n",
    "    # max_filter_list = max_filter_list.where(max_filter_list.DOI.isin(doi))\n",
    "if atc:\n",
    "    if max([len(i) for i in atc]) == 3:\n",
    "        max_filter_list = max_filter_list.withColumn(\"ATC\", func.substring(max_filter_list.ATC, 0, 3)).distinct()\n",
    "    elif max([len(i) for i in atc]) == 4:\n",
    "        max_filter_list = max_filter_list.withColumn(\"ATC\", func.substring(max_filter_list.ATC, 0, 4)).distinct()\n",
    "    max_filter_list = max_filter_list.where(max_filter_list.ATC.isin(atc))\n",
    "if molecule:\n",
    "    max_filter_list = max_filter_list.where(max_filter_list['标准通用名'].isin(molecule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 注释项目排名\n",
    "max_filter_list = max_filter_list.join(project_rank, on=\"project\", how=\"left\").persist()\n",
    "\n",
    "# 4. 根据月份数以及项目排名进行去重，确定最终提数来源，生成报告 report_a \n",
    "report = max_filter_list.select(\"project\",\"project_score\",\"标准通用名\", \"ATC\", \"Date\") \\\n",
    "                        .distinct() \\\n",
    "                        .groupby([\"标准通用名\", \"ATC\", \"project\",\"project_score\"]).count() \\\n",
    "                        .withColumnRenamed(\"count\", \"months_num\") \\\n",
    "                        .persist()\n",
    "\n",
    "# 分子最大月份数, 月份最全-得分\n",
    "months_max = report.groupby(\"标准通用名\", \"ATC\").agg(func.max(\"months_num\").alias(\"max_month\"))\n",
    "\n",
    "report = report.join(months_max, on=[\"标准通用名\", \"ATC\"], how=\"left\")\n",
    "report = report.withColumn(\"drop_for_months\", func.when(report.months_num == report.max_month, func.lit(0)).otherwise(func.lit(1)))\n",
    "\n",
    "# 对于raw_data 医院数量作为第二去重条件\n",
    "if data_type == 'raw':\n",
    "    # 数据来源\n",
    "    Source_window = Window.partitionBy(\"project\", \"标准通用名\").orderBy(func.col('Source'))\n",
    "    rank_window = Window.partitionBy(\"project\", \"标准通用名\").orderBy(func.col('Source').desc())\n",
    "\n",
    "    Source = max_filter_list.select(\"project\", \"标准通用名\", 'Source').distinct() \\\n",
    "                            .select(\"project\", \"标准通用名\",\n",
    "                                     func.collect_list(func.col('Source')).over(Source_window).alias('Source'),\n",
    "                                     func.rank().over(rank_window).alias('rank')).persist()\n",
    "    Source = Source.where(Source.rank == 1).drop('rank')\n",
    "    Source = Source.withColumn('Source', func.concat_ws(',', func.col('Source')))\n",
    "\n",
    "    report = report.join(Source, on=[\"project\", \"标准通用名\"], how='left').persist()                                \n",
    "\n",
    "    # 医院数统计                    \n",
    "    PHA_num = max_filter_list.select(\"project\",\"project_score\",\"标准通用名\", \"ATC\", \"PHA\") \\\n",
    "                        .distinct() \\\n",
    "                        .groupby([\"标准通用名\", \"ATC\", \"project\",\"project_score\"]).count() \\\n",
    "                        .withColumnRenamed(\"count\", \"PHA_num\") \\\n",
    "                        .persist()\n",
    "    report = report.join(PHA_num, on=[\"标准通用名\", \"ATC\", \"project\", \"project_score\"], how=\"left\").persist()\n",
    "\n",
    "    # 月份相同的 计算医院数量最大的\n",
    "    PHA_num_max = report.where(report.drop_for_months == 0) \\\n",
    "                .groupby(\"标准通用名\").agg(func.max(\"PHA_num\").alias(\"max_PHA_num\"))\n",
    "    report = report.join(PHA_num_max, on=\"标准通用名\", how=\"left\")\n",
    "\n",
    "    report = report.withColumn(\"drop_for_PHA\", func.when(report.PHA_num == report.max_PHA_num, func.lit(0)).otherwise(func.lit(1)))\n",
    "    report = report.withColumn(\"drop_for_PHA\", func.when(report.drop_for_months == 0, report.drop_for_PHA).otherwise(None))\n",
    "\n",
    "    # 项目得分\n",
    "    score_max = report.where(report.drop_for_PHA == 0) \\\n",
    "                .groupby(\"标准通用名\").agg(func.min(\"project_score\").alias(\"max_score\"))\n",
    "    report = report.join(score_max, on=\"标准通用名\", how=\"left\")\n",
    "    report = report.withColumn(\"drop_for_score\", func.when(report.project_score == report.max_score, func.lit(0)).otherwise(func.lit(1)))\n",
    "    report = report.withColumn(\"drop_for_score\", func.when(report.drop_for_PHA == 0, report.drop_for_score).otherwise(None))\n",
    "elif data_type == 'max':\n",
    "    # 项目得分\n",
    "    score_max = report.where(report.drop_for_months == 0) \\\n",
    "                .groupby(\"标准通用名\").agg(func.min(\"project_score\").alias(\"max_score\"))\n",
    "    report = report.join(score_max, on=\"标准通用名\", how=\"left\")\n",
    "    report = report.withColumn(\"drop_for_score\", func.when(report.project_score == report.max_score, func.lit(0)).otherwise(func.lit(1)))\n",
    "    report = report.withColumn(\"drop_for_score\", func.when(report.drop_for_months == 0, report.drop_for_score).otherwise(None))\n",
    "\n",
    "# 时间范围range，最小月-最大月\n",
    "time_range = max_filter_list.select(\"project\",\"标准通用名\", \"ATC\", \"Date\") \\\n",
    "                .distinct() \\\n",
    "                .groupby([\"project\",\"标准通用名\", \"ATC\"]).agg(func.min(\"Date\").alias(\"min_time\"), func.max(\"Date\").alias(\"max_time\"))\n",
    "time_range = time_range.withColumn(\"time_range\", func.concat(time_range.min_time, func.lit(\"_\"), time_range.max_time))\n",
    "\n",
    "# report_a生成\n",
    "# report_a.withColumn(\"time_range\", func.lit(str(time_left) + '_' + str(time_right)))\n",
    "report_a = report.drop(\"max_score\", \"max_month\") \\\n",
    "            .join(time_range.drop(\"min_time\", \"max_time\"), on=[\"project\",\"标准通用名\", \"ATC\"], how=\"left\")\n",
    "# 列名顺序调整\n",
    "if data_type == 'raw':\n",
    "    report_a = report_a.select(\"project\", \"ATC\", \"标准通用名\", 'Source', \"time_range\", \"months_num\", \"drop_for_months\", \n",
    "                                \"PHA_num\", \"drop_for_PHA\", \"project_score\", \"drop_for_score\")\n",
    "    report_a = report_a.orderBy([\"标准通用名\", \"months_num\", \"PHA_num\", \"project_score\"], ascending=[0, 0, 0, 1])\n",
    "else:\n",
    "    report_a = report_a.select(\"project\", \"ATC\", \"标准通用名\", \"time_range\", \"months_num\", \"drop_for_months\", \"project_score\", \"drop_for_score\")\n",
    "    report_a = report_a.orderBy([\"标准通用名\", \"months_num\", \"project_score\"], ascending=[0, 0, 1])               \n",
    "\n",
    "report_a = report_a.withColumn(\"flag\", func.when(report_a.drop_for_score == 0, func.lit(1)).otherwise(func.lit(None)))\n",
    "# 输出report_a            \n",
    "report_a = report_a.repartition(1)\n",
    "report_a.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(report_a_path)\n",
    "# 重新读入，否则当数据量大的时候后面的join report_a 报错\n",
    "report_a = spark.read.csv(report_a_path, header=True)\n",
    "\n",
    "# 根据 report_a 去重\n",
    "if atc:\n",
    "    max_filter_list = max_filter_list.join(report_a.where(report_a.flag == 1).select(\"ATC\", \"标准通用名\", \"project\").distinct(), \n",
    "                                on=[\"ATC\", \"标准通用名\", \"project\"], \n",
    "                                how=\"inner\").persist()\n",
    "else:\n",
    "    max_filter_list = max_filter_list.join(report_a.where(report_a.flag == 1).select(\"标准通用名\", \"project\").distinct(), \n",
    "                                on=[\"标准通用名\", \"project\"], \n",
    "                                how=\"inner\").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三. 原始数据提取\n",
    "\n",
    "project_Date_list = max_filter_list.select(\"project\", \"Date\").distinct()\n",
    "\n",
    "# 获取要读取的文件路径\n",
    "if data_type == 'raw':\n",
    "    max_filter_path = project_Date_list.withColumn('path', func.concat(func.lit(extract_path + '/'), \n",
    "                                                    project_Date_list.project, func.lit(\"_rawdata_standard\")))\n",
    "elif data_type == 'max':\n",
    "    max_filter_path = project_Date_list.withColumn('path', func.concat(func.lit(extract_path + '/'), \n",
    "                                                    project_Date_list.project, func.lit(\"_max_standard\")))\n",
    "\n",
    "# max_filter_path = project_Date_list.join(path_for_extract, on=\"project\", how=\"left\")\n",
    "max_filter_path = max_filter_path.withColumn(\"path_month\", func.concat(max_filter_path.path, func.lit(\"/Date_copy=\"), max_filter_path.Date))\n",
    "max_filter_path_month = max_filter_path.select(\"path_month\").distinct().toPandas()[\"path_month\"].tolist()\n",
    "\n",
    "# 3. 根据 max_filter_path_month 汇总max结果        \n",
    "index = 0\n",
    "for eachpath in max_filter_path_month:\n",
    "    df = spark.read.parquet(eachpath)\n",
    "    # 过滤数据\n",
    "    if doi:\n",
    "        df = df.join(market_packid, on='PACK_ID', how='inner')\n",
    "        df = df.drop('DOI') \\\n",
    "                .withColumnRenamed('Market', 'DOI')\n",
    "        # df = df.where(df.DOI.isin(doi))\n",
    "    if atc:\n",
    "        if max([len(i) for i in atc]) == 3:\n",
    "            df = df.withColumn(\"ATC\", func.substring(df.ATC, 0, 3)).distinct()\n",
    "        elif max([len(i) for i in atc]) == 4:\n",
    "            df = df.withColumn(\"ATC\", func.substring(df.ATC, 0, 4)).distinct()\n",
    "        df = df.where(df.ATC.isin(atc))\n",
    "    if molecule:\n",
    "        df = df.where(df['标准通用名'].isin(molecule))\n",
    "\n",
    "    # 汇总\n",
    "    if index ==0:\n",
    "        # max_filter_raw = df\n",
    "        df = df.repartition(1)\n",
    "        df.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(max_filter_raw_path)\n",
    "    else:\n",
    "        # max_filter_raw = max_filter_raw.union(df)\n",
    "        df = df.repartition(1)\n",
    "        df.write.format(\"parquet\") \\\n",
    "            .mode(\"append\").save(max_filter_raw_path)\n",
    "    index += 1\n",
    "\n",
    "# max_filter_raw\n",
    "max_filter_raw = spark.read.parquet(max_filter_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 注释项目排名\n",
    "max_filter_out = max_filter_raw.join(project_rank, on=\"project\", how=\"left\").persist()\n",
    "\n",
    "# 5. 原始提数结果\n",
    "if data_type == 'max':\n",
    "    out_cols = [\"project\", \"project_score\", \"Date\", \"ATC\", \"标准通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \n",
    "            \"标准包装数量\", \"标准生产企业\", \"标准省份名称\", \"标准城市名称\", \"DOI\", \"PACK_ID\", \"Predict_Sales\", \"Predict_Unit\", \"PANEL\"]\n",
    "elif data_type == 'raw':\n",
    "    out_cols = [\"project\", \"project_score\", \"ID\", \"Raw_Hosp_Name\", \"PHA\", \"PHA医院名称\" ,\"Date\", \"ATC\", \"标准通用名\", \n",
    "                \"标准商品名\", \"标准剂型\", \"标准规格\", \"标准包装数量\", \"标准生产企业\", \"标准省份名称\", \"标准城市名称\", \n",
    "                \"DOI\", \"PACK_ID\", \"Sales\", \"Units\", \"Units_Box\"]\n",
    "max_filter_out = max_filter_out.select(out_cols).distinct()\n",
    "\n",
    "if data_type == 'max':\n",
    "    # Sales，Units 处理\n",
    "    '''\n",
    "    包装数量为空的是others， Sales 或者 Units 可以为0\n",
    "    包装数量不为空的，Sales和Units只要有一列为0，那么都调整为0；Units先四舍五入为整数，然后变化的系数乘以Sales获得新的Sales\n",
    "    Sales 保留两位小数\n",
    "    负值调整为0\n",
    "    去掉 Sales，Units 同时为0的行\n",
    "    '''\n",
    "    max_filter_out = max_filter_out.withColumn(\"Predict_Sales\", max_filter_out[\"Predict_Sales\"].cast(DoubleType())) \\\n",
    "                            .withColumn(\"Predict_Unit\", max_filter_out[\"Predict_Unit\"].cast(DoubleType()))\n",
    "\n",
    "    max_filter_out = max_filter_out.withColumn(\"Units\", func.when((~max_filter_out[\"标准包装数量\"].isNull()) & (max_filter_out.Predict_Unit <= 0), func.lit(0)) \\\n",
    "                                                                    .otherwise(func.round(max_filter_out.Predict_Unit, 0)))\n",
    "\n",
    "    max_filter_out = max_filter_out.withColumn(\"p\", max_filter_out.Units/max_filter_out.Predict_Unit)\n",
    "    max_filter_out = max_filter_out.withColumn(\"p\", func.when((~max_filter_out[\"标准包装数量\"].isNull()) & (max_filter_out[\"p\"].isNull()), func.lit(0)) \\\n",
    "                                                        .otherwise(max_filter_out.p))\n",
    "    max_filter_out = max_filter_out.withColumn(\"p\", func.when((max_filter_out[\"标准包装数量\"].isNull()) & (max_filter_out[\"p\"].isNull()), func.lit(1)) \\\n",
    "                                                        .otherwise(max_filter_out.p))\n",
    "\n",
    "    max_filter_out = max_filter_out.withColumn(\"Sales\", max_filter_out.Predict_Sales * max_filter_out.p)\n",
    "\n",
    "    max_filter_out = max_filter_out.withColumn(\"Sales\", func.round(max_filter_out.Sales, 2)) \\\n",
    "                                .withColumn(\"Units\", max_filter_out[\"Units\"].cast(IntegerType())) \\\n",
    "                                .drop(\"Predict_Unit\", \"Predict_Sales\", \"p\")\n",
    "\n",
    "    # 负值调整为0\n",
    "    max_filter_out = max_filter_out.withColumn(\"Sales\", func.when(max_filter_out.Sales < 0 , func.lit(0)).otherwise(max_filter_out.Sales))\n",
    "    max_filter_out = max_filter_out.withColumn(\"Units\", func.when(max_filter_out.Sales == 0, func.lit(0)).otherwise(max_filter_out.Units))\n",
    "\n",
    "    # 去掉 Sales，Units 同时为0的行\n",
    "    max_filter_out_1 = max_filter_out.where(max_filter_out[\"标准包装数量\"].isNull())\n",
    "    max_filter_out_2 = max_filter_out.where((~max_filter_out[\"标准包装数量\"].isNull()) & (max_filter_out.Sales != 0) & (max_filter_out.Units != 0))\n",
    "\n",
    "    max_filter_out =  max_filter_out_1.union(max_filter_out_2)   \n",
    "\n",
    "# 根据 report_a 去重\n",
    "if atc:\n",
    "    out_extract_data = max_filter_out.join(report_a.where(report_a.flag == 1).select(\"ATC\", \"标准通用名\", \"project\").distinct(), \n",
    "                                on=[\"ATC\", \"标准通用名\", \"project\"], \n",
    "                                how=\"inner\").persist()\n",
    "else:\n",
    "    out_extract_data = max_filter_out.join(report_a.where(report_a.flag == 1).select(\"标准通用名\", \"project\").distinct(), \n",
    "                                on=[\"标准通用名\", \"project\"], \n",
    "                                how=\"inner\").persist()\n",
    "\n",
    "# 输出提数结果\n",
    "if data_type == 'max':\n",
    "    if doi:\n",
    "        out_cols = [\"project\", \"project_score\", \"Date\", \"ATC\", \"标准通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \n",
    "                \"标准包装数量\", \"标准生产企业\", \"标准省份名称\", \"标准城市名称\", \"PACK_ID\", \"Sales\", \"Units\", \"PANEL\", \"DOI\"]\n",
    "    else:\n",
    "        out_cols = [\"project\", \"project_score\", \"Date\", \"ATC\", \"标准通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \n",
    "                \"标准包装数量\", \"标准生产企业\", \"标准省份名称\", \"标准城市名称\", \"PACK_ID\", \"Sales\", \"Units\", \"PANEL\"]\n",
    "elif data_type == 'raw':\n",
    "    out_cols = [\"project\", \"project_score\", \"ID\", \"Raw_Hosp_Name\", \"PHA\", \"PHA医院名称\" ,\"Date\", \"ATC\", \"标准通用名\", \"标准商品名\", \n",
    "                \"标准剂型\", \"标准规格\", \"标准包装数量\", \"标准生产企业\", \"标准省份名称\", \"标准城市名称\", \"PACK_ID\", \"Sales\", \"Units\", \n",
    "                \"Units_Box\"]\n",
    "\n",
    "out_extract_data_final = out_extract_data.select(out_cols).distinct()\n",
    "out_extract_data_final = out_extract_data_final.repartition(1)\n",
    "out_extract_data_final.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(out_extract_data_path)\n",
    "\n",
    "# 缓解存储\n",
    "# out_extract_data = out_extract_data.repartition(4)\n",
    "# out_extract_data.write.format(\"parquet\") \\\n",
    "#          .mode(\"overwrite\").save(out_tmp_path)\n",
    "# out_extract_data = spark.read.parquet(out_tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_c\n",
    "extract_sales = out_extract_data.select(\"project\", \"PACK_ID\", \"ATC\", \"标准通用名\").distinct() \\\n",
    "                        .join(ims_sales.select(\"PACK_ID\", \"Sales_ims\").distinct(), on=\"PACK_ID\", how=\"left\") \\\n",
    "                        .groupby(\"project\", \"ATC\", \"标准通用名\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_extract\")).persist()\n",
    "\n",
    "molecule_names = out_extract_data.select(\"标准通用名\").distinct().toPandas()[\"标准通用名\"].values.tolist()\n",
    "\n",
    "molecule_sales = ims_sales.select(\"PACK_ID\", \"ATC\", \"MOLE_NAME_CH\", \"Sales_ims\").distinct() \\\n",
    "                    .where(ims_sales.MOLE_NAME_CH.isin(molecule_names)) \\\n",
    "                    .groupby(\"ATC\", \"MOLE_NAME_CH\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_molecule\")) \\\n",
    "                    .withColumnRenamed(\"MOLE_NAME_CH\", \"标准通用名\").persist()\n",
    "\n",
    "report_c = extract_sales.join(molecule_sales, on=[\"标准通用名\", \"ATC\"], how=\"left\")\n",
    "report_c = report_c.withColumn(\"Sales_rate\", report_c.Sales_ims_extract/report_c.Sales_ims_molecule) \\\n",
    "                .join(report_a.where(report_a.flag == 1).select(\"标准通用名\", \"ATC\", \"project\", \"months_num\", \"time_range\"), \n",
    "                        on=[\"标准通用名\", \"ATC\", \"project\"], how=\"left\") \\\n",
    "                .drop(\"Sales_ims_extract\", \"Sales_ims_molecule\").persist()\n",
    "# 列名顺序调整\n",
    "report_c = report_c.select(\"project\", \"ATC\", \"标准通用名\", \"time_range\", \"months_num\", \"Sales_rate\") \\\n",
    "                    .orderBy([\"标准通用名\"]) \n",
    "\n",
    "report_c = report_c.repartition(1)\n",
    "report_c.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(report_c_path)\n",
    "\n",
    "# report_atc\n",
    "if atc:\n",
    "    time_range_act = out_extract_data.select(\"ATC\", \"Date\").distinct() \\\n",
    "                        .groupby([\"ATC\"]).agg(func.min(\"Date\").alias(\"min_time\"), func.max(\"Date\").alias(\"max_time\"))\n",
    "    time_range_act = time_range_act.withColumn(\"time_range\", func.concat(time_range_act.min_time, func.lit(\"_\"), time_range_act.max_time))\n",
    "\n",
    "    extract_sales = out_extract_data.select(\"PACK_ID\", \"ATC\").distinct() \\\n",
    "                        .join(ims_sales.select(\"PACK_ID\", \"Sales_ims\").distinct(), on=\"PACK_ID\", how=\"left\") \\\n",
    "                        .groupby(\"ATC\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_extract\")).persist()\n",
    "    atc_sales = ims_sales.select(\"PACK_ID\", \"ATC\", \"Sales_ims\").distinct() \\\n",
    "                    .where(ims_sales.ATC.isin(atc)) \\\n",
    "                    .groupby(\"ATC\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_atc\")).persist()\n",
    "    report_b = extract_sales.join(atc_sales, on=\"ATC\", how=\"left\")\n",
    "    report_b = report_b.withColumn(\"Sales_rate\", report_b.Sales_ims_extract/report_b.Sales_ims_atc) \\\n",
    "                    .join(time_range_act.select(\"ATC\", \"time_range\"), on=\"ATC\", how=\"left\") \\\n",
    "                    .drop(\"Sales_ims_extract\", \"Sales_ims_atc\")\n",
    "    # 列名顺序调整\n",
    "    report_b = report_b.select(\"ATC\", \"time_range\", \"Sales_rate\")\n",
    "\n",
    "    report_b = report_b.repartition(1)\n",
    "    report_b.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(report_b_path)\n",
    "\n",
    "# report_molecule        \n",
    "if molecule:\n",
    "    time_range = out_extract_data.select(\"标准通用名\", \"Date\").distinct() \\\n",
    "                        .groupby([\"标准通用名\"]).agg(func.min(\"Date\").alias(\"min_time\"), func.max(\"Date\").alias(\"max_time\"))\n",
    "    time_range = time_range.withColumn(\"time_range\", func.concat(time_range.min_time, func.lit(\"_\"), time_range.max_time))\n",
    "\n",
    "    extract_sales = out_extract_data.select(\"PACK_ID\", \"标准通用名\").distinct() \\\n",
    "                        .join(ims_sales.select(\"PACK_ID\", \"Sales_ims\").distinct(), on=\"PACK_ID\", how=\"left\") \\\n",
    "                        .groupby(\"标准通用名\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_extract\")).persist()\n",
    "    molecule_sales = ims_sales.select(\"PACK_ID\", \"MOLE_NAME_CH\", \"Sales_ims\").distinct() \\\n",
    "                    .where(ims_sales.MOLE_NAME_CH.isin(molecule)) \\\n",
    "                    .groupby(\"MOLE_NAME_CH\").agg(func.sum(\"Sales_ims\").alias(\"Sales_ims_molecule\")).persist()\n",
    "    report_d = extract_sales.join(molecule_sales, extract_sales['标准通用名']==molecule_sales['MOLE_NAME_CH'], how=\"left\")\n",
    "    report_d = report_d.withColumn(\"Sales_rate\", report_d.Sales_ims_extract/report_d.Sales_ims_molecule) \\\n",
    "                    .join(time_range.select(\"标准通用名\", \"time_range\"), on=\"标准通用名\", how=\"left\") \\\n",
    "                    .drop(\"Sales_ims_extract\", \"Sales_ims_molecule\", \"MOLE_NAME_CH\")\n",
    "    # 列名顺序调整\n",
    "    report_d = report_d.select(\"标准通用名\", \"time_range\", \"Sales_rate\")\n",
    "\n",
    "    report_d = report_d.repartition(1)\n",
    "    report_d.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(report_d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
