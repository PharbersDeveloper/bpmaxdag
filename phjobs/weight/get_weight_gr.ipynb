{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"get_weight_gr\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "minimum_product_sep = \"\\'|\\'\"\n",
    "minimum_product_columns = 'Brand, Form, Specifications, Pack_Number, Manufacturer'\n",
    "market_city_brand = 'Empty'\n",
    "universe_choice = 'Empty'\n",
    "job_choice = 'Empty'\n",
    "year_list = 'Empty'\n",
    "add_imsinfo_path = 'Empty'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write weight.get_weight_gr in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 测试\n",
    "year_list = '2019,2020'\n",
    "project_name = \"灵北\"\n",
    "outdir = \"202012\"\n",
    "market_city_brand = \"精神:常州市_3\"\n",
    "job_choice = \"weight\"\n",
    "minimum_product_sep = \"|\"\n",
    "#add_imsinfo_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/Takeda/add_ims_info.csv'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精神:常州市_3\n",
      "{'精神': {'常州市': '3'}}\n"
     ]
    }
   ],
   "source": [
    "# 输入\n",
    "\n",
    "# 是否运行此job\n",
    "if job_choice != \"weight\":\n",
    "     raise ValueError('不运行weight')\n",
    "\n",
    "print(market_city_brand)\n",
    "\n",
    "# year_list=['2018', '2019']\n",
    "year_list = year_list.replace(\" \",\"\").split(\",\")\n",
    "year_min = year_list[0]\n",
    "year_max = year_list[1]\n",
    "\n",
    "universe_choice_dict={}\n",
    "if universe_choice != \"Empty\":\n",
    "    for each in universe_choice.replace(\" \",\"\").split(\",\"):\n",
    "        market_name = each.split(\":\")[0]\n",
    "        universe_name = each.split(\":\")[1]\n",
    "        universe_choice_dict[market_name]=universe_name\n",
    "\n",
    "# market_city_brand_dict = json.loads(market_city_brand)\n",
    "minimum_product_columns = minimum_product_columns.replace(\" \",\"\").split(\",\")\n",
    "if minimum_product_sep == \"kong\":\n",
    "    minimum_product_sep = \"\"\n",
    "\n",
    "market_city_brand_dict={}\n",
    "for each in market_city_brand.replace(\" \",\"\").split(\",\"):\n",
    "    market_name = each.split(\":\")[0]\n",
    "    if market_name not in market_city_brand_dict.keys():\n",
    "        market_city_brand_dict[market_name]={}\n",
    "    city_brand = each.split(\":\")[1]\n",
    "    for each in city_brand.replace(\" \",\"\").split(\"|\"): \n",
    "        city = each.split(\"_\")[0]\n",
    "        brand = each.split(\"_\")[1]\n",
    "        market_city_brand_dict[market_name][city]=brand\n",
    "print(market_city_brand_dict)\n",
    "\n",
    "id_bedsize_path = max_path + '/' + '/Common_files/ID_Bedsize'\n",
    "universe_path = max_path + '/' + project_name + '/universe_base'\n",
    "city_info_path = max_path + '/' + project_name + '/province_city_mapping'\n",
    "mkt_mapping_path = max_path + '/' + project_name + '/mkt_mapping'\n",
    "cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "ims_sales_path = max_path + \"/Common_files/IMS_flat_files/202012/cn_IMS_Sales_Fdata_202012_1.csv\"\n",
    "# raw_data = spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/Test/Merck/raw_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  数据执行  ============\n",
    "\n",
    "# ====  一. 函数定义  ====\n",
    "@udf(StringType())\n",
    "def city_change(name):\n",
    "    # 城市名定义\n",
    "    if name in [\"福州市\", \"厦门市\", \"泉州市\"]:\n",
    "        newname = \"福厦泉\"\n",
    "    elif name in [\"珠海市\", \"东莞市\", \"中山市\", \"佛山市\"]:\n",
    "        newname = \"珠三角\"\n",
    "    elif name in [\"绍兴市\", \"嘉兴市\", \"台州市\", \"金华市\"]:\n",
    "        newname = \"浙江市\"\n",
    "    elif name in [\"苏州市\", \"无锡市\"]:\n",
    "        newname = \"苏锡市\"\n",
    "    else:\n",
    "        newname = name\n",
    "    return newname\n",
    "        \n",
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字。\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df\n",
    "\n",
    "# ====  二. 数据准备  ====    \n",
    "\n",
    "# 1. prod_map 文件\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "# a. 列名清洗统一\n",
    "if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[21], \"pfc\")\n",
    "if project_name == \"Eisai\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[22], \"pfc\")\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "# b. 选取需要的列\n",
    "product_map = product_map \\\n",
    "                .select(\"min1\", \"pfc\", \"通用名\", \"标准商品名\") \\\n",
    "                .withColumn(\"pfc\", product_map[\"pfc\"].cast(IntegerType())) \\\n",
    "                .distinct()\n",
    "# c. pfc为0统一替换为null\n",
    "product_map = product_map.withColumn(\"pfc\", func.when(product_map.pfc == 0, None).otherwise(product_map.pfc)).distinct()\n",
    "# d. min2处理\n",
    "product_map = product_map.withColumnRenamed(\"pfc\", \"Pack_ID\") \\\n",
    "                .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&amp;\", \"&\")) \\\n",
    "                .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&lt;\", \"<\")) \\\n",
    "                .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&gt;\", \">\"))\n",
    "\n",
    "# 2. universe 文件                  \n",
    "universe = spark.read.parquet(universe_path)\n",
    "universe = universe.select(\"Panel_ID\", \"Province\", \"City\", \"PANEL\") \\\n",
    "                    .withColumnRenamed(\"Panel_ID\", \"PHA\") \\\n",
    "                    .withColumn(\"PANEL\", col(\"PANEL\").cast(DoubleType())) \\\n",
    "                    .distinct()\n",
    "\n",
    "\n",
    "# 3. mkt_mapping 文件\n",
    "mkt_mapping = spark.read.parquet(mkt_mapping_path)\n",
    "mkt_mapping = mkt_mapping.select('mkt', '标准通用名').distinct() \\\n",
    "                        .withColumnRenamed(\"标准通用名\", \"通用名\")\n",
    "\n",
    "# 4. cpa_pha_mapping 文件\n",
    "cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)\n",
    "cpa_pha_mapping = cpa_pha_mapping.where(col(\"推荐版本\") == 1).select('ID', 'PHA').distinct()\n",
    "\n",
    "\n",
    "# 5. city_info_path\n",
    "city_info = spark.read.parquet(city_info_path)\n",
    "city_info = deal_ID_length(city_info)\n",
    "city_info = city_info.select('ID', 'Province', 'City') \\\n",
    "                    .withColumnRenamed('City', 'City_add').distinct()\n",
    "\n",
    "\n",
    "# 6. id_bedsize\n",
    "schema= StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"Bedsize\", DoubleType(), True),\n",
    "        StructField(\"Bedsize>99\", DoubleType(), True)\n",
    "        ])\n",
    "id_bedsize = spark.read.parquet(id_bedsize_path, schema=schema)\n",
    "id_bedsize = deal_ID_length(id_bedsize)\n",
    "id_bedsize = id_bedsize.join(cpa_pha_mapping, on='ID', how='left')\n",
    "\n",
    "# 7. raw_data 文件\n",
    "raw_data = spark.read.parquet(raw_data_path)\n",
    "# a. 生成min1\n",
    "if project_name != \"Mylan\":\n",
    "    raw_data = raw_data.withColumn(\"Brand\", func.when((raw_data.Brand.isNull()) | (raw_data.Brand == 'NA'), raw_data.Molecule).\n",
    "                               otherwise(raw_data.Brand))\n",
    "for colname, coltype in raw_data.dtypes:\n",
    "    if coltype == \"logical\":\n",
    "        raw_data = raw_data.withColumn(colname, raw_data[colname].cast(StringType()))\n",
    "\n",
    "raw_data = raw_data.withColumn(\"tmp\", func.when(func.isnull(raw_data[minimum_product_columns[0]]), func.lit(\"NA\")).\n",
    "                               otherwise(raw_data[minimum_product_columns[0]]))\n",
    "\n",
    "for i in minimum_product_columns[1:]:\n",
    "    raw_data = raw_data.withColumn(i, raw_data[i].cast(StringType()))\n",
    "    raw_data = raw_data.withColumn(\"tmp\", func.concat(\n",
    "        raw_data[\"tmp\"],\n",
    "        func.lit(minimum_product_sep),\n",
    "        func.when(func.isnull(raw_data[i]), func.lit(\"NA\")).otherwise(raw_data[i])))\n",
    "# Mylan不重新生成minimum_product_newname: min1，其他项目生成min1\n",
    "if project_name == \"Mylan\":\n",
    "    raw_data = raw_data.drop(\"tmp\")\n",
    "else:\n",
    "    if 'min1' in raw_data.columns:\n",
    "        raw_data = raw_data.drop('min1')\n",
    "    raw_data = raw_data.withColumnRenamed('tmp', 'min1')\n",
    "\n",
    "# b.字段类型修改\n",
    "raw_data = raw_data.withColumn('Year', func.substring(col('Date').cast(StringType()), 1, 4))\n",
    "raw_data = raw_data.withColumn('Year', col('Year').cast(StringType())) \\\n",
    "                        .withColumn('Date', col('Date').cast(IntegerType())) \\\n",
    "                        .withColumn('Sales', col('Sales').cast(DoubleType())) \\\n",
    "                        .withColumn('Units', col('Sales').cast(DoubleType())) \\\n",
    "                        .withColumn('Units_Box', col('Units_Box').cast(DoubleType()))\n",
    "\n",
    "# c.匹配信息: Pack_ID,通用名,标准商品名; mkt; PHA\n",
    "raw_data2 = deal_ID_length(raw_data)\n",
    "raw_data2 = raw_data2.join(product_map, on='min1', how='left')\n",
    "raw_data2 = raw_data2.join(mkt_mapping, on='通用名', how='left')\n",
    "raw_data2 = raw_data2.join(cpa_pha_mapping, on='ID', how='left')\n",
    "raw_data2 = raw_data2.join(universe.select(\"PHA\", \"Province\", \"City\").distinct(), on='PHA', how='left')\n",
    "\n",
    "raw_data2 = raw_data2.withColumn('标准商品名', func.when(col('标准商品名').isNull(), col('通用名')).otherwise(col('标准商品名')))\n",
    "\n",
    "# 8. ims 数据\n",
    "geo_name = {'BJH':'北京市', 'BOI':'天津市', 'BOJ':'济南市', 'CGH':'常州市', 'CHT':'全国', 'FXQ':'福厦泉', \n",
    "            'GZH':'广州市', 'NBH':'宁波市', 'SHH':'上海市', 'WZH':'温州市', 'SXC':'苏锡城市群',\n",
    "           'YZH':'杭州市', 'HCG':'长沙市', 'ZZH':'郑州市', 'YZW':'武汉市', 'XAH':'西安市', 'SIQ':'重庆市',\n",
    "           'CTX':'浙江市', 'YZN':'南京市', 'NNH':'南宁市', 'SID':'成都市', 'HFH':'合肥市', 'HRH':'哈尔滨市',\n",
    "           'SZH':'深圳市', 'WLH':'乌鲁木齐市', 'PRV':'珠三角', 'CCH':'长春市', 'NCG':'南昌市', 'SJH':'石家庄市',\n",
    "           'GYH':'贵阳市', 'BOS':'沈阳市', 'LZH':'兰州市', 'BOD':'大连市', 'KMH':'昆明市', 'BOQ':'青岛市', 'TYH':'太原市'}\n",
    "\n",
    "\n",
    "ims_sales = spark.read.csv(ims_sales_path, header=True)\n",
    "ims_sales = ims_sales.select('Geography_id', 'Pack_ID', 'Period_Code', 'LC')\n",
    "\n",
    "if add_imsinfo_path != 'Empty':\n",
    "    add_imsinfo_file = spark.read.csv(add_imsinfo_path, header=True)\n",
    "    add_imsinfo_file = add_imsinfo_file.select('Geography_id', 'Pack_ID', 'Period_Code', 'LC')\n",
    "    # 去掉add_imsinfo_file中有的\n",
    "    ims_sales_keep = ims_sales.join(add_imsinfo_file, on=[\"Pack_ID\", \"Geography_id\"], how='left_anti')\n",
    "    ims_sales = ims_sales_keep.union(add_imsinfo_file.select(ims_sales_keep.columns))\n",
    "\n",
    "geo_dict={}\n",
    "geo_dict['Geography_id'] = list(geo_name.keys())\n",
    "geo_dict['City'] = list(geo_name.values())\n",
    "geo_df = pd.DataFrame(geo_dict)\n",
    "geo_df = spark.createDataFrame(geo_df)  \n",
    "\n",
    "ims_sales = ims_sales.withColumn('Year', (func.regexp_replace(col('Period_Code'), 'M', '')/100).cast(IntegerType())) \\\n",
    "                    .join(geo_df.drop_duplicates(['Geography_id']), on='Geography_id', how='inner') \\\n",
    "                    .join(raw_data2.select('Pack_ID', 'mkt', '标准商品名').dropDuplicates(['Pack_ID']), \n",
    "                                        on='Pack_ID', how='inner')\n",
    "ims_sales = ims_sales.withColumn('Year',col('Year').cast(StringType()))\n",
    "\n",
    "ims_sales = ims_sales.where(col('Year').isin(year_list)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TK1\n"
     ]
    }
   ],
   "source": [
    "# ====  三. 每个市场分析  ==== \n",
    "\n",
    "market_list = list(market_city_brand_dict.keys())\n",
    "\n",
    "for market in market_list:\n",
    "    # 输入文件\n",
    "    print(market)\n",
    "    factor_path = max_path + '/' + project_name + '/factor/factor_' + market\n",
    "    universe_ot_path = max_path + '/' + project_name + '/universe/universe_ot_' + market\n",
    "\n",
    "    # 输出文件\n",
    "    data_target_path = max_path + '/' + project_name + '/weight/' + market + '_data_target'\n",
    "    ims_gr_path = max_path + '/' + project_name + '/weight/' + market + '_ims_gr'\n",
    "\n",
    "    # 1. 数据准备\n",
    "    # universe_ot 文件        \n",
    "    universe_ot = spark.read.parquet(universe_ot_path)\n",
    "    universe_ot = universe_ot.withColumnRenamed('Panel_ID', 'PHA')\n",
    "    # universe 文件: 如果有指定的universe则重新读取，否则就用已有的 universe_base\n",
    "    if market in universe_choice_dict.keys():\n",
    "        universe_path = max_path + '/' + project_name + '/' + universe_choice_dict[market]\n",
    "        universe = spark.read.parquet(universe_path)\n",
    "        universe = universe.select(\"Panel_ID\", \"Province\", \"City\", \"PANEL\") \\\n",
    "                            .withColumnRenamed(\"Panel_ID\", \"PHA\") \\\n",
    "                            .withColumn(\"PANEL\", col(\"PANEL\").cast(DoubleType())) \\\n",
    "                            .distinct()\n",
    "\n",
    "    # factor 文件    \n",
    "    factor = spark.read.parquet(factor_path)\n",
    "    if \"factor\" not in factor.columns:\n",
    "        factor = factor.withColumnRenamed(\"factor_new\", \"factor\")\n",
    "    factor = factor.select('City', 'factor').distinct()\n",
    "\n",
    "    # raw_data\n",
    "    data = raw_data2.where(col('mkt') == market)\n",
    "\n",
    "    # 2. 权重计算\n",
    "    '''\n",
    "    权重计算：\n",
    "        利用医药收入和城市Factor\n",
    "        从universe中去掉outliers以及其他Panel == 0的样本\n",
    "    '''\n",
    "    # %% 目的是实现 PANEL = 0 的样本回填\n",
    "    pha_list = universe.where(col('PANEL') == 0).where(~col('PHA').isNull()).select('PHA').distinct().toPandas()['PHA'].tolist()\n",
    "    pha_list2 = data.where(col('Date') > 202000).where(~col('PHA').isNull()).select('PHA').distinct().toPandas()['PHA'].tolist()\n",
    "\n",
    "    universe_ot_rm = universe_ot.where((col('PANEL') == 1) | (col('PHA').isin(pha_list))) \\\n",
    "                            .where( ~((col('PANEL') == 0) & (col('PHA').isin(pha_list2)) )) \\\n",
    "                            .fillna(0, subset=['Est_DrugIncome_RMB', 'BEDSIZE']) \\\n",
    "                            .withColumn('Panel_income', col('Est_DrugIncome_RMB') * col('PANEL')) \\\n",
    "                            .withColumn('Bedmark', func.when(col('BEDSIZE') >= 100, func.lit(1)).otherwise(func.lit(0)))\n",
    "    universe_ot_rm = universe_ot_rm.withColumn('non_Panel_income', col('Est_DrugIncome_RMB') * (1-col('PANEL')) * col('Bedmark'))\n",
    "\n",
    "    Seg_Panel_income = universe_ot_rm.groupBy('Seg').agg(func.sum('Panel_income').alias('Seg_Panel_income'))\n",
    "    Seg_non_Panel_income = universe_ot_rm.groupBy('Seg', 'City') \\\n",
    "                                .agg(func.sum('non_Panel_income').alias('Seg_non_Panel_income'))\n",
    "    universe_ot_rm = universe_ot_rm.join(Seg_Panel_income, on='Seg', how='left') \\\n",
    "                                .join(Seg_non_Panel_income, on=['Seg', 'City'], how='left').persist()\n",
    "\n",
    "    # %% 按Segment+City拆分样本\n",
    "    seg_city = universe_ot_rm.select('Seg','City','Seg_non_Panel_income','Seg_Panel_income').distinct()\n",
    "\n",
    "    seg_pha = universe_ot_rm.where(col('PANEL') == 1).select('Seg','City','PHA','BEDSIZE').distinct() \\\n",
    "                            .withColumnRenamed('City', 'City_Sample')\n",
    "\n",
    "    weight_table = seg_city.join(seg_pha, on='Seg', how='left') \\\n",
    "                            .join(factor, on='City', how='left').persist()\n",
    "\n",
    "    # 只给在城市内100床位以上的样本，添加 1 \n",
    "    weight_table = weight_table.withColumn('tmp', func.when((col('City') == col('City_Sample')) & (col('BEDSIZE') > 99) , \\\n",
    "                                                        func.lit(1)).otherwise(func.lit(0)))\n",
    "    weight_table = weight_table.withColumn('weight', col('tmp') + \\\n",
    "                                                col('factor') * col('Seg_non_Panel_income') / col('Seg_Panel_income'))\n",
    "\n",
    "    # pandas当 Seg_non_Panel_income和Seg_Panel_income都为0结果是 null，只有 Seg_Panel_income 是 0 结果为 inf \n",
    "    # pyspark 都会是null, 用-999代表无穷大\n",
    "    weight_table = weight_table.withColumn('weight', func.when((col('Seg_Panel_income') == 0) & (col('Seg_non_Panel_income') != 0), \\\n",
    "                                                           func.lit(-999)).otherwise(col('weight')))\n",
    "    weight_init = weight_table.select('Seg','City','PHA','weight','City_Sample','BEDSIZE').distinct()\n",
    "\n",
    "    # %% 权重与PANEL的连接\n",
    "    # 置零之和 不等于 和的置零，因此要汇总到Date层面\n",
    "    data_sub = data.groupBy('PHA','ID','Date', 'Year').agg(func.sum('Sales').alias('Sales'))\n",
    "\n",
    "    tmp_weight = weight_init.join(data_sub, on='PHA', how='outer').persist()\n",
    "    tmp_weight = tmp_weight.fillna({'weight':1, 'Sales':0})\n",
    "    # 无穷大仍然为 null \n",
    "    tmp_weight = tmp_weight.withColumn('weight', func.when(col('weight') == -999, func.lit(None)).otherwise(col('weight')))\n",
    "\n",
    "    # %% 匹配省份城市\n",
    "    tmp_city = tmp_weight.join(city_info, on='ID', how='left')\n",
    "    tmp_city = tmp_city.withColumn('City', func.when(col('City').isNull(), col('City_add')).otherwise(col('City'))) \\\n",
    "                .withColumn('City_Sample', func.when(col('City_Sample').isNull(), col('City_add')) \\\n",
    "                                                .otherwise(col('City_Sample')))\n",
    "\n",
    "    # %% 床位数匹配以及权重修正\n",
    "    tmp_bed =  tmp_city.join(id_bedsize.select('ID', 'Bedsize>99').distinct(), on='ID', how='left')\n",
    "\n",
    "    # 不在大全的床位数小于1的，权重减1，等同于剔除\n",
    "    tmp_bed = tmp_bed.withColumn('weight', func.when((col('Bedsize>99') == 0) & (col('PHA').isNull()), \\\n",
    "                                                col('weight')-1).otherwise(col('weight')))\n",
    "\n",
    "    # %% MAX结果计算\n",
    "    tmp_bed = tmp_bed.withColumn('MAX_' + year_max, col('Sales') * col('weight'))\n",
    "    tmp_bed_seg = tmp_bed.groupBy('Seg','Date').agg(func.sum('MAX_' + year_max).alias('MAX_' + year_max)).persist()\n",
    "    tmp_bed_seg = tmp_bed_seg.withColumn('Positive', func.when(col('MAX_' + year_max) >= 0, func.lit(1)).otherwise(func.lit(0)))\n",
    "    tmp_bed = tmp_bed.join(tmp_bed_seg.select('Seg','Positive').distinct(), on='Seg', how='left')\n",
    "    tmp_bed = tmp_bed.fillna({'Positive':1})\n",
    "\n",
    "    #tmp_max_city = tmp_bed.where(col('Positive') == 1).where(~col('Year').isNull()) \\\n",
    "    #                    .groupBy('City','Year') \\\n",
    "    #                    .agg(func.sum('MAX_2019').alias('MAX_2019'), func.sum('Sales').alias('Sales')).persist()\n",
    "\n",
    "    # %% 权重初始值\n",
    "    weight_0 = tmp_bed.select('City','PHA','Positive','City_Sample','Seg','weight') \\\n",
    "                        .withColumn('City', city_change(col('City'))) \\\n",
    "                        .withColumn('City_Sample', city_change(col('City_Sample'))).distinct()\n",
    "\n",
    "\n",
    "    # 3. 需要优化的城市数据\n",
    "    # 城市列表\n",
    "    city_brand_dict = market_city_brand_dict[market]\n",
    "    city_list = list(city_brand_dict.keys())\n",
    "\n",
    "    # %% 观察放大结果\n",
    "    data_target = data.where(col('Year').isin(year_list)) \\\n",
    "                        .select('Date','PHA','标准商品名','Sales', 'Year').distinct() \\\n",
    "                        .join(weight_0, on='PHA', how='inner').persist()\n",
    "    data_target = data_target.where(col('City').isin(city_list))\n",
    "\n",
    "    data_target = data_target.withColumn('MAX_weighted', col('weight')*col('Sales')) \\\n",
    "                            .withColumn('tmp', func.concat(col('Year'), func.lit('_'), col('标准商品名'))) \\\n",
    "                            .join(id_bedsize.select('PHA','Bedsize>99').dropDuplicates(['PHA']), on='PHA', how='left')\n",
    "\n",
    "    # 输出\n",
    "    data_target = data_target.repartition(1)\n",
    "    data_target.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(data_target_path)\n",
    "\n",
    "    # 4. ims数据\n",
    "    '''\n",
    "    这个字典最好根据IMS数据自动生成\n",
    "        本例是贝达项目的宁波市调整\n",
    "\n",
    "    若想自动优化各个城市，可以循环多重字典。字典生成需要：\n",
    "        1. 处理IMS数据，计算增长率等\n",
    "        2. 按 \"城市 -> 类别 -> 产品\" 的层级生成字典\n",
    "    '''\n",
    "    if False:\n",
    "        data_count = data\n",
    "        #选择要进行计算增长率的分子\n",
    "        data_count = data_count.groupBy('Year','标准商品名','City').agg(func.sum('Sales').alias('Sales'))\n",
    "        data_count = data_count.groupBy('City','标准商品名').pivot('Year').agg(func.sum('Sales')).fillna(0).persist()\n",
    "        data_count = data_count.withColumn('gr', col(year_max)/col(year_min))\n",
    "        all_sales = data_count.groupBy('City').agg(func.sum(year_min).alias(year_min + '_sales'), func.sum(year_max).alias(year_max + '_sales'))\n",
    "\n",
    "        data_count = data_count.join(all_sales, on='City', how='inner')\n",
    "        gr_city = data_count.groupBy('City').agg(func.sum(year_min + '_sales').alias(year_min), func.sum(year_min + '_sales').alias(year_max))\n",
    "        gr_city = gr_city.withColumn('gr', col(year_max)/col(year_min)) \\\n",
    "                        .withColumnRenamed('City', 'city')\n",
    "\n",
    "        data_count = data_count.withColumn('share', col(year_max)/col(year_max + '_sales')) \\\n",
    "                                .withColumn('share_ly', col(year_min)/col(year_min + '_sales'))\n",
    "        data_count = data_count.select('City','标准商品名','gr','share','share_ly').distinct() \\\n",
    "                                .fillna(0)\n",
    "\n",
    "    # ims 数据\n",
    "    ims_sales_mkt = ims_sales.where(col('mkt') == market)\n",
    "    ims_sales_brand = ims_sales_mkt.groupBy('mkt', 'City', '标准商品名', 'Year').agg(func.sum('LC').alias('Sales'))\n",
    "    ims_sales_city = ims_sales_mkt.groupBy('mkt', 'City', 'Year').agg(func.sum('LC').alias('Sales_city'))\n",
    "\n",
    "    ims_sales_gr = ims_sales_brand.join(ims_sales_city, on=['mkt', 'City', 'Year'], how='left').persist()\n",
    "    ims_sales_gr = ims_sales_gr.groupBy('mkt', 'City', '标准商品名').pivot('Year') \\\n",
    "                            .agg(func.sum('Sales').alias('Sales'), func.sum('Sales_city').alias('Sales_city')) \\\n",
    "                            .fillna(0)\n",
    "    ims_sales_gr = ims_sales_gr.withColumn('gr', col(year_max + '_Sales')/col(year_min + '_Sales')) \\\n",
    "                               .withColumn('share', col(year_max + '_Sales')/col(year_max + '_Sales_city')) \\\n",
    "                               .withColumn('share_ly', col(year_min + '_Sales')/col(year_min + '_Sales_city'))\n",
    "    ims_sales_gr = ims_sales_gr.select('mkt', 'City', '标准商品名', 'gr', 'share', 'share_ly') \\\n",
    "                                .withColumnRenamed('mkt', 'DOI')\n",
    "\n",
    "    '''                           \n",
    "    # 测试用\n",
    "    ims_sales_gr = spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/Test/Eisai/Share_total.csv', header=True)\n",
    "    ims_sales_gr = spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/Test/Merck/Share_total_merk.csv', header=True)\n",
    "    ims_sales_gr = ims_sales_gr.select('IMS_gr', 'IMS_share_2019', 'IMS_share_2018', 'Brand', 'City', 'DOI') \\\n",
    "                                .withColumn('IMS_gr', col('IMS_gr').cast(DoubleType())) \\\n",
    "                                .withColumn('IMS_share_2019', col('IMS_share_2019').cast(DoubleType())) \\\n",
    "                                .withColumn('IMS_share_2018', col('IMS_share_2018').cast(DoubleType())) \\\n",
    "                                .withColumnRenamed('IMS_gr', 'gr') \\\n",
    "                                .withColumnRenamed('IMS_share_2019', 'share') \\\n",
    "                                .withColumnRenamed('IMS_share_2018', 'share_ly') \\\n",
    "                                .fillna(0)\n",
    "    ims_sales_gr = ims_sales_gr.where(col('Brand') != 'total')\n",
    "    ims_sales_gr = ims_sales_gr.where(col('DOI') == market) \\\n",
    "                                .withColumnRenamed('Brand', '标准商品名')\n",
    "    # 测试用 over\n",
    "    '''\n",
    "\n",
    "    ims_sales_gr = ims_sales_gr.repartition(1)\n",
    "    ims_sales_gr.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(ims_gr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
