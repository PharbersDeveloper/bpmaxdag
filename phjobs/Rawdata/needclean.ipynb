{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"needclean\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "minimum_product_sep = \"\\'|\\'\"\n",
    "minimum_product_columns = \"Brand, Form, Specifications, Pack_Number, Manufacturer\"\n",
    "test = 'False'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write Rawdata.needclean in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "if minimum_product_sep == \"kong\":\n",
    "    minimum_product_sep = \"\"\n",
    "minimum_product_columns = minimum_product_columns.replace(\" \",\"\").split(\",\")\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "\n",
    "if test == 'True':\n",
    "    all_raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/raw_data'\n",
    "else:\n",
    "    all_raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "\n",
    "# 输出\n",
    "need_clean_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/need_cleaning_raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`min1`' given input columns: [Brand, Corp, Form, Manufacturer, Molecule, Pack_Number, Prd_desc, Route, Specifications, min2, model, pfc, version, 标准剂型, 标准包装数量, 标准商品名, 标准生产企业, 标准规格, 标准途径, 标准通用名, 标准集团];;\n'Project [Molecule#165, Brand#166, Form#167, Specifications#168, Pack_Number#169, Manufacturer#170, Corp#171, Route#172, min2#173, 标准通用名#174, 标准商品名#175, 标准剂型#176, 标准规格#177, 标准包装数量#178, 标准生产企业#179, 标准集团#180, 标准途径#181, model#182, version#183, pfc#184, Prd_desc#185, regexp_replace('min1, &amp;, &) AS min1#207]\n+- Deduplicate [标准途径#181, Prd_desc#185, 标准包装数量#178, Molecule#165, model#182, 标准商品名#175, Corp#171, Form#167, Route#172, 标准剂型#176, 标准规格#177, version#183, 标准集团#180, Manufacturer#170, 标准生产企业#179, 标准通用名#174, Pack_Number#169, Specifications#168, Brand#166, min2#173, pfc#184]\n   +- Relation[Molecule#165,Brand#166,Form#167,Specifications#168,Pack_Number#169,Manufacturer#170,Corp#171,Route#172,min2#173,标准通用名#174,标准商品名#175,标准剂型#176,标准规格#177,标准包装数量#178,标准生产企业#179,标准集团#180,标准途径#181,model#182,version#183,pfc#184,Prd_desc#185] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec1982fe3110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 已有的product_map文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mproduct_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_map_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mproduct_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&amp;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&lt;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`min1`' given input columns: [Brand, Corp, Form, Manufacturer, Molecule, Pack_Number, Prd_desc, Route, Specifications, min2, model, pfc, version, 标准剂型, 标准包装数量, 标准商品名, 标准生产企业, 标准规格, 标准途径, 标准通用名, 标准集团];;\n'Project [Molecule#165, Brand#166, Form#167, Specifications#168, Pack_Number#169, Manufacturer#170, Corp#171, Route#172, min2#173, 标准通用名#174, 标准商品名#175, 标准剂型#176, 标准规格#177, 标准包装数量#178, 标准生产企业#179, 标准集团#180, 标准途径#181, model#182, version#183, pfc#184, Prd_desc#185, regexp_replace('min1, &amp;, &) AS min1#207]\n+- Deduplicate [标准途径#181, Prd_desc#185, 标准包装数量#178, Molecule#165, model#182, 标准商品名#175, Corp#171, Form#167, Route#172, 标准剂型#176, 标准规格#177, version#183, 标准集团#180, Manufacturer#170, 标准生产企业#179, 标准通用名#174, Pack_Number#169, Specifications#168, Brand#166, min2#173, pfc#184]\n   +- Relation[Molecule#165,Brand#166,Form#167,Specifications#168,Pack_Number#169,Manufacturer#170,Corp#171,Route#172,min2#173,标准通用名#174,标准商品名#175,标准剂型#176,标准规格#177,标准包装数量#178,标准生产企业#179,标准集团#180,标准途径#181,model#182,version#183,pfc#184,Prd_desc#185] parquet\n"
     ]
    }
   ],
   "source": [
    "# =========  数据执行  =============\n",
    "all_raw_data = spark.read.parquet(all_raw_data_path)\n",
    "\n",
    "clean = all_raw_data.select('Brand','Form','Specifications','Pack_Number','Manufacturer','Molecule','Corp','Route','Path','Sheet').distinct()\n",
    "\n",
    "# 生成min1\n",
    "clean = clean.withColumn('Brand', func.when((clean.Brand.isNull()) | (clean.Brand == 'NA'), clean.Molecule).otherwise(clean.Brand))\n",
    "clean = clean.withColumn(\"min1\", func.when(clean[minimum_product_columns[0]].isNull(), func.lit(\"NA\")).\n",
    "                                   otherwise(clean[minimum_product_columns[0]]))\n",
    "for col in minimum_product_columns[1:]:\n",
    "    clean = clean.withColumn(col, clean[col].cast(StringType()))\n",
    "    clean = clean.withColumn(\"min1\", func.concat(\n",
    "        clean[\"min1\"],\n",
    "        func.lit(minimum_product_sep),\n",
    "        func.when(func.isnull(clean[col]), func.lit(\"NA\")).otherwise(clean[col])))\n",
    "\n",
    "# 已有的product_map文件\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "product_map = product_map.distinct() \\\n",
    "                    .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&amp;\", \"&\")) \\\n",
    "                    .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&lt;\", \"<\")) \\\n",
    "                    .withColumn(\"min1\", func.regexp_replace(\"min1\", \"&gt;\", \">\"))\n",
    "\n",
    "# min1不在product_map中的为需要清洗的条目                 \n",
    "need_clean = clean.join(product_map.select('min1').distinct(), on='min1', how='left_anti')\n",
    "if need_clean.count() > 0:\n",
    "    need_clean = need_clean.repartition(1)\n",
    "    need_clean.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(need_clean_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
