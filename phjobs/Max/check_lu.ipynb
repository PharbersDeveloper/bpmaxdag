{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"check_lu\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "out_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "if_base = 'False'\n",
    "time_left = 'Empty'\n",
    "time_right = 'Empty'\n",
    "left_models = \"Empty\"\n",
    "left_models_time_left = \"Empty\"\n",
    "right_models = \"Empty\"\n",
    "right_models_time_right = \"Empty\"\n",
    "all_models = 'Empty'\n",
    "universe_choice = 'Empty'\n",
    "if_others = 'False'\n",
    "out_dir = 'Empty'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write Max.check_lu in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "out_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = '汇宇'\n",
    "if_base = 'False'\n",
    "time_left = '202001'\n",
    "time_right = '202011'\n",
    "left_models = \"Empty\"\n",
    "left_models_time_left = \"Empty\"\n",
    "right_models = \"Empty\"\n",
    "right_models_time_right = \"Empty\"\n",
    "all_models = '阿扎胞苷,紫杉醇,奥沙利铂'\n",
    "universe_choice = '奥沙利铂:universe_base_肿瘤'\n",
    "if_others = 'False'\n",
    "out_dir = '202011'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入输出\n",
    "if if_base == \"False\":\n",
    "    if_base = False\n",
    "elif if_base == \"True\":\n",
    "    if_base = True\n",
    "else:\n",
    "    raise ValueError('if_base: False or True')\n",
    "if left_models != \"Empty\":\n",
    "    left_models = left_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    left_models = []\n",
    "if right_models != \"Empty\":\n",
    "    right_models = right_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    right_models = []\n",
    "if left_models_time_left == \"Empty\":\n",
    "    left_models_time_left = 0\n",
    "if right_models_time_right == \"Empty\":\n",
    "    right_models_time_right = 0\n",
    "\n",
    "time_parameters = [int(time_left), int(time_right), left_models, int(left_models_time_left), right_models, int(right_models_time_right)]\n",
    "\n",
    "if all_models != \"Empty\":\n",
    "    all_models = all_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    all_models = []\n",
    "\n",
    "project_path = max_path + \"/\" + project_name\n",
    "\n",
    "if if_others == \"True\":\n",
    "    out_dir = out_dir + \"/others_box/\"\n",
    "out_path_dir = out_path + \"/\" + project_name + '/' + out_dir\n",
    "\n",
    "# 市场的universe文件\n",
    "universe_choice_dict={}\n",
    "if universe_choice != \"Empty\":\n",
    "    for each in universe_choice.replace(\", \",\",\").split(\",\"):\n",
    "        market_name = each.split(\":\")[0]\n",
    "        universe_name = each.split(\":\")[1]\n",
    "        universe_choice_dict[market_name]=universe_name\n",
    "\n",
    "# 医院权重文件\t \n",
    "PHA_weight_path = max_path + \"/\" + project_name + '/PHA_weight'\n",
    "PHA_weight = spark.read.parquet(PHA_weight_path)\n",
    "PHA_weight = PHA_weight.select('Province', 'City', 'DOI', 'Weight', 'PHA')\n",
    "PHA_weight = PHA_weight.withColumnRenamed('Province', 'Province_w') \\\n",
    "                        .withColumnRenamed('City', 'City_w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = out_path_dir + \"/max_check/check_lu\"\n",
    "out_csv_path = out_path_dir + \"/max_check/check_lu.csv\"\n",
    "\n",
    "if if_others == \"False\":\n",
    "    if_box=False\n",
    "elif if_others == \"True\":\n",
    "    if_box=True\n",
    "\n",
    "for index, market in enumerate(all_models):\n",
    "    if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "        if market in ['SNY6', 'SNY10', 'SNY12', 'SNY13', 'AZ12', 'AZ18', 'AZ21']:\n",
    "            universe_path = project_path + '/universe_az_sanofi_onc'\n",
    "        elif market in ['SNY5', 'SNY9', 'AZ10', 'AZ11', 'AZ15', 'AZ16', 'AZ14', 'AZ26', 'AZ24']:\n",
    "            universe_path = project_path + '/universe_az_sanofi_mch'\n",
    "        else:\n",
    "            universe_path = project_path + '/universe_base'\n",
    "    else:\n",
    "        if market in universe_choice_dict.keys():\n",
    "            universe_path = project_path + '/' + universe_choice_dict[market]\n",
    "        else:\n",
    "            universe_path = project_path + '/universe_base'\n",
    "\n",
    "    # universe_outlier_path 以及 factor_path 文件选择\n",
    "    universe_outlier_path = project_path + \"/universe/universe_ot_\" + market\n",
    "    if if_base:\n",
    "        factor_path = project_path + \"/factor/factor_base\"\n",
    "    else:\n",
    "        factor_path = project_path + \"/factor/factor_\" + market\n",
    "\n",
    "    # panel 文件选择与读取 获得 original_panel\n",
    "    panel_box_path = out_path_dir + \"/panel_result_box\"\n",
    "    panel_path = out_path_dir + \"/panel_result\"\n",
    "\n",
    "    if if_box:\n",
    "        original_panel_path = panel_box_path\n",
    "    else:\n",
    "        original_panel_path = panel_path\n",
    "\n",
    "    PHA_weight_market = PHA_weight.where(PHA_weight.DOI == market)\n",
    "\n",
    "    # =========== 数据执行 =============\n",
    "\n",
    "    #logger.info('数据执行-start')\n",
    "\n",
    "    # 选择 market 的时间范围：choose_months\n",
    "    time_left_raw = time_parameters[0]\n",
    "    time_right_raw = time_parameters[1]\n",
    "    left_models = time_parameters[2]\n",
    "    left_models_time_left = time_parameters[3]\n",
    "    right_models = time_parameters[4]\n",
    "    right_models_time_right = time_parameters[5]\n",
    "\n",
    "    if market in left_models:\n",
    "        time_left = left_models_time_left\n",
    "    else:\n",
    "        time_left = time_left_raw\n",
    "        \n",
    "    if market in right_models:\n",
    "        time_right = right_models_time_right\n",
    "    else:\n",
    "        time_right = time_right_raw\n",
    "        \n",
    "    time_range = str(time_left) + '_' + str(time_right)\n",
    "\n",
    "    # universe_outlier 文件读取与处理：read_uni_ot\n",
    "    universe_outlier = spark.read.parquet(universe_outlier_path)\n",
    "    if \"CITYGROUP\" in universe_outlier.columns:\n",
    "        universe_outlier = universe_outlier.withColumnRenamed(\"CITYGROUP\", \"City_Tier_2010\")\n",
    "    elif \"City_Tier\" in universe_outlier.columns:\n",
    "        universe_outlier = universe_outlier.withColumnRenamed(\"City_Tier\", \"City_Tier_2010\")\n",
    "    universe_outlier = universe_outlier.withColumnRenamed(\"Panel_ID\", \"PHA\") \\\n",
    "        .withColumnRenamed(\"Hosp_name\", \"HOSP_NAME\")\n",
    "    universe_outlier = universe_outlier.withColumn(\"City_Tier_2010\", universe_outlier[\"City_Tier_2010\"].cast(StringType()))\n",
    "    universe_outlier = universe_outlier.select(\"PHA\", \"Est_DrugIncome_RMB\", \"PANEL\", \"Seg\", \"BEDSIZE\")\n",
    "\n",
    "    # universe 文件读取与处理：read_universe\n",
    "    universe = spark.read.parquet(universe_path)\n",
    "    if \"CITYGROUP\" in universe.columns:\n",
    "        universe = universe.withColumnRenamed(\"CITYGROUP\", \"City_Tier_2010\")\n",
    "    elif \"City_Tier\" in universe.columns:\n",
    "        universe = universe.withColumnRenamed(\"City_Tier\", \"City_Tier_2010\")\n",
    "    universe = universe.withColumnRenamed(\"Panel_ID\", \"PHA\") \\\n",
    "        .withColumnRenamed(\"Hosp_name\", \"HOSP_NAME\")\n",
    "    universe = universe.withColumn(\"City_Tier_2010\", universe[\"City_Tier_2010\"].cast(StringType()))\n",
    "\n",
    "    # panel 文件读取 获得 original_panel\n",
    "    original_panel = spark.read.parquet(original_panel_path)\n",
    "    original_panel = original_panel.where((original_panel.DOI == market) & (original_panel.Date >= time_left) & (original_panel.Date <= time_right)).cache() # TEST\n",
    "\n",
    "    # 获得 panel, panel_seg：group_panel_by_seg\n",
    "\n",
    "    # panel：整理成max的格式，包含了所有在universe的panel列标记为1的医院，当作所有样本医院的max\n",
    "    universe_panel_all = universe.where(universe.PANEL == 1).select('PHA', 'BEDSIZE', 'PANEL', 'Seg')\n",
    "    panel = original_panel \\\n",
    "        .join(universe_panel_all, original_panel.HOSP_ID == universe_panel_all.PHA, how=\"inner\") \\\n",
    "        .groupBy('PHA', 'Province', 'City', 'Date', 'Molecule', 'Prod_Name', 'BEDSIZE', 'PANEL', 'Seg') \\\n",
    "        .agg(func.sum(\"Sales\").alias(\"Predict_Sales\"), func.sum(\"Units\").alias(\"Predict_Unit\")).cache()\n",
    "\n",
    "    # panel_seg：整理成seg层面，包含了所有在universe_ot的panel列标记为1的医院，可以用来得到非样本医院的max\n",
    "    panel_drugincome = universe_outlier.where(universe_outlier.PANEL == 1) \\\n",
    "        .groupBy(\"Seg\") \\\n",
    "        .agg(func.sum(\"Est_DrugIncome_RMB\").alias(\"DrugIncome_Panel\")).cache() # TEST\n",
    "    original_panel_tmp = original_panel.join(universe_outlier, original_panel.HOSP_ID == universe_outlier.PHA, how='left').cache() # TEST\n",
    "\n",
    "    panel_seg = original_panel_tmp.where(original_panel_tmp.PANEL == 1) \\\n",
    "        .groupBy('Date', 'Prod_Name', 'Seg', 'Molecule') \\\n",
    "        .agg(func.sum(\"Sales\").alias(\"Sales_Panel\"), func.sum(\"Units\").alias(\"Units_Panel\")).cache()\n",
    "    panel_seg = panel_seg.join(panel_drugincome, on=\"Seg\", how=\"left\").cache() # TEST\n",
    "\n",
    "    # *** PHA_city 权重计算\n",
    "    original_panel_weight = original_panel_tmp.join(PHA_weight_market, on=['PHA'], how='left')\n",
    "    original_panel_weight = original_panel_weight.withColumn('Weight', func.when(original_panel_weight.Weight.isNull(), func.lit(1)) \\\n",
    "                                                                            .otherwise(original_panel_weight.Weight))\n",
    "    original_panel_weight = original_panel_weight.withColumn('Sales_w', original_panel_weight.Sales * original_panel_weight.Weight) \\\n",
    "                                                .withColumn('Units_w', original_panel_weight.Units * original_panel_weight.Weight)\n",
    "\n",
    "    panel_seg_weight = original_panel_weight.where(original_panel_weight.PANEL == 1) \\\n",
    "        .groupBy('Date', 'Prod_Name', 'Seg', 'Molecule', 'Province_w', 'City_w') \\\n",
    "        .agg(func.sum(\"Sales_w\").alias(\"Sales_Panel_w\"), func.sum(\"Units_w\").alias(\"Units_Panel_w\")).cache() # TEST\n",
    "    panel_seg_weight = panel_seg_weight.join(panel_drugincome, on=\"Seg\", how=\"left\").cache() # TEST\n",
    "\n",
    "    panel_seg_weight = panel_seg_weight.withColumnRenamed('Province_w', 'Province') \\\n",
    "                    .withColumnRenamed('City_w', 'City')\n",
    "\n",
    "    # 将非样本的segment和factor等信息合并起来：get_uni_with_factor\n",
    "    factor = spark.read.parquet(factor_path)\n",
    "    if \"factor\" not in factor.columns:\n",
    "        factor = factor.withColumnRenamed(\"factor_new\", \"factor\")\n",
    "    factor = factor.select('City', 'factor')\n",
    "    universe_factor_panel = universe.join(factor, on=\"City\", how=\"left\").cache() # TEST\n",
    "    universe_factor_panel = universe_factor_panel \\\n",
    "        .withColumn(\"factor\", func.when(func.isnull(universe_factor_panel.factor), func.lit(1)).otherwise(universe_factor_panel.factor)) \\\n",
    "        .where(universe_factor_panel.PANEL == 1) \\\n",
    "        .select('Province', 'City', 'PHA', 'Est_DrugIncome_RMB', 'Seg', 'BEDSIZE', 'PANEL', 'factor').cache() # TEST\n",
    "\n",
    "    # 为这些非样本医院匹配上样本金额、产品、年月、所在segment的drugincome之和\n",
    "    # 优先有权重的结果\n",
    "    max_result = universe_factor_panel.join(panel_seg, on=\"Seg\", how=\"left\")\n",
    "    max_result = max_result.join(panel_seg_weight.select('Date', 'Prod_Name', 'Molecule', 'Seg', 'Province', 'City', 'Sales_Panel_w', 'Units_Panel_w').distinct(), \n",
    "                                    on=['Date', 'Prod_Name', 'Molecule', 'Seg', 'Province', 'City'], how=\"left\")\n",
    "    max_result = max_result.withColumn('Sales_Panel', func.when(max_result.Sales_Panel_w.isNull(), max_result.Sales_Panel) \\\n",
    "                                                            .otherwise(max_result.Sales_Panel_w)) \\\n",
    "                            .withColumn('Units_Panel', func.when(max_result.Units_Panel_w.isNull(), max_result.Units_Panel) \\\n",
    "                                                            .otherwise(max_result.Units_Panel_w)) \\\n",
    "                            .drop('Sales_Panel_w', 'Units_Panel_w')\n",
    "\n",
    "    # 预测值等于样本金额乘上当前医院drugincome再除以所在segment的drugincome之和\n",
    "    max_result = max_result.withColumn(\"Predict_Sales\", (max_result.Sales_Panel / max_result.DrugIncome_Panel) * max_result.Est_DrugIncome_RMB) \\\n",
    "        .withColumn(\"Predict_Unit\", (max_result.Units_Panel / max_result.DrugIncome_Panel) * max_result.Est_DrugIncome_RMB).cache() # TEST\n",
    "\n",
    "    # 为什么有空，因为部分segment无样本或者样本金额为0：remove_nega\n",
    "    max_result = max_result.where(~func.isnull(max_result.Predict_Sales))\n",
    "    max_result = max_result.withColumn(\"positive\", func.when(max_result[\"Predict_Sales\"] > 0, 1).otherwise(0))\n",
    "    max_result = max_result.withColumn(\"positive\", func.when(max_result[\"Predict_Unit\"] > 0, 1).otherwise(max_result.positive))\n",
    "    max_result = max_result.where(max_result.positive == 1).drop(\"positive\")\n",
    "\n",
    "    # 乘上factor\n",
    "    max_result = max_result.withColumn(\"Predict_Sales\", max_result.Predict_Sales * max_result.factor) \\\n",
    "        .withColumn(\"Predict_Unit\", max_result.Predict_Unit * max_result.factor) \\\n",
    "        .select('PHA', 'Province', 'City', 'Date', 'Molecule', 'Prod_Name', 'BEDSIZE', 'PANEL',\n",
    "                'Seg', 'Predict_Sales', 'Predict_Unit')\n",
    "\n",
    "    panel_out = panel.withColumnRenamed('Predict_Sales', 'Sales') \\\n",
    "                    .withColumnRenamed('Predict_Unit', 'Unit') \\\n",
    "                    .select('PHA', 'Date', 'Prod_Name', 'Sales', 'Unit')\n",
    "    out = max_result.join(panel_out, on=['PHA', 'Date', 'Prod_Name'], how='left')\n",
    "    \n",
    "    out = out.withColumn('DOI', func.lit(market))\n",
    "    \n",
    "    # 输出结果\n",
    "    out = out.repartition(1)\n",
    "    if index == 0:\n",
    "        out.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(out_path)\n",
    "    else:\n",
    "        out.write.format(\"parquet\") \\\n",
    "            .mode(\"append\").save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = spark.read.parquet(out_path)\n",
    "out_csv = out_csv.repartition(1)\n",
    "out_csv.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(out_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
