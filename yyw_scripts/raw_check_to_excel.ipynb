{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"raw_check_to_excel\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 10.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "a = 123\n",
    "b = 456\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write .raw_check_to_excel in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "合并s3上的raw_data_check/.csv 为excel\n",
    "'''\n",
    "project_name = 'Takeda'\n",
    "outdir = '202012'\n",
    "outdir_local = \"/home/ywyuan/tmp_file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "raw_data_check_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/'\n",
    "check_result_path = raw_data_check_path + '/check_result.csv'\n",
    "check_1_path = raw_data_check_path + '/check_1_每个月产品个数.csv'\n",
    "check_2_path = raw_data_check_path + '/check_2_各产品历史月份销量.csv'\n",
    "check_3_path = raw_data_check_path + '/check_3_历史医院个数.csv'\n",
    "check_5_path = raw_data_check_path + '/check_5_最近12期每家医院每个月的金额规模.csv'\n",
    "check_8_path = raw_data_check_path + '/check_8_每个医院每个月产品个数.csv'\n",
    "check_9_1_path = raw_data_check_path + '/check_9_1_所有产品每个月金额.csv'\n",
    "check_9_2_path = raw_data_check_path + '/check_9_2_所有产品每个月份额.csv'\n",
    "check_9_3_path = raw_data_check_path + '/check_9_3_所有产品每个月排名.csv'\n",
    "check_10_path = raw_data_check_path + '/check_10_在售产品医院个数.csv'\n",
    "check_11_path = raw_data_check_path + '/check_11_金额_医院贡献率等级.csv'\n",
    "check_12_path = raw_data_check_path + '/check_12_金额_医院分子贡献率等级.csv'\n",
    "check_13_path = raw_data_check_path + '/check_13_数量_医院贡献率等级.csv'\n",
    "check_14_path = raw_data_check_path + '/check_14_数量_医院分子贡献率等级.csv'\n",
    "check_15_path = raw_data_check_path + '/check_15_最近12期每家医院每个月每个产品的价格与倍数.csv'\n",
    "check_16_path = raw_data_check_path + '/check_16_各医院各产品价格与所在地区对比.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_result = spark.read.csv(check_result_path, header=True)\n",
    "check_result = check_result.toPandas()\n",
    "check_1 = spark.read.csv(check_1_path, header=True)\n",
    "check_1 = check_1.toPandas()\n",
    "check_1[check_1.columns[1:]]=check_1[check_1.columns[1:]].astype(float)\n",
    "\n",
    "check_2 = spark.read.csv(check_2_path, header=True)\n",
    "check_2 = check_2.toPandas()\n",
    "check_2[check_2.columns[1:]]=check_2[check_2.columns[1:]].astype(float)\n",
    "\n",
    "check_3 = spark.read.csv(check_3_path, header=True)\n",
    "check_3 = check_3.toPandas()\n",
    "check_3[check_3.columns[1:]]=check_3[check_3.columns[1:]].astype(float)\n",
    "\n",
    "check_5 = spark.read.csv(check_5_path, header=True)\n",
    "check_5 = check_5.toPandas()\n",
    "check_5[check_5.columns[1:-1]]=check_5[check_5.columns[1:-1]].astype(float)\n",
    "\n",
    "check_8 = spark.read.csv(check_8_path, header=True)\n",
    "check_8 = check_8.toPandas()\n",
    "check_8[check_8.columns[1:]]=check_8[check_8.columns[1:]].astype(float)\n",
    "\n",
    "check_9_1 = spark.read.csv(check_9_1_path, header=True)\n",
    "check_9_1 = check_9_1.toPandas()\n",
    "check_9_1[check_9_1.columns[1:]]=check_9_1[check_9_1.columns[1:]].astype(float)\n",
    "\n",
    "check_9_2 = spark.read.csv(check_9_2_path, header=True)\n",
    "check_9_2 = check_9_2.toPandas()\n",
    "check_9_2[check_9_2.columns[1:]]=check_9_2[check_9_2.columns[1:]].astype(float)\n",
    "\n",
    "check_9_3 = spark.read.csv(check_9_3_path, header=True)\n",
    "check_9_3 = check_9_3.toPandas()\n",
    "check_9_3[check_9_3.columns[1:]]=check_9_3[check_9_3.columns[1:]].astype(float)\n",
    "\n",
    "check_10 = spark.read.csv(check_10_path, header=True)\n",
    "check_10 = check_10.toPandas()\n",
    "check_10[check_10.columns[1:]]=check_10[check_10.columns[1:]].astype(float)\n",
    "\n",
    "check_11 = spark.read.csv(check_11_path, header=True)\n",
    "check_11 = check_11.toPandas()\n",
    "check_11[check_11.columns[1:-1]]=check_11[check_11.columns[1:-1]].astype(float)\n",
    "\n",
    "check_12 = spark.read.csv(check_12_path, header=True)\n",
    "check_12 = check_12.toPandas()\n",
    "check_12[check_12.columns[2:-1]]=check_12[check_12.columns[2:-1]].astype(float)\n",
    "\n",
    "check_13 = spark.read.csv(check_13_path, header=True)\n",
    "check_13 = check_13.toPandas()\n",
    "check_13[check_13.columns[1:-1]]=check_13[check_13.columns[1:-1]].astype(float)\n",
    "\n",
    "check_14 = spark.read.csv(check_14_path, header=True)\n",
    "check_14 = check_14.toPandas()\n",
    "check_14[check_14.columns[2:-1]]=check_14[check_14.columns[2:-1]].astype(float)\n",
    "\n",
    "check_15 = spark.read.csv(check_15_path, header=True)\n",
    "check_15 = check_15.toPandas()\n",
    "check_15[check_15.columns[2:]]=check_15[check_15.columns[2:]].astype(float)\n",
    "\n",
    "check_16 = spark.read.csv(check_16_path, header=True)\n",
    "check_16 = check_16.toPandas()\n",
    "check_16[check_16.columns[4:]]=check_16[check_16.columns[4:]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(outdir_local + \"/\" + project_name + \"_\" + outdir + \"_raw_data_check.xlsx\" ) as xlsx:\n",
    "    check_result.to_excel(xlsx, sheet_name=\"check_result\", index=False)\n",
    "    check_1.to_excel(xlsx, sheet_name=\"每个月产品个数\", index=False)\n",
    "    check_2.to_excel(xlsx, sheet_name=\"各产品历史月份销量\", index=False)\n",
    "    check_3.to_excel(xlsx, sheet_name=\"历史医院个数\", index=False)\n",
    "    check_5.to_excel(xlsx, sheet_name=\"最近12期每家医院每个月的金额规模\", index=False)\n",
    "    check_8.to_excel(xlsx, sheet_name=\"每个医院每个月产品个数\", index=False)\n",
    "    check_9_1.to_excel(xlsx, sheet_name=\"所有产品每个月金额\", index=False)\n",
    "    check_9_2.to_excel(xlsx, sheet_name=\"所有产品每个月份额\", index=False)\n",
    "    check_9_3.to_excel(xlsx, sheet_name=\"所有产品每个月排名\", index=False)\n",
    "    check_10.to_excel(xlsx, sheet_name=\"在售产品医院个数\", index=False)\n",
    "    check_11.to_excel(xlsx, sheet_name=\"金额_医院贡献率等级\", index=False)\n",
    "    check_12.to_excel(xlsx, sheet_name=\"金额_医院分子贡献率等级\", index=False)\n",
    "    check_13.to_excel(xlsx, sheet_name=\"数量_医院贡献率等级\", index=False)\n",
    "    check_14.to_excel(xlsx, sheet_name=\"数量_医院分子贡献率等级\", index=False)\n",
    "    check_15.to_excel(xlsx, sheet_name=\"价格_最近12期每家医院每个月每个产品的价格与倍数\", index=False)\n",
    "    check_16.to_excel(xlsx, sheet_name=\"价格_各医院各产品价格与所在地区对比\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
