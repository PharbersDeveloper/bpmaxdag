{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"pretreat\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "history_outdir = 'Empty'\n",
    "raw_data_path = 'Empty'\n",
    "if_two_source = 'False'\n",
    "cut_time_left = 'Empty'\n",
    "cut_time_right = 'Empty'\n",
    "if_union = 'True'\n",
    "test = 'False'\n",
    "auto_max = 'True'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write Rawdata.pretreat in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = '贝达'\n",
    "outdir = '202012'\n",
    "if_two_source = 'True'\n",
    "cut_time_left = '202001'\n",
    "cut_time_right = '202012'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "if if_two_source != \"False\" and if_two_source != \"True\":\n",
    "    phlogger.error('wrong input: if_two_source, False or True') \n",
    "    raise ValueError('wrong input: if_two_source, False or True')\n",
    "    \n",
    "if if_union == 'True':\n",
    "    cut_time_left = int(cut_time_left)\n",
    "    cut_time_right = int(cut_time_right)\n",
    "\n",
    "molecule_adjust_path = max_path + \"/Common_files/新老通用名转换.csv\"\n",
    "\n",
    "if raw_data_path == 'Empty':\n",
    "    raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_upload.csv'\n",
    "\n",
    "if history_outdir == 'Empty':\n",
    "    history_outdir = str(int(outdir) - 1)\n",
    "\n",
    "history_raw_data_path = max_path + '/' + project_name + '/' + history_outdir + '/raw_data'\n",
    "if if_two_source == 'True':\n",
    "    history_raw_data_std_path = max_path + '/' + project_name + '/' + history_outdir + '/raw_data_std'\n",
    "    cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "    cpa_pha_mapping_common_path = max_path + '/Common_files/cpa_pha_mapping'\n",
    "\n",
    "std_names = [\"Date\", \"ID\", \"Raw_Hosp_Name\", \"Brand\", \"Form\", \"Specifications\", \"Pack_Number\", \"Manufacturer\", \n",
    "\"Molecule\", \"Source\", \"Corp\", \"Route\", \"ORG_Measure\"]\n",
    "\n",
    "if project_name == 'Mylan':\n",
    "    std_names = [\"Date\", \"ID\", \"Raw_Hosp_Name\", \"Brand\", \"Form\", \"Specifications\", \"Pack_Number\", \"Manufacturer\", \n",
    "    \"Molecule\", \"Source\", \"Corp\", \"Route\", \"ORG_Measure\", \"min1\", \"Pack_ID\"]\n",
    "\n",
    "# 输出\n",
    "same_sheet_dup_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/same_sheet_dup.csv'\n",
    "across_sheet_dup_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/across_sheet_dup.csv'\n",
    "across_sheet_dup_bymonth_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/across_sheet_dup_bymonth.csv'\n",
    "\n",
    "if test != \"False\" and test != \"True\":\n",
    "    phlogger.error('wrong input: test, False or True') \n",
    "    raise ValueError('wrong input: test, False or True')\n",
    "\n",
    "if test == 'False':\n",
    "    all_raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "    if if_two_source == 'True':\n",
    "        all_raw_data_std_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_std'        \n",
    "else:\n",
    "    all_raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/raw_data'\n",
    "    if if_two_source == 'True':\n",
    "        all_raw_data_std_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/raw_data_std'\n",
    "\n",
    "# 备份原始未修改数据\n",
    "if if_two_source == 'True':\n",
    "    history_raw_data_delivery_path = max_path + '/' + project_name + '/201912/raw_data_std'\n",
    "else:\n",
    "    history_raw_data_delivery_path = max_path + '/' + project_name + '/201912/raw_data'\n",
    "raw_data_delivery_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_delivery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|Sheet|Path|dup_count|count|ratio|\n",
      "+-----+----+---------+-----+-----+\n",
      "+-----+----+---------+-----+-----+\n",
      "\n",
      "同Sheet中重复条目数: None\n"
     ]
    }
   ],
   "source": [
    "# =============  数据执行 ==============\n",
    "raw_data = spark.read.csv(raw_data_path, header=True)    \n",
    "if 'Corp' not in raw_data.columns:\n",
    "    raw_data = raw_data.withColumn('Corp', func.lit(''))\n",
    "if 'Route' not in raw_data.columns:\n",
    "    raw_data = raw_data.withColumn('Route', func.lit(''))\n",
    "for colname, coltype in raw_data.dtypes:\n",
    "    if coltype == \"boolean\":\n",
    "        raw_data = raw_data.withColumn(colname, raw_data[colname].cast(StringType()))\n",
    "\n",
    "# 1. 同sheet去重(两行完全一样的)\n",
    "raw_data = raw_data.groupby(raw_data.columns).count()\n",
    "same_sheet_dup = raw_data.where(raw_data['count'] > 1)\n",
    "\n",
    "# 重复条目数情况\n",
    "describe = same_sheet_dup.groupby('Sheet', 'Path').count() \\\n",
    "                    .withColumnRenamed('count', 'dup_count') \\\n",
    "                    .join(raw_data.groupby('Sheet', 'Path').count(), on=['Sheet', 'Path'], how='left')\n",
    "describe = describe.withColumn('ratio', describe['dup_count']/describe['count'])\n",
    "print(\"同Sheet中重复条目数:\", describe.show())\n",
    "\n",
    "\n",
    "# 同sheet重复条目输出\t\n",
    "if same_sheet_dup.count() > 0:\n",
    "    same_sheet_dup = same_sheet_dup.repartition(1)\n",
    "    same_sheet_dup.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(same_sheet_dup_path)\n",
    "\n",
    "# group计算金额和数量（std_names + ['Sheet', 'Path']）\n",
    "raw_data = raw_data.withColumn('Sales', raw_data['Sales'].cast(DoubleType())) \\\n",
    "                .withColumn('Units', raw_data['Units'].cast(DoubleType())) \\\n",
    "                .withColumn('Units_Box', raw_data['Units_Box'].cast(DoubleType()))\n",
    "raw_data = raw_data.groupby(std_names + ['Sheet', 'Path']).agg(func.sum(raw_data.Sales).alias('Sales'), \n",
    "                                            func.sum(raw_data.Units).alias('Units'), \n",
    "                                            func.sum(raw_data.Units_Box).alias('Units_Box')).persist()\n",
    "\n",
    "# 分子名新旧转换\n",
    "molecule_adjust = spark.read.csv(molecule_adjust_path, header=True)\n",
    "molecule_adjust = molecule_adjust.dropDuplicates([\"Mole_Old\"])\n",
    "\n",
    "raw_data = raw_data.join(molecule_adjust, raw_data['Molecule'] == molecule_adjust['Mole_Old'], how='left').persist()\n",
    "raw_data = raw_data.withColumn(\"S_Molecule\", func.when(raw_data.Mole_New.isNull(), raw_data.Molecule).otherwise(raw_data.Mole_New)) \\\n",
    "                    .drop('Mole_Old', 'Mole_New')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "跨sheet去重，不同月份文件夹来源数据去重情况：\n",
      "重复条目数： 0\n",
      "去重后条目数： 54809\n",
      "总条目数： 54809\n",
      "跨sheet去重，根据金额去重情况：\n",
      "重复条目数： 0\n",
      "去重后条目数： 54809\n",
      "总条目数： 54809\n"
     ]
    }
   ],
   "source": [
    "# 2. 跨sheet去重\n",
    "# 2.1 path来自不同月份文件夹：'Date, S_Molecule' 优先最大月份文件夹来源的数据，生成去重后结果 raw_data_dedup_bymonth\n",
    "\n",
    "# 获取path的日期文件夹名, 纯数字的为文件夹名，只要包含字符就不是月份名\n",
    "@udf(StringType())\n",
    "def path_split(path):\n",
    "    path_month = path.replace('//', '/').split('/')\n",
    "    month = ''\n",
    "    for each in path_month:\n",
    "        if len(re.findall('\\D+', each)) == 0:\n",
    "            month = each\n",
    "    return month\n",
    "\n",
    "dedup_check_bymonth = raw_data.select('Date', 'S_Molecule', 'Path', 'Sheet', 'Source').distinct() \\\n",
    "                            .withColumn('month_dir', path_split(raw_data.Path))\n",
    "dedup_check_bymonth = dedup_check_bymonth.withColumn('month_dir', dedup_check_bymonth.month_dir.cast(IntegerType()))\n",
    "# func.rank() 排名重复占位\n",
    "dedup_check_bymonth = dedup_check_bymonth.withColumn('rank', func.rank().over(Window.partitionBy('Date', 'S_Molecule', 'Source') \\\n",
    "                                                                .orderBy(dedup_check_bymonth['month_dir'].desc()))).persist()\n",
    "dedup_check_bymonth = dedup_check_bymonth.where(dedup_check_bymonth['rank'] == 1).select('Date', 'S_Molecule', 'Path', 'Sheet','Source')\n",
    "\n",
    "# 去重后数据\n",
    "raw_data_dedup_bymonth = raw_data.join(dedup_check_bymonth, on=['Date', 'S_Molecule', 'Path', 'Sheet','Source'], how='inner').persist()\n",
    "# 重复数据\n",
    "across_sheet_dup_bymonth = raw_data.join(dedup_check_bymonth, on=['Date', 'S_Molecule', 'Path', 'Sheet', 'Source'], how='left_anti')\n",
    "\n",
    "print('跨sheet去重，不同月份文件夹来源数据去重情况：')\n",
    "print('重复条目数：', across_sheet_dup_bymonth.count())\n",
    "print('去重后条目数：', raw_data_dedup_bymonth.count())\n",
    "print('总条目数：', raw_data.count())\n",
    "\n",
    "if across_sheet_dup_bymonth.count() > 0:\n",
    "    across_sheet_dup_bymonth = across_sheet_dup_bymonth.repartition(1)\n",
    "    across_sheet_dup_bymonth.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(across_sheet_dup_bymonth_path)\n",
    "\n",
    "# 2.2  保留金额大的数据，生成去重后结果 raw_data_dedup\n",
    "dedup_check_bysales = raw_data_dedup_bymonth.groupby('Date', 'S_Molecule', 'Path', 'Sheet', 'Source') \\\n",
    "                        .agg(func.sum(raw_data_dedup_bymonth.Sales).alias('Sales'), func.sum(raw_data_dedup_bymonth.Sales).alias('Units'))\n",
    "# func.row_number()\n",
    "dedup_check_bysales = dedup_check_bysales.withColumn('rank', func.row_number().over(Window.partitionBy('Date', 'S_Molecule', 'Source') \\\n",
    "                                                                .orderBy(dedup_check_bysales['Sales'].desc()))).persist()\n",
    "dedup_check_bysales = dedup_check_bysales.where(dedup_check_bysales['rank'] == 1).select('Date', 'S_Molecule', 'Path', 'Sheet','Source')\n",
    "\n",
    "# 去重后数据\n",
    "raw_data_dedup = raw_data_dedup_bymonth.join(dedup_check_bysales, on=['Date', 'S_Molecule', 'Path', 'Sheet','Source'], how='inner').persist()\n",
    "# 重复数据\n",
    "across_sheet_dup = raw_data_dedup_bymonth.join(dedup_check_bysales, on=['Date', 'S_Molecule', 'Path', 'Sheet','Source'], how='left_anti')\n",
    "\n",
    "print('跨sheet去重，根据金额去重情况：')\n",
    "print('重复条目数：', across_sheet_dup.count())\n",
    "print('去重后条目数：', raw_data_dedup.count())\n",
    "print('总条目数：', raw_data.count())\n",
    "\n",
    "if across_sheet_dup.count() > 0:\n",
    "    across_sheet_dup = across_sheet_dup.repartition(1)\n",
    "    across_sheet_dup.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(across_sheet_dup_path)\n",
    "\n",
    "# ID 的长度统一\n",
    "def distinguish_cpa_gyc(col, gyc_hospital_id_length):\n",
    "    # gyc_hospital_id_length是国药诚信医院编码长度，一般是7位数字，cpa医院编码一般是6位数字。医院编码长度可以用来区分cpa和gyc\n",
    "    return (func.length(col) < gyc_hospital_id_length)\n",
    "def deal_ID_length(df):\n",
    "    # 不足6位补足\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(distinguish_cpa_gyc(df.ID, 7), func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 跨源去重，跨源去重优先保留CPA医院\n",
    "if if_two_source == 'True':\n",
    "    # drop_dup_hospital\n",
    "    cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "    cpa_pha_mapping = cpa_pha_mapping.where(cpa_pha_mapping[\"推荐版本\"] == 1).select('ID', 'PHA')\n",
    "    cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)\n",
    "\n",
    "    cpa_pha_mapping_common = spark.read.parquet(cpa_pha_mapping_common_path)\n",
    "    cpa_pha_mapping_common = cpa_pha_mapping_common.where(cpa_pha_mapping_common[\"推荐版本\"] == 1).select('ID', 'PHA')\n",
    "    cpa_pha_mapping_common = deal_ID_length(cpa_pha_mapping_common)\n",
    "\n",
    "    # raw_data_dedup = raw_data_dedup.withColumn('ID', raw_data_dedup.ID.cast(IntegerType()))\n",
    "    raw_data_dedup = deal_ID_length(raw_data_dedup)\n",
    "\n",
    "    def drop_dup_hospital(df, cpa_pha_map):\n",
    "        column_names = df.columns\n",
    "        df = df.join(cpa_pha_map, on='ID', how='left')\n",
    "\n",
    "        Source_window = Window.partitionBy(\"Date\", \"PHA\").orderBy(func.col('Source'))\n",
    "        rank_window = Window.partitionBy(\"Date\", \"PHA\").orderBy(func.col('Source').desc())\n",
    "        Source = df.select(\"Date\", \"PHA\", 'Source').distinct() \\\n",
    "            .select(\"Date\", \"PHA\", func.collect_list(func.col('Source')).over(Source_window).alias('Source_list'),\n",
    "            func.rank().over(rank_window).alias('rank')).persist()\n",
    "        Source = Source.where(Source.rank == 1).drop('rank')\n",
    "        Source = Source.withColumn('count', func.size('Source_list'))\n",
    "\n",
    "        df = df.join(Source, on=['Date', 'PHA'], how='left')\n",
    "\n",
    "        # 无重复\n",
    "        df1 = df.where((df['count'] <=1) | (df['count'].isNull()))\n",
    "        # 有重复\n",
    "        df2 = df.where(df['count'] >1)\n",
    "        df2 = df2.withColumn('Source_choice', \n",
    "                        func.when(func.array_contains('Source_list', 'CPA'), func.lit('CPA')) \\\n",
    "                            .otherwise(func.when(func.array_contains('Source_list', 'GYC'), func.lit('GYC')) \\\n",
    "                                            .otherwise(func.lit('DDD'))))\n",
    "        df2 = df2.where(df2['Source'] == df2['Source_choice'])\n",
    "\n",
    "        # 合并\n",
    "        df_all = df1.union(df2.select(df1.columns))\n",
    "\n",
    "        '''\n",
    "        df_filter = df.where(~df.PHA.isNull()).select('Date', 'PHA', 'ID').distinct() \\\n",
    "                                .groupby('Date', 'PHA').count()\n",
    "\n",
    "        df = df.join(df_filter, on=['Date', 'PHA'], how='left')\n",
    "\n",
    "        # 有重复的优先保留CPA（CPA的ID为6位，GYC的ID为7位）,其次是国药，最后是其他\n",
    "        df = df.where((df['count'] <=1) | ((df['count'] > 1) & (func.length('ID')==6)) | (df['count'].isNull()))\n",
    "\n",
    "        '''\n",
    "\n",
    "        # 检查去重结果\n",
    "        check = df_all.select('PHA','ID','Date').distinct() \\\n",
    "                    .groupby('Date', 'PHA').count()\n",
    "        check_dup = check.where(check['count'] >1 ).where(~check.PHA.isNull())\n",
    "        if check_dup.count() > 0 :\n",
    "            print('有未去重的医院')\n",
    "\n",
    "        df_all = df_all.select(column_names)\n",
    "        return df_all\n",
    "\n",
    "    raw_data_dedup_std = drop_dup_hospital(raw_data_dedup, cpa_pha_mapping_common)                  \n",
    "    raw_data_dedup = drop_dup_hospital(raw_data_dedup, cpa_pha_mapping)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 与历史数据合并\n",
    "def union_raw_data(raw_data_dedup, history_raw_data_path, all_raw_data_path):\n",
    "    history_raw_data = spark.read.parquet(history_raw_data_path)\n",
    "    if 'Corp' not in history_raw_data.columns:\n",
    "        history_raw_data = history_raw_data.withColumn('Corp', func.lit(''))\n",
    "    if 'Route' not in history_raw_data.columns:\n",
    "        history_raw_data = history_raw_data.withColumn('Route', func.lit(''))\n",
    "    for colname, coltype in history_raw_data.dtypes:\n",
    "        if coltype == \"boolean\":\n",
    "            history_raw_data = history_raw_data.withColumn(colname, history_raw_data[colname].cast(StringType()))\n",
    "\n",
    "    history_raw_data = history_raw_data.withColumn('Date', history_raw_data.Date.cast(IntegerType()))\n",
    "    history_raw_data = history_raw_data.where(history_raw_data.Date < cut_time_left)\n",
    "    history_raw_data = history_raw_data.drop(\"Brand_new\", \"all_info\")\n",
    "    history_raw_data = deal_ID_length(history_raw_data)\n",
    "\n",
    "    raw_data_dedup = raw_data_dedup.withColumn('Date', raw_data_dedup.Date.cast(IntegerType()))\n",
    "    # new_raw_data = raw_data_dedup.where((raw_data_dedup.Date >= cut_time_left) & (raw_data_dedup.Date <= cut_time_right))\n",
    "    new_raw_data = deal_ID_length(raw_data_dedup)\n",
    "    new_date_mole = new_raw_data.select('Date', 'Molecule', 'ID').distinct()\n",
    "\n",
    "    history_raw_data = history_raw_data.join(new_date_mole, on=['Date', 'Molecule', 'ID'], how='left_anti')\n",
    "\n",
    "    all_raw_data = new_raw_data.select(history_raw_data.columns).union(history_raw_data)    \n",
    "       \n",
    "    all_raw_data = deal_ID_length(all_raw_data)\n",
    "\n",
    "    all_raw_data = all_raw_data.repartition(2)\n",
    "    all_raw_data.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(all_raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与历史数据合并\n",
    "if if_union == 'True':\n",
    "    # 用于max计算\n",
    "    union_raw_data(raw_data_dedup, history_raw_data_path, all_raw_data_path)             \n",
    "    if if_two_source == 'True':\n",
    "        # 用于max计算\n",
    "        union_raw_data(raw_data_dedup_std, history_raw_data_std_path, all_raw_data_std_path)\n",
    "        # 备份生成手动修改前的交付结果\n",
    "        union_raw_data(raw_data_dedup_std, history_raw_data_delivery_path, raw_data_delivery_path)\n",
    "    else:\n",
    "        # 备份生成手动修改前的交付结果\n",
    "        union_raw_data(raw_data_dedup, history_raw_data_delivery_path, raw_data_delivery_path)   \n",
    "# 不与历史数据合并        \n",
    "else:\n",
    "    raw_data_dedup = deal_ID_length(raw_data_dedup)\n",
    "\n",
    "    raw_data_dedup = raw_data_dedup.repartition(2)\n",
    "    raw_data_dedup.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(all_raw_data_path)\n",
    "\n",
    "    if if_two_source == 'True':\n",
    "        raw_data_dedup_std = deal_ID_length(raw_data_dedup_std)\n",
    "\n",
    "        raw_data_dedup_std = raw_data_dedup_std.repartition(2)\n",
    "        raw_data_dedup_std.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(all_raw_data_std_path)\n",
    "        # 备份生成手动修改前的交付结果\n",
    "        raw_data_dedup_std.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(raw_data_delivery_path)\n",
    "    else:\n",
    "        # 备份生成手动修改前的交付结果\n",
    "        raw_data_dedup.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(raw_data_delivery_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
