{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convinced-journey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "21/06/07 10:17:15 INFO RSCDriver: Connecting to: ip-192-168-11-190.cn-northwest-1.compute.internal:10001\n",
      "21/06/07 10:17:15 INFO RSCDriver: Starting RPC server...\n",
      "21/06/07 10:17:15 INFO RpcServer: Connected to the port 10003\n",
      "21/06/07 10:17:15 WARN RSCConf: Your hostname, ip-192-168-11-190.cn-northwest-1.compute.internal, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "21/06/07 10:17:15 WARN RSCConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "21/06/07 10:17:15 INFO RSCDriver: Received job request 51185a90-ef0e-4ad8-957b-577773f84ae5\n",
      "21/06/07 10:17:15 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "21/06/07 10:17:18 INFO SparkEntries: Starting Spark context...\n",
      "21/06/07 10:17:18 INFO SparkContext: Running Spark version 3.0.1-amzn-0\n",
      "21/06/07 10:17:18 INFO ResourceUtils: ==============================================================\n",
      "21/06/07 10:17:18 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "21/06/07 10:17:18 INFO ResourceUtils: ==============================================================\n",
      "21/06/07 10:17:18 INFO SparkContext: Submitted application: livy-session-2\n",
      "21/06/07 10:17:19 INFO SecurityManager: Changing view acls to: livy\n",
      "21/06/07 10:17:19 INFO SecurityManager: Changing modify acls to: livy\n",
      "21/06/07 10:17:19 INFO SecurityManager: Changing view acls groups to: \n",
      "21/06/07 10:17:19 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/06/07 10:17:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "21/06/07 10:17:19 INFO Utils: Successfully started service 'sparkDriver' on port 44059.\n",
      "21/06/07 10:17:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/06/07 10:17:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/06/07 10:17:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/06/07 10:17:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/06/07 10:17:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/06/07 10:17:19 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-200aab07-332e-4c09-abd7-0ccffb20c3b1\n",
      "21/06/07 10:17:19 INFO MemoryStore: MemoryStore started with capacity 400.0 MiB\n",
      "21/06/07 10:17:19 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/06/07 10:17:19 INFO log: Logging initialized @5911ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "21/06/07 10:17:19 INFO Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_292-b10\n",
      "21/06/07 10:17:19 INFO Server: Started @6056ms\n",
      "21/06/07 10:17:19 INFO AbstractConnector: Started ServerConnector@10e0af41{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "21/06/07 10:17:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@364b68c7{/jobs,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2920ef72{/jobs/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1023660d{/jobs/job,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@47152816{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3f89cdba{/stages,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@253b2ad3{/stages/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7169a0ae{/stages/stage,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f437ca8{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d66b883{/stages/pool,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@544f6776{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7803d25{/storage,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17364489{/storage/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f0a88e6{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d06b8f9{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@106c36b7{/environment,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@d7aaa0a{/environment/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@71f13a7b{/executors,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@64246079{/executors/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57862f63{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1981f032{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@375e4d0d{/static,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@fdb13c2{/,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48def95c{/api,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@56a0da0c{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17cb1e23{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "21/06/07 10:17:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-192-168-11-190.cn-northwest-1.compute.internal:4040\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/livy-api-0.7.0-incubating.jar with timestamp 1623032239753\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/livy-rsc-0.7.0-incubating.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/livy-thriftserver-session-0.7.0-incubating.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/netty-all-4.1.17.Final.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/commons-codec-1.9.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.0-incubating.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/livy-core_2.12-0.7.0-incubating.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:19 INFO SparkContext: Added JAR file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.0-incubating.jar at spark://ip-192-168-11-190.cn-northwest-1.compute.internal:44059/jars/livy-repl_2.12-0.7.0-incubating.jar with timestamp 1623032239754\n",
      "21/06/07 10:17:20 INFO RMProxy: Connecting to ResourceManager at ip-192-168-11-190.cn-northwest-1.compute.internal/192.168.11.190:8032\n",
      "21/06/07 10:17:20 INFO Client: Requesting a new application from cluster with 5 NodeManagers\n",
      "21/06/07 10:17:20 INFO Configuration: resource-types.xml not found\n",
      "21/06/07 10:17:20 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "21/06/07 10:17:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24576 MB per container)\n",
      "21/06/07 10:17:20 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "21/06/07 10:17:20 INFO Client: Setting up container launch context for our AM\n",
      "21/06/07 10:17:20 INFO Client: Setting up the launch environment for our AM container\n",
      "21/06/07 10:17:20 INFO Client: Preparing resources for our AM container\n",
      "21/06/07 10:17:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/06/07 10:17:22 INFO Client: Uploading resource file:/mnt/tmp/spark-e655b02d-59bf-42b7-8c96-cf82a516ee85/__spark_libs__7029765698006233881.zip -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/__spark_libs__7029765698006233881.zip\n",
      "21/06/07 10:17:23 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.0-incubating.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/livy-api-0.7.0-incubating.jar\n",
      "21/06/07 10:17:23 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.0-incubating.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/livy-rsc-0.7.0-incubating.jar\n",
      "21/06/07 10:17:23 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.0-incubating.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/livy-thriftserver-session-0.7.0-incubating.jar\n",
      "21/06/07 10:17:23 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/netty-all-4.1.17.Final.jar\n",
      "21/06/07 10:17:24 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/commons-codec-1.9.jar\n",
      "21/06/07 10:17:24 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.0-incubating.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/livy-core_2.12-0.7.0-incubating.jar\n",
      "21/06/07 10:17:24 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.0-incubating.jar -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/livy-repl_2.12-0.7.0-incubating.jar\n",
      "21/06/07 10:17:24 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/hive-site.xml\n",
      "21/06/07 10:17:24 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/sparkr.zip\n",
      "21/06/07 10:17:25 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/pyspark.zip\n",
      "21/06/07 10:17:25 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/py4j-0.10.9-src.zip\n",
      "21/06/07 10:17:25 INFO Client: Uploading resource file:/mnt/tmp/spark-e655b02d-59bf-42b7-8c96-cf82a516ee85/__spark_conf__1512574423033731861.zip -> hdfs://ip-192-168-11-190.cn-northwest-1.compute.internal:8020/user/livy/.sparkStaging/application_1623030986673_0004/__spark_conf__.zip\n",
      "21/06/07 10:17:25 INFO SecurityManager: Changing view acls to: livy\n",
      "21/06/07 10:17:25 INFO SecurityManager: Changing modify acls to: livy\n",
      "21/06/07 10:17:25 INFO SecurityManager: Changing view acls groups to: \n",
      "21/06/07 10:17:25 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/06/07 10:17:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "21/06/07 10:17:25 INFO Client: Submitting application application_1623030986673_0004 to ResourceManager\n",
      "21/06/07 10:17:25 INFO YarnClientImpl: Submitted application application_1623030986673_0004\n",
      "21/06/07 10:17:26 INFO Client: Application report for application_1623030986673_0004 (state: ACCEPTED)\n",
      "21/06/07 10:17:26 INFO Client: .\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impaired-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, StructType, StructType\n",
    "from pyspark.sql.functions import col, date_format, count, isnull, lit\n",
    "from pyspark.sql.functions import when, isnan, udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql import DataFrame    \n",
    "\n",
    "from typing import Iterator\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cross-broadway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "## ====== 输入文件和输出文件 ======\n",
    "\n",
    "# \n",
    "g_whether_save_result = True\n",
    "\n",
    "p_main_dir = \"s3://ph-origin-files/user/zazhao/2020年结果-csv/\"\n",
    "p_patient = p_main_dir + \"病人\"\n",
    "p_detection = p_main_dir + \"检测\"\n",
    "p_data_summary = p_main_dir+\"条目数汇总表-2020.csv\"\n",
    "\n",
    "p_mapping_file = p_main_dir+\"清洗规则/\"\n",
    "p_out_main_dir = p_main_dir+\"输出文件/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## 读取检测数据\n",
    "df_raw_detection = spark.read.csv(p_detection, header=True)\n",
    "# df_raw_detection.show(1)\n",
    "\n",
    "df_raw_detection = df_raw_detection.select([ 'PATIENT_ID', 'VISIT_ID', 'ITEM_NAME', 'SUBJECT', 'REPORT_ITEM_NAME', \n",
    "                                            'RESULT', 'UNITS', 'ABNORMAL_INDICATOR', 'REQUESTED_DATE_TIME', \n",
    "                                            'RESULTS_RPT_DATE_TIME', 'DEPT_NAME'])\n",
    "\n",
    "df_raw_detection = df_raw_detection.withColumn(\"VISIT_ID\", Func.col(\"VISIT_ID\").cast(\"int\"))\\\n",
    "                                    .withColumn(\"REQUESTED_DATE_TIME_STD\",  date_format(\"REQUESTED_DATE_TIME\", \"yyyMMdd\")) \\\n",
    "                                    .withColumn(\"RESULTS_RPT_DATE_TIME_STD\", date_format(\"RESULTS_RPT_DATE_TIME\", \"yyyMMdd\")) \n",
    "# df_raw_detection.where( df_raw_detection[\"VISIT_ID\"].isNull() ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guilty-consortium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_raw_detection = spark.read.csv(p_detection, header=True)\n",
    "# df_raw_detection.show(1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imperial-budapest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## 读取病人数据\n",
    "\n",
    "df_raw_patient = spark.read.csv( p_patient, header=True)\n",
    "\n",
    "old_col = ['省份', '城市', '医院等级', '就诊类型', '医院ID', '患者ID', '就诊序号', \n",
    "        '处方日期', '入院时间', '出院时间', '年龄', \n",
    "        '性别', '医保类型', '诊断', '科室', \n",
    "        '药品名称', '规格', '剂型', '厂家', '金额', '数量', '数量单位']\n",
    "new_col = [\"PROVINCE\", \"CITY\", \"HOSP_LEVEL\", \"TREAMENT_TYPE\", \"HOSP_ID\", \"PATIENT_ID\", \"VISIT_ID\",\n",
    "          \"PRESCRIPTION_DATE\", \"ADMISSION_DATE\" , \"DISCHARGE_DATE\",  \"AGE\", \n",
    "         \"GENDER\", \"HIS_TYPE\", \"DIAGNOISE\", \"df_dept_mapping_NAME\",\n",
    "          \"DRUG_NAME\", \"SPECIFICATION\", \"FORM\", \"MANUFACTURES\", \"MONEY\", \"NUMBER\", \"NUMBER_UNIT\"]\n",
    "\n",
    "df_raw_patient = df_raw_patient.select(old_col)\n",
    "\n",
    "\n",
    "## 去除字符串前后的空格,因为会影响到和其他表间进行匹配\n",
    "df_raw_patient = df_raw_patient.select([Func.trim(col(i)).alias(i)  for i in df_raw_patient.columns])\n",
    "\n",
    "###################### 以下是需要转换列名为英文时才需要 \n",
    "# # 列名标准化\n",
    "# data_patient = data_patient.select( list( map( lambda x:col(x[0]).alias(x[1]),  zip(old_col, new_col) ) ))\n",
    "# # 转换日期格式\n",
    "# data_patient = data_patient.withColumn(\"PRESCRIPTION_DATE_STD\", date_format(\"PRESCRIPTION_DATE\", \"yyyMM\") )\\\n",
    "#                                     .withColumn(\"ADMISSION_DATE_STD\", date_format(\"ADMISSION_DATE\", \"yyyMM\") )\\\n",
    "#                                     .withColumn(\"DISCHARGE_DATE_STD\", date_format(\"DISCHARGE_DATE\", \"yyyMM\") )\n",
    "# ## 年龄转换成数字\n",
    "# df_patient = df_patient.withColumn(\"AGE\", col(\"AGE\").cast(\"int\"))\n",
    "######################\n",
    "\n",
    "## 日期格式转换\n",
    "df_raw_patient = df_raw_patient.withColumn(\"标准处方日期\", date_format(\"处方日期\", \"yyyMMdd\") )\\\n",
    "                                    .withColumn(\"标准入院时间\", date_format(\"入院时间\", \"yyyMMdd\") )\\\n",
    "                                    .withColumn(\"标准出院时间\", date_format(\"出院时间\", \"yyyMMdd\") )\n",
    "\n",
    "## 年龄转换成数字\n",
    "df_raw_patient = df_raw_patient.withColumn(\"年龄\", col(\"年龄\").cast(\"int\"))\\\n",
    "                                .withColumn(\"就诊序号\", col(\"就诊序号\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ba3338-468f-49a7-870d-9d6c2a31d75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a = df_temp_patient.groupBy(\"医院ID\", \"患者ID\", \"就诊序号\").agg(  Func.countDistinct(\"检测日期\") ) \n",
    "# a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "powered-secretariat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd571d79aa24581b1a13348339ece72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://ec2-161-189-170-189.cn-northwest-1.compute.amazonaws.com.cn:8998/sessions/92/statements/7 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "print( df_raw_detection.count(), df_raw_patient.count() )\n",
    "# print(df_raw_detection)\n",
    "# print(df_raw_patient)\n",
    "\n",
    "df_temp_detection = df_raw_detection.select([\"PATIENT_ID\", \"VISIT_ID\", \"RESULTS_RPT_DATE_TIME_STD\"  ] )\\\n",
    "                                        .withColumnRenamed(\"VISIT_ID\", \"就诊序号\") \\\n",
    "                                        .withColumnRenamed(\"PATIENT_ID\", \"患者ID\") \\\n",
    "                                        .withColumnRenamed(\"RESULTS_RPT_DATE_TIME_STD\", \"标准处方日期\") \\\n",
    "                                        .withColumn(\"检测日期\", col(\"标准处方日期\").cast(\"int\")  )\n",
    "df_raw_patient = df_raw_patient.withColumn(\"标准处方日期\", col(\"标准处方日期\").cast(\"int\") )\n",
    "\n",
    "########\n",
    "df_temp_patient = df_raw_patient.join(df_temp_detection,  on= [\"患者ID\", \"就诊序号\", \"标准处方日期\"], how=\"left\")\n",
    "\n",
    "\n",
    "df_temp_patient.count()\n",
    "# win_temp = Window.partitionBy(\"医院ID\", \"患者ID\", \"就诊序号\")\n",
    "\n",
    "# data = df_temp_patient.withColumn(\"数据内容\", Func.when( col(\"标准处方日期\") < col(\"检测日期\"), \"检测前结果\" )\\\n",
    "#                                                   .otherwise(\"检测后结果\").over(win_temp) )\n",
    "# print( df_temp_patient.count() )\n",
    "# df_temp_patient = df_temp_patient.groupBy(\"医院ID\", \"患者ID\", \"就诊序号\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee8d552e-54ed-4d77-a966-251bc80ec8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 11 did not reach idle status in time. Current status is busy.\n"
     ]
    }
   ],
   "source": [
    "# df_raw_patient.groupBy(\"患者ID\").agg( Func.countDistinct('医院ID').alias(\"医院DDDDD\") ).where( col(\"医院DDDDD\")>3).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brown-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------\n",
      " 医院ID   | SH0000130 \n",
      " 省份     | 陕西      \n",
      " 城市     | 西安      \n",
      " 医院等级 | 三甲      \n",
      "only showing top 1 row\n",
      "\n",
      "+---------+----+----+--------+--------+----------+\n",
      "|   医院ID|省份|城市|医院等级|城市等级|新医院等级|\n",
      "+---------+----+----+--------+--------+----------+\n",
      "|SH0000005|北京|北京|    三甲|       1|      三级|\n",
      "|SH0000015|北京|北京|    三甲|       1|      三级|\n",
      "+---------+----+----+--------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## ============== 一. 确定样本医院 =================\n",
    "df_sample_hospital = df_raw_patient.select(['医院ID','省份', '城市', '医院等级']).distinct()\n",
    "\n",
    "city_mapping = {\n",
    "    '北京':'1','上海':'1','深圳':'1','广州':'1',\n",
    "    '成都':'2','杭州':'2','重庆':'2','武汉':'2',\n",
    "    '苏州':'2','西安':'2','天津':'2','南京':'2',\n",
    "    '郑州':'2','长沙':'2','沈阳':'2','青岛':'2',\n",
    "    '宁波':'2','东莞':'2','无锡':'2','昆明':'2',\n",
    "    '大连':'2','厦门':'2','合肥':'2','佛山':'2',\n",
    "    '福州':'2','哈尔滨':'2','济南':'2','温州':'2',\n",
    "    '长春':'2','石家庄':'2','常州':'2','泉州':'2',\n",
    "    '南宁':'2','贵阳':'2','南昌':'2','南通':'2',\n",
    "    '金华':'2','徐州':'2','太原':'2','嘉兴':'2',\n",
    "    '烟台':'2','惠州':'2','保定':'2','台州':'2',\n",
    "    '中山':'2','绍兴':'2','乌鲁木齐':'2','潍坊':'2',\n",
    "    '兰州':'2'\n",
    "}\n",
    "\n",
    "hosp_mapping = {\n",
    "    '特级':'三级','三甲':'三级','二乙':'三级','三丙':'三级',\n",
    "    '二甲':'二级','二乙':'二级','二丙':'二级',\n",
    "    '甲':'一级','乙':'一级','丙':'一级'\n",
    "}\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.SCALAR )\n",
    "def change_city(x : Iterator[ pd.Series ] ):\n",
    "    return x.apply(lambda i:  city_mapping[i] if i in city_mapping else \"null\" )\n",
    "\n",
    "@pandas_udf(\"string\", PandasUDFType.SCALAR )\n",
    "def change_hosp(x : Iterator[ pd.Series ]):\n",
    "    return x.apply(lambda i: hosp_mapping[i] if i in hosp_mapping else \"null\")\n",
    "\n",
    "df_sample_hospital.show(1, vertical=True)\n",
    "df_sample_hospital = df_sample_hospital.withColumn(\"城市等级\",change_city( col(\"城市\" ) ))\n",
    "df_sample_hospital = df_sample_hospital.withColumn(\"新医院等级\",change_hosp( col(\"医院等级\") ))\n",
    "df_sample_hospital = df_sample_hospital.select(['医院ID','省份', '城市', '医院等级', \"城市等级\",\"新医院等级\"]).orderBy(\"医院ID\")\n",
    "df_sample_hospital.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secret-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## 清洗药品名称\n",
    "@pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "def change(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    new_iter = iterator.apply( lambda x: changeSpecification(x) )\n",
    "    return new_iter\n",
    "\n",
    "def changeSpecification(x ):\n",
    "    # 处理读入的字符为空的情况\n",
    "    if x==None:\n",
    "        return \"null\"\n",
    "    elif re.findall( r'.*(莫西沙星).*', x):\n",
    "        new_name = \"莫西沙星\"\n",
    "    elif re.findall( r'.*(左氧氟沙星).*', x):\n",
    "        new_name = \"左氧氟沙星\"\n",
    "    elif re.findall( r'.*(头孢曲松).*', x):\n",
    "        new_name = \"头孢曲松\"\n",
    "    elif re.findall( r'.*(阿奇霉素).*', x):\n",
    "        new_name = \"阿奇霉素\"\n",
    "    elif re.findall( r'.*(多西环素).*', x):\n",
    "        new_name = \"多西环素\"\n",
    "    elif re.findall( r'.*(米诺环素).*', x):\n",
    "        new_name = \"米诺环素\"\n",
    "    elif (re.findall( r'.*(他唑巴坦|他唑邦坦|三唑巴坦|他唑巴).*', x)!=list()) \\\n",
    "            & ( re.findall( r'.*(哌拉西林).*', x)!=list() ):\n",
    "        new_name = \"哌拉西林他唑巴坦纳\"\n",
    "    elif ( re.findall( r'.*(哌拉西林).*', x)!=list() )\\\n",
    "            & ( re.findall( r'.*(舒巴坦).*', x)!=list() ):\n",
    "        new_name = \"哌拉西林舒巴坦纳\"\n",
    "    elif re.findall( r'.*(哌拉西林).*', x):\n",
    "        new_name = \"哌拉西林纳\"\n",
    "    elif ( re.findall( r'.*(头孢哌酮).*', x)!=list() )\\\n",
    "            & ( re.findall( r'.*(舒巴坦).*', x)!=list() ):\n",
    "        new_name = \"头孢哌酮钠舒巴坦钠\"\n",
    "    elif ( re.findall( r'.*(头孢哌酮).*', x)!=list() )\\\n",
    "            & ( re.findall( r'.*(他唑巴坦).*', x)!=list() ):\n",
    "        new_name = \"头孢哌酮钠他唑巴坦钠\"\n",
    "    elif re.findall( r'.*(头孢哌酮).*', x):\n",
    "        new_name = \"头孢哌酮钠\"\n",
    "    elif ( re.findall( r'.*(美洛西林).*', x)!=list() )\\\n",
    "            & ( re.findall( r'.*(舒巴坦).*', x)!=list() ):\n",
    "        new_name = \"美洛西林钠舒巴坦钠\"\n",
    "    elif re.findall( r'.*(美洛西林).*', x):\n",
    "        new_name = \"美洛西林钠\"\n",
    "    elif re.findall( r'.*(依替米星).*', x):\n",
    "        new_name = \"依替米星\"\n",
    "    elif re.findall( r'.*(头孢米诺).*', x):\n",
    "        new_name = \"头孢米诺\"\n",
    "    elif re.findall( r'.*(替加环素).*', x):\n",
    "        new_name = \"替加环素\"\n",
    "    elif re.findall( r'.*(头孢西丁).*', x):\n",
    "        new_name = \"头孢西丁\"\n",
    "    elif re.findall( r'.*(头孢他啶).*', x):\n",
    "        new_name = \"头孢他啶\"\n",
    "    elif re.findall( r'.*(厄他培南).*', x):\n",
    "        new_name = \"厄他培南\"\n",
    "    elif re.findall( r'.*(利奈唑胺).*', x):\n",
    "        new_name = \"利奈唑胺\"    \n",
    "    elif re.findall( r'.*(万古霉素).*', x):\n",
    "        new_name = \"万古霉素\"\n",
    "    elif ( re.findall( r'.*(头孢噻肟).*', x)!=list()) & \\\n",
    "            ( re.findall( r'.*(舒巴坦).*', x)!=list()):\n",
    "        new_name = \"头孢噻肟舒巴坦钠\"\n",
    "    elif re.findall( r'.*(头孢噻肟).*', x):\n",
    "        new_name = \"头孢噻肟钠\"\n",
    "    elif re.findall( r'.*(拉氧头孢).*', x):\n",
    "        new_name = \"拉氧头孢\"\n",
    "    elif re.findall( r'.*(环丙沙星).*', x):\n",
    "        new_name = \"环丙沙星\"\n",
    "    else:\n",
    "        new_name =\"null\"\n",
    "    return new_name\n",
    "\n",
    "# data_temp = data_patient.withColumn(\"DRUG_NAME_STD\", changeSpecification( col(\"DRUG_NAME\"))) \n",
    "# data_patient_drug = data_patient.withColumn(\"标准药品名称\", change( data_patient[\"药品名称\"]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comprehensive-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "## ============== 诊断清洗 ======================\n",
    "\n",
    "###  日期提取\n",
    "\n",
    "### 清洗诊断列\n",
    "@pandas_udf(\"string\" , PandasUDFType.SCALAR)\n",
    "def standDiagnoise(x:pd.Series)->pd.Series:\n",
    "    return x.apply(lambda i: changeDiagnoise(i))\n",
    "\n",
    "def changeDiagnoise(x):\n",
    "    if x==None:\n",
    "        new_x = \"其他\"\n",
    "    elif re.findall(r\".*(肺部感染|肺内感染|肺感染|支原体感染|衣原体感染).*\", x):\n",
    "        new_x = \"肺部感染\"\n",
    "    elif re.findall( r\"(肺炎|肺部炎症)\", x):\n",
    "        new_x = \"肺炎\"\n",
    "    elif re.findall( r\"(社区获得|CAP)\", x) and (re.findall(r\"CPAD\", x)==list() ):\n",
    "        new_x = \"社区获得性肺炎\"\n",
    "    elif re.findall( r\"(呼吸道感染|呼感)\", x):\n",
    "        new_x = \"呼吸道感染\"\n",
    "    elif re.findall( r\"(支气管肺炎)\", x):\n",
    "        new_x = \"支气管肺炎\"\n",
    "    elif re.findall( r\"(气管炎|急支|慢支|支气管周围炎|支炎)\", x):\n",
    "        new_x = \"支气管炎\"\n",
    "    elif re.findall( r\"(上感|上呼吸道感染)\", x):\n",
    "        new_x = \"上呼吸道感染\"\n",
    "    elif re.findall( r\"(扁桃体炎|扁桃体感染|扁桃体周围炎|化扁)\", x):\n",
    "        new_x = \"扁桃体炎\"\n",
    "    elif re.findall( r\"(咽炎|喉炎|咽峡炎|咽部感染|会厌炎)\", x):\n",
    "        new_x = \"咽炎\"\n",
    "    elif re.findall( r\"(流感|流行性感冒|甲流|乙流)\", x):\n",
    "        new_x = \"流感\"\n",
    "    elif re.findall( r\"(蜂窝织炎|蜂窝组织炎|丹毒|坏死性感染|化脓性感染|软组织感染|软组织炎)\", x):\n",
    "        new_x = \"皮肤软组织感染\"\n",
    "    elif re.findall( r\"(皮肤感染|皮炎|皮疹|湿疹|痤疮|毛囊炎|疖|外伤|烧伤|痈|疣)\", x):\n",
    "        new_x = \"其他皮肤病\"\n",
    "    elif re.findall( r\"(结膜炎|角膜炎|LASIK|睑板腺炎|睑腺炎|白内障|中耳炎|耳道炎|牙周炎|鼻炎|冠周炎|龈炎|睑缘炎|鼻窦炎)\", x):\n",
    "        new_x = \"五官类疾病\"\n",
    "    elif re.findall( r\"(胃炎|肠炎|食管炎|幽门螺杆菌感染|腹痛|阑尾炎|胆囊炎|胰腺炎|肠道感染|幽门螺旋杆菌|Hp感染|HP感染|腹泻)\", x):\n",
    "        new_x = \"消化系统感染\"\n",
    "    elif re.findall( r\"(泌尿系感染|尿路感染|尿道炎|前列腺炎|阴道炎|宫颈炎|尿道感染|尿路结石伴感染|盆腔炎)\", x):\n",
    "        new_x = \"泌尿生殖系统感染\"\n",
    "    elif re.findall( r\"(炎|感染)\", x):\n",
    "        new_x = \"其他感染/炎症\"\n",
    "    elif re.findall( r\"(呼吸困难|呼吸衰竭)\", x):\n",
    "        new_x = \"呼吸困难\"\n",
    "    elif re.findall( r\"(发烧|发热)\", x):\n",
    "        new_x = \"发热\"\n",
    "    elif re.findall( r\"(咳痰|有痰)\", x):\n",
    "        new_x = \"咳痰\"\n",
    "    elif re.findall( r\"(咳)\", x):\n",
    "        new_x = \"咳嗽\"\n",
    "    elif re.findall( r\"(感冒)\", x):\n",
    "        new_x = \"普通感冒\"\n",
    "    elif re.findall( r\"(咽痛|喉痛)\", x):\n",
    "        new_x = \"咽痛\"\n",
    "    else:\n",
    "        new_x = \"其他\"\n",
    "    return new_x\n",
    "    \n",
    "df_patient_diagnois =  df_raw_patient.withColumn(\"标准诊断\", standDiagnoise(  col(\"诊断\") )) \n",
    "# df_patient_diagnois.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "growing-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "##  添加新的列\n",
    "df_patient_diagnois_target  = df_patient_diagnois.withColumn(\"心律不齐\",  when( col(\"诊断\").\\\n",
    "                        rlike(r'心率失常|心律失常|心律不齐|心率不齐|心动过速|心动过缓|早搏|房室|QT|房颤|纤颤'), 1).otherwise(0) )\\\n",
    "                .withColumn(\"心衰\", when( col(\"诊断\").rlike('心衰|心力衰竭'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"其他心血管疾病\", when( col(\"诊断\").rlike('心功能|冠心病|冠状|动脉|心梗|心肌|心血管|心绞痛|心脏病'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"脑血管疾病\", when( col(\"诊断\").rlike('脑梗|脑血管|中风|脑血栓|脑出血'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"神经系统疾病\", when( col(\"诊断\").rlike('癫痫|EP|高颅压|颅内压增高|颅内高压|帕金森|阿尔兹海默|'+\\\n",
    "                                                                   '痴呆|神经炎|颅内感染|脑神经损害|脊神经|神经病|周围神经系统'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"高血糖\", when( col(\"诊断\").rlike('高血糖'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"高血压\", when( col(\"诊断\").rlike('高血压'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"高血脂\", when( col(\"诊断\").rlike('高血脂|高脂|胆固醇'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"肝功能异常\", when( col(\"诊断\").rlike('肝炎|肝损|肝功|肝硬|肝病|肝衰|肝纤维|药肝|脂肪肝'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"肾功能异常\", when( col(\"诊断\").rlike('CRF|肾功|肾衰|肾病|透析|肾小管|肾小球|CAPD|尿毒|肾炎'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"结缔组织病\", when( col(\"诊断\").rlike('结缔|风湿|关节炎'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"COPD\", when( col(\"诊断\").rlike('COPD|慢性阻塞性肺|慢阻肺'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"哮喘\", when( col(\"诊断\").rlike('哮喘|哮支'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"支气管扩张\", when( col(\"诊断\").rlike('支气管扩张'), 1 ).otherwise(0))\\\n",
    "                .withColumn(\"恶性实体瘤\", when( ( col(\"诊断\").rlike('癌|恶性肿瘤|恶性瘤|占位|放疗|化疗|CA|原位|转移|黑色素瘤') )\n",
    "                                          &(col(\"诊断\").rlike('CAPD|CAP')== False ) ,  1  ) .otherwise(0)) \\\n",
    "                .withColumn(\"原始诊断字符数\", Func.length( col(\"诊断\") ) )\n",
    "# df_diagnois_with_target.where(col(\"恶性实体瘤\")==1).show(1, False, vertical=True)\n",
    "# df_patient_diagnois_target.show(1,  vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "urban-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "## ======================  清洗医保、科室清洗、医院ID ======================\n",
    "\n",
    "p_dept_mapping = p_mapping_file + \"科室清洗规则-重新划分Hanhui.csv\"\n",
    "df_dept_mapping = spark.read.csv(p_dept_mapping,header=True).withColumnRenamed(\"std_dept\", \"标准科室\")\n",
    "\n",
    "\n",
    "# 科室清洗\n",
    "df_patient_dept = df_patient_diagnois_target.join(df_dept_mapping, on=[\"科室\"], how=\"left\")\n",
    "# print(\"无法匹配到的科室  \", df_raw_patient.join(df_dept_mapping, on=[\"科室\"], how=\"anti\").count() )\n",
    "# left_anti， leftouter\n",
    "\n",
    "\n",
    "# 医保类型清洗\n",
    "p_medical_insurance_mapping = p_mapping_file + \"医保清洗规则.csv\"\n",
    "df_medical_insurance_mapping = spark.read.csv(p_medical_insurance_mapping,header=True) \\\n",
    "                                        .withColumnRenamed(\"std_charge_type\", \"标准医保类型\")\n",
    "# print(df_medical_insurance_mapping.columns)\n",
    "df_patient_std = df_patient_dept.join(df_medical_insurance_mapping,  on=\"医保类型\", how=\"left\")\n",
    "\n",
    "## 为 Null的医保类型转换成 其他\n",
    "df_patient_std = df_patient_std.fillna(\"其他\", subset=[\"医保类型\", \"标准医保类型\"])\n",
    "# df_patient_std.select(\"标准医保类型\").groupBy(\"标准医保类型\").agg( Func.count(\"*\").alias(\"样本数\") ).show()\n",
    "# print(\"无法匹配到的医保类型:  \", df_patient_dept.join(df_medical_insurance_mapping,  on=\"医保类型\", how=\"anti\").count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "final-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "## 输出无法匹配的科室 \n",
    "# temp = df_raw_patient.join(df_dept_mapping, on=[\"科室\"], how=\"anti\").groupBy(\"科室\").agg( Func.count(\"*\").alias(\"样本数\") )\n",
    "# temp.repartition(1).write.mode(\"overwrite\").csv(p_out_main_dir+\"无法匹配到的科室\", sep=',', header=\"true\", encoding=\"utf-8\")\n",
    "\n",
    "# temp = df_patient_dept.join(df_medical_insurance_mapping,  on=\"医保类型\", how=\"anti\")\n",
    "# temp.groupBy(\"医保类型\").agg( Func.count(\"*\").alias(\"样本数\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "circular-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性别清洗\n",
    "\n",
    "# df_patient.select(\"GENDER\").distinct().show()\n",
    "df_patient_std = df_patient_std.withColumn(\"标准性别\",  Func.when(   ~( col(\"性别\")==\"男\") & ( ~(col(\"性别\")==\"女\") ) |\\\n",
    "                                                           col(\"性别\").isNull()\n",
    "                                                           , \"其他\" ) .otherwise( col(\"性别\"))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "consecutive-navigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### 清洗后筛选\n",
    "\n",
    "#1、诊断清洗的筛查\n",
    "# 判断有空的\n",
    "df_patient_null_result =  df_patient_std.select([ Func.count( when(  Func.isnull(c)| col(c).isNull(), c ) ).alias(c) \n",
    "                                            for c in df_patient_std.columns])\n",
    "\n",
    "\n",
    "\n",
    "# 诊断特殊字符 及 空值\n",
    "df_temp = df_patient_std.where(    (col(\"心律不齐\") == 0 ) & (col(\"其他心血管疾病\") == 0 ) & \\\n",
    "                                ( col( \"脑血管疾病\" ) == 0) & ( col(\"神经系统疾病\") == 0 ) & \n",
    "                                ( col(\"高血糖\") == 0 ) & ( col(\"高血压\") == 0 )& \\\n",
    "                                ( col(\"高血脂\") == 0 ) & ( col(\"肝功能异常\") == 0) & \\\n",
    "                                ( col(\"肾功能异常\") == 0) & (col(\"结缔组织病\") == 0) & \\\n",
    "                                ( col(\"COPD\") == 0) & (col(\"哮喘\") == 0 )& \\\n",
    "                                ( col(\"支气管扩张\") == 0 )& ( col(\"恶性实体瘤\") == 0 )& \\\n",
    "                                ( col(\"标准诊断\") == '其他' )&  (col(\"原始诊断字符数\") <= 1) )\n",
    "\n",
    "print( df_temp.count() )\n",
    "\n",
    "# 未清洗的数据\n",
    "##患者层面未能清洗的数据\n",
    "\n",
    "##医保\n",
    "\n",
    "##科室\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "personal-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法匹配到的产品名称   942\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "#  二、药品信息清洗-产品匹配\n",
    "\n",
    "# df_patient = df_patient.dropna()\n",
    "\n",
    "# 读入药品标准表\n",
    "p_drug_mapping = p_mapping_file + \"raw_done.csv\"\n",
    "df_drug_mapping = spark.read.csv(p_drug_mapping, header=True,encoding=\"gbk\")\n",
    "df_drug_mapping = df_drug_mapping.withColumnRenamed(\"pfc\", \"PACK_ID\")\\\n",
    "                                    .withColumnRenamed(\"brand\", \"BRAND\")\\\n",
    "                                    .withColumnRenamed(\"molecule\", \"MOLECULE\")\\\n",
    "                                    .withColumnRenamed(\"for\", \"FORM\")\\\n",
    "                                    .withColumnRenamed(\"spec\", \"SPEC\")\\\n",
    "                                    .withColumnRenamed(\"pack_number\", \"PACK_NUMBER\")\\\n",
    "                                    .withColumnRenamed(\"manufacturer\", \"MANUFACTURER\")\n",
    "# print(df_drug_mapping)\n",
    "# left 方式匹配\n",
    "df_patient_std_ = df_patient_std.join(df_drug_mapping, on=[\"药品名称\", \"规格\", \"剂型\", \"厂家\"], how=\"left\")\n",
    "print(\"无法匹配到的产品名称  \", df_patient_std.join(df_drug_mapping, on=[\"药品名称\", \"规格\", \"剂型\", \"厂家\"], how=\"anti\").count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abstract-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_drug_mapping = p_mapping_file + \"raw_done.csv\"\n",
    "# df_drug_mapping = spark.read.csv(p_drug_mapping, header=True,encoding=\"gbk\")\n",
    "# df_drug_mapping.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bigger-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法被打标签的病人样本:   949319\n"
     ]
    }
   ],
   "source": [
    "# 三、给病人打标签\n",
    "\n",
    "## 原始为feather格式文件,转换为CSV格式\n",
    "p_patient_target = p_mapping_file + \"标签病人层面_测试用.csv\"\n",
    "df_target_patient_mapping = spark.read.csv(p_patient_target, header=True) \\\n",
    "                        .withColumnRenamed(\"PATIENT_ID\", \"患者ID\")\\\n",
    "                        .withColumnRenamed(\"VISIT_ID\", \"就诊序号\")\n",
    "# df_target_patient_mapping = df_target_patient_mapping.withColumn(\"标准处方日期\", date_format(\"处方日期\", \"yyyMMdd\") )\\\n",
    "#                                                         .withColumn(\"标准入院时间\", date_format(\"入院时间\", \"yyyMMdd\") )\\\n",
    "#                                                         .withColumn(\"标准出院时间\", date_format(\"出院时间\", \"yyyMMdd\") )\n",
    "# # df_patient_std_1 = df_patient_std_.join( df_target_patient_mapping, on=[\"医院ID\", \"患者ID\", \"就诊序号\"], how=\"left\")\n",
    "\n",
    "\n",
    "df_target_patient_mapping = df_target_patient_mapping.withColumn(\"标准处方日期\", date_format(\"处方日期\", \"yyyMMdd\") )\\\n",
    "                                                        .withColumn(\"标准入院时间\", date_format(\"入院时间\", \"yyyMMdd\") )\\\n",
    "                                                        .withColumn(\"标准出院时间\", date_format(\"出院时间\", \"yyyMMdd\") ) \\\n",
    "                                                        .drop(\"处方日期\", \"入院时间\", \"出院时间\")\n",
    "df_patient_std_1 = df_patient_std_.join( df_target_patient_mapping, on=[\"医院ID\",\"患者ID\", \"就诊序号\", \n",
    "                                                                 \"标准处方日期\", \"标准入院时间\", \"标准出院时间\" ], how=\"left\")\n",
    "\n",
    "\n",
    "print( \"无法被打标签的病人样本:  \", df_patient_std_.join( df_target_patient_mapping, on=[\"医院ID\",\"患者ID\", \"就诊序号\" ], how=\"anti\").count() )\n",
    "# df_patient_target.show(2, vertical=True)\n",
    "# df_patient_with_target.show(2, vertical=True)\n",
    "# print(df_patient_target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pharmaceutical-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_target_patient_mapping.where( col(\"就诊序号\").isNull() ).count()\n",
    "# print( df_patient_std_.select(\"就诊序号\").join( df_target_patient_mapping.select(\"就诊序号\"),  how=\"anti\").count() )\n",
    "# print(df_patient_std_.join( df_target_patient_mapping, on=[\"患者ID\",  ], how=\"anti\").count() )\n",
    "# df_target_patient_mapping.show(1)\n",
    "\n",
    "# df_patient_std_1.where( col(\"处方日期\").isNull()).count()\n",
    "# df_patient_std_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "double-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_patient_target = p_mapping_file + \"标签病人层面_测试用.csv\"\n",
    "# df_target_patient_mapping = spark.read.csv(p_patient_target, header=True)\n",
    "# df_target_patient_mapping.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "written-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法被匹配到标准分子类别的:   108621\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# 四、分子类别匹配\n",
    "\n",
    "p_molecule = \"s3a://ph-origin-files/user/zazhao/2020年结果-csv/清洗规则/20个分子分类.csv\"\n",
    "df_molecule_class = spark.read.csv(p_molecule, header=True)\\\n",
    "                        .select(['分子名', 'Molecule', 'mole_category'])\\\n",
    "                        .withColumnRenamed(\"Molecule\", \"MOLECULE_OTHER\")\\\n",
    "                        .withColumnRenamed(\"分子名\", \"MOLECULE\")\\\n",
    "                        .withColumnRenamed(\"mole_category\", \"MOLECULE_CATEGORY\")\n",
    "\n",
    "print( \"无法被匹配到标准分子类别的:  \", df_patient_std_.join(df_molecule_class, on=\"MOLECULE\",how=\"anti\" ).count() )\n",
    "df_patient_std_2 =  df_patient_std_1.join(df_molecule_class, on=\"MOLECULE\",how=\"left\" )\n",
    "# df_patient_with_mol_class.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "diagnostic-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patient_std_2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lasting-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 五、标签列生成\n",
    "\n",
    "df_patient_std_3 = df_patient_std_1.withColumn(\"年龄区间\",  Func.when( col(\"年龄\")<8, lit(\"<8\") )\\\n",
    "                                                  .when( (col(\"年龄\") >=8)&(col(\"年龄\") <=14), lit(\"8-14\") )\\\n",
    "                                                  .when( (col(\"年龄\") >=15)&(col(\"年龄\") <=18), lit(\"15-18\") )\\\n",
    "                                                  .when( (col(\"年龄\") >=19)&(col(\"年龄\") <=45), lit(\"19-15\") )\\\n",
    "                                                  .when( (col(\"年龄\") >45)&(col(\"年龄\") <=65), lit(\"46-65\") )\\\n",
    "                                                  .when( (col(\"年龄\") >65), lit(\">65\") ) )\\\n",
    "                                                .withColumn(\"混合感染\", Func.when( ( col(\"鲍曼氏不动杆菌\").contains(\"阳\") )| \\\n",
    "                                                           (col(\"大肠埃希菌\").contains(\"阳\"))| (col(\"肺炎克雷伯菌\").contains(\"阳\"))| \\\n",
    "                                                           (col(\"肺炎链球菌\").contains(\"阳\"))| (col(\"金黄色葡萄球菌\").contains(\"阳\"))| \\\n",
    "                                                           (col(\"流感嗜血菌\").contains(\"阳\"))| (col(\"嗜麦芽寡养单胞菌\").contains(\"阳\"))| \\\n",
    "                                                           (col(\"嗜麦芽窄食单胞菌\").contains(\"阳\"))| (col(\"铜绿假单胞菌\").contains(\"阳\"))| \\\n",
    "                                                           (col(\"阴沟肠杆菌\").contains(\"阳\")),  10).otherwise(0)\n",
    "                                                           )\n",
    "df_patient_std_3 = df_patient_std_3.withColumn(\"混合感染\", Func.when( ( col(\"冠状病毒\").contains(\"阳\") )| \\\n",
    "                                                           (col(\"合胞病毒\").contains(\"阳\"))| (col(\"流感病毒\").contains(\"阳\"))| \\\n",
    "                                                             (col(\"腺病毒\").contains(\"阳\")), col(\"混合感染\")+10 ).otherwise( col(\"混合感染\") )  )  \n",
    "df_patient_std_3 = df_patient_std_3.withColumn(\"混合感染\", Func.when( ( col(\"肺炎支原体\").contains(\"阳\") )| \\\n",
    "                                                           (col(\"肺炎衣原体\").contains(\"阳\"))| (col(\"嗜肺军团菌\").contains(\"阳\")), \\\n",
    "                                                              col(\"混合感染\")+10 ).otherwise( col(\"混合感染\") )  )  \n",
    "# df_patient_with_target_.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "spiritual-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_out_id_mapping = p_mapping_file+\"门诊诊疗周期.csv\"\n",
    "# df_out_id_mapping = spark.read.csv(p_out_id_mapping, header=True)\n",
    "# df_out_id_mapping.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "contemporary-interpretation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+-------------------+-------------------+-------------------+----------+---------+--------+----------+----------+----------+--------+--------+--------+------+----------+--------------+----------+------------+----------+--------------+----------+----------------+----------------+------------+----------+------+\n",
      "|   医院ID|              患者ID|就诊序号|           处方日期|           入院时间|           出院时间|白细胞计数|C反应蛋白|降钙素原|嗜肺军团菌|肺炎衣原体|肺炎支原体|冠状病毒|合胞病毒|流感病毒|腺病毒|柯萨奇病毒|鲍曼氏不动杆菌|大肠埃希菌|肺炎克雷伯菌|肺炎链球菌|金黄色葡萄球菌|流感嗜血菌|嗜麦芽寡养单胞菌|嗜麦芽窄食单胞菌|铜绿假单胞菌|阴沟肠杆菌|OUT_ID|\n",
      "+---------+--------------------+--------+-------------------+-------------------+-------------------+----------+---------+--------+----------+----------+----------+--------+--------+--------+------+----------+--------------+----------+------------+----------+--------------+----------+----------------+----------------+------------+----------+------+\n",
      "|SH0000005|000995071ecaa4278...|       1|2018-09-22 11:03:09|2018-09-21 19:57:26|2018-11-03 08:47:49|      null|        H|    >0.5|        NA|        NA|        NA|      NA|      NA|      NA|    NA|        NA|            NA|        NA|          NA|        NA|            NA|        NA|              NA|              NA|          NA|        NA|     1|\n",
      "+---------+--------------------+--------+-------------------+-------------------+-------------------+----------+---------+--------+----------+----------+----------+--------+--------+--------+------+----------+--------------+----------+------------+----------+--------------+----------+----------------+----------------+------------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "## OUT_ID 匹配\n",
    "p_out_id_mapping = p_mapping_file+\"门诊诊疗周期.csv\"\n",
    "df_out_id_mapping = spark.read.csv(p_out_id_mapping, header=True)\n",
    "\n",
    "# OUT_ID匹配  （对门诊部分14天为1诊疗周期的内容进行生成，代码未找到，但找到了结果文件，可直接进行匹配）\n",
    "# 门诊部分信息\n",
    "df_out_id_mapping = df_out_id_mapping.select([\"HCODE\", \"PATIENT_ID\", \"VISIT_ID\", \"OUT_ID\"]).distinct()\n",
    "df_out_id_mapping = df_out_id_mapping.select([ col(\"HCODE\").alias(\"医院ID\"),col(\"PATIENT_ID\").alias(\"患者ID\"),\n",
    "                                              col(\"VISIT_ID\").alias(\"就诊序号\"),col(\"OUT_ID\")])\n",
    "\n",
    "##########################################  注意此处需要讨论 具体是和 那个表匹配\n",
    "df_tag_out_id = df_target_patient_mapping.drop(\"OUT_ID\")\\\n",
    "                    .join(df_out_id_mapping,on=[\"医院ID\", \"患者ID\", \"就诊序号\"], how=\"left\") \n",
    "# 如果OUT_ID 为空就用 就诊序号代替            \n",
    "df_tag_out_id = df_tag_out_id.withColumn( \"OUT_ID\", when( col(\"OUT_ID\").isNull() ,df_tag_out_id[\"就诊序号\"]) \\\n",
    "                                                    .otherwise( col(\"OUT_ID\") ) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-demonstration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "## 筛选研究范围\n",
    "\n",
    "# df_patient_std_4 = df_patient_std.withColumn(\"其他心血管疾病\",when(col(\"诊断\").rlike('心功能|冠心病|冠状|动脉|心梗|心肌|心血管|心绞痛|心脏病'),1).otherwise(0))\n",
    "#             .withColumn(\"心衰\",when(col(\"'心衰|心力衰竭'\").rlike(match2),1).otherwise(0))\n",
    "#             .withColumn(\"age_range\",when(col(\"年龄\") < 8,\"<8\") \\\n",
    "#                     .when(( col(\"年龄\") >=8) & ( col(\"年龄\") <=14),\"8-14\") \\\n",
    "#                     .when( ( col(\"年龄\") >=15) & ( col(\"年龄\") <=18), \"15-18\") \\\n",
    "#                     .when(( col(\"年龄\") <=19) & ( col(\"年龄\") <=45),\"19-45\") \\\n",
    "#                     .when(( col(\"年龄\") >45) & ( col(\"年龄\") <=65),\"46-65\") \\\n",
    "#                     .when(( col(\"年龄\") >65),\">65\"))\n",
    "\n",
    "df_patient_std_4 =  df_patient_std_4.withColumn(\"uni_code\",df_patient_std_4[\"医院ID\"] + df_patient_std_4[\"患者ID\"])\n",
    "\n",
    "\n",
    "# 使用替加环素的患者 \n",
    "## 及只要使用过 替加环素的患者,就找出来\n",
    "data_part = df_patient_std_4.filter(col(\"molecule\") == \"替加环素\").select(\"uni_code\").distinct()\n",
    "            \n",
    "\n",
    "delivery_base = data.filter(  col(\"标准诊断\").rlike('社区获得性肺炎|肺部感染|呼吸道感染|支气管肺炎|肺炎')) \\  # 筛选患者\n",
    "            .filter(col(\"mole_category\").isNotNull())\\                                               # 筛选目标分子  \n",
    "            .filter(~col(\"剂型\").rlike('滴耳剂|眼用凝胶|滴眼剂|凝胶剂|软膏剂') )                           # 筛选目标剂型\n",
    "\n",
    "## 过滤不含替加环素的患者 的数据\n",
    "delivery_base = delivery_base.join(data_part, on=\"uni_code\", how=\"left_anti\" )\n",
    "\n",
    "## 是否需要筛选住院部分\n",
    "\n",
    "\n",
    "## 是否需要 分类类别 重新定义\n",
    "# delivery_base = delivery_base.withColumn(\"mole_category\",   Func.when(col(\"mole_category\") == \"环素类\",\"四环素类\") \\\n",
    "#                                                                  .otherwise(col(\"mole_category\")) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-sessions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "arbitrary-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "## 添加额外标签\n",
    "df_patient_with_target_ = df_patient_std_3.withColumn(\"细菌感染\", Func.when( (col(\"鲍曼氏不动杆菌\").contains(\"阳\"))|(col(\"大肠埃希菌\").contains(\"阳\"))|\\\n",
    "                                                                 (col(\"肺炎克雷伯菌\").contains(\"阳\")) |(col(\"肺炎链球菌\").contains(\"阳\"))|\\\n",
    "                                                                 (col(\"金黄色葡萄球菌\").contains(\"阳\")) | (col(\"流感嗜血菌\").contains(\"阳\"))|\\\n",
    "                                                                 (col(\"嗜麦芽寡养单胞菌\").contains(\"阳\")) | (col(\"嗜麦芽窄食单胞菌\").contains(\"阳\"))|\\\n",
    "                                                                 (col(\"铜绿假单胞菌\").contains(\"阳\")) | (col(\"阴沟肠杆菌\").contains(\"阳\"))\n",
    "                                                            ,1).otherwise(0) )\\\n",
    "                                .withColumn(\"病毒感染\", Func.when( (col(\"冠状病毒\").contains(\"阳\"))| (col(\"合胞病毒\").contains(\"阳\"))|\\\n",
    "                                                                 (col(\"流感病毒\").contains(\"阳\"))| (col(\"腺病毒\").contains(\"阳\")) \n",
    "                                                            ,1).otherwise(0) )\\\n",
    "                                .withColumn(\"非典型病原菌感染\", Func.when( (col(\"肺炎支原体\").contains(\"阳\"))| (col(\"肺炎衣原体\").contains(\"阳\")) | \\\n",
    "                                                                 (col(\"嗜肺军团菌\").contains(\"阳\"))\n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg1_grp1\", Func.when( (col(\"年龄区间\").contains(\"8-14\")) |(col(\"年龄区间\").contains(\"15-18\") )\n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg1_grp2\", Func.when( (col(\"神经系统疾病\")==1 ) | (col(\"心律不齐\")==1 ) |\\\n",
    "                                                                    (col(\"心衰\")==1 )\n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg2_grp1\", Func.when( (col(\"年龄区间\").contains(\"8-14\")) |(col(\"年龄区间\").contains(\"15-18\") )|\\\n",
    "                                                                    (col(\"心律不齐\")==1 ) | (col(\"心衰\")==1 )| \\\n",
    "                                                                    (col(\"神经系统疾病\")==1 ) | (col(\"肝功能异常\")==1 )| \\\n",
    "                                                                    (col(\"肾功能异常\")==1 ) | (col(\"COPD\")==1 )| \\\n",
    "                                                                    (col(\"恶性实体瘤\")==1 ) \n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg3_grp1\", Func.when( ( (col(\"非典型病原菌感染\")==1 ) & (col(\"细菌感染\")==1 ) )|\\\n",
    "                                                                    ( (col(\"非典型病原菌感染\")==1 ) & (col(\"病毒感染\")==1 ) )\n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg3_grp2\", Func.when( (col(\"嗜麦芽窄食单胞菌\").contains(\"阳\") ) |\\\n",
    "                                                                    (col(\"鲍曼氏不动杆菌\").contains(\"阳\") )| \\\n",
    "                                                                    (col(\"金黄色葡萄球菌\").contains(\"阳\") ) |\n",
    "                                                                    (col(\"大肠埃希菌\").contains(\"阳\") ) \n",
    "                                                            ,1).otherwise(0)) \\\n",
    "                                .withColumn(\"seg3_grp3\", Func.when( col(\"seg3_grp2\").isNull()\n",
    "                                                            ,0).otherwise( col(\"seg3_grp2\"))) \n",
    "                                \n",
    "# df_patient_with_target_.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sunset-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patient_with_target_.show(1, vertical=True)\n",
    "# df_patient_std_2.select(\"MOLECULE_CATEGORY\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-nicholas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spread-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# 单药 和 联药分析\n",
    "# 输入数据是  病人层面表 + 分子标准名称表 + 门诊诊疗周期 + F(额外标签)\n",
    "\n",
    "# 联用种类个数\n",
    "data_temp = df_patient_std_2.select([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"标准处方日期\", \"MOLECULE_CATEGORY\", \"MOLECULE\"])\\\n",
    "                                .withColumn(\"标准处方日期\", col(\"标准处方日期\").cast(\"int\"))\n",
    "\n",
    "df_data_a = df_patient_std_2.withColumn(\"RX_DATE_STD\", col(\"标准处方日期\")) \\\n",
    "                            .groupBy([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"RX_DATE_STD\"])\\\n",
    "                            .agg( Func.countDistinct(\"MOLECULE_CATEGORY\").alias(\"分子种类数\") )\n",
    "\n",
    "# df_data_a.orderBy([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"RX_DATE_STD\"]).show(20)\n",
    "\n",
    "# 联用方式\n",
    "df_data_b = df_patient_std_2.withColumn(\"RX_DATE_STD\", col(\"标准处方日期\")) \\\n",
    "                            .groupBy([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"RX_DATE_STD\"])\\\n",
    "                            .agg(  Func.collect_set(col(\"MOLECULE_CATEGORY\")).alias(\"formula\"), \\\n",
    "                                 Func.collect_set( col(\"MOLECULE\")  ).alias(\"mole_comb\") )\n",
    "df_data_b = df_data_b.withColumn(\"formula\", Func.concat_ws(\"+\", col(\"formula\")) )\\\n",
    "                        .withColumn(\"mole_comb\", Func.concat_ws(\"+\", col(\"mole_comb\")))\n",
    "\n",
    "\n",
    "\n",
    "# 是否为初始药\n",
    "\n",
    "\n",
    "\n",
    "win = Window.partitionBy([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\"]).orderBy( col(\"RX_DATE_STD\").desc() )\n",
    "df_data_c = df_data_a.withColumn(\"SEQ\", Func.row_number().over( win ))\\\n",
    "                        .withColumn(\"IF_FIRST_RX\", when( col(\"SEQ\")==1, 1).otherwise(0) )\n",
    "df_data_c.show()\n",
    "df_data_c_max = df_data_c.groupBy( [\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\" ]).agg(Func.max(\"SEQ\").alias(\"MAX_SEQ\") )\n",
    "df_data_c_max.show()\n",
    "\n",
    "df_data_c = df_data_c.join( df_data_c_max, on=[ \"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\" ], how=\"inner\")\n",
    "df_data_c_max.show()\n",
    "\n",
    "\n",
    "# 合并上面三个表\n",
    "df_data_d = df_data_c.join(df_data_b, on=[\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"RX_DATE_STD\"], how=\"left\")\n",
    "\n",
    "# 是否为换药\n",
    "df_data_e = df_data_d.groupBy([\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\" ])\\\n",
    "                        .agg( Func.countDistinct(\"formula\").alias(\"formula_numbers\") )\\\n",
    "                        .withColumn(\"IF_CHANGE\",  Func.when(col(\"formula_numbers\")>1, 1).otherwise(0))\n",
    "\n",
    "# 合并\n",
    "df_data_f = df_data_d.join( df_data_e, on=[\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\" ], how=\"left\")\n",
    "df_data_f = df_data_f.withColumnRenamed(\"RX_DATE_STD\", \"标准处方日期\" )\n",
    "\n",
    "# 和处方数据进行匹配\n",
    "df_patient_std_2_1 = df_patient_std_2.join(df_data_f, on=[\"医院ID\", \"就诊类型\", \"患者ID\", \"OUT_ID\", \"标准处方日期\"], how=\"left\" )\\\n",
    "                                        .withColumn(\"single_or_formula\", Func.when( col(\"formula\").rlike(\"\\+\")\n",
    "                                                            ,\"联用\").otherwise(\"单药\") )\n",
    "df_patient_std_2_1 = df_patient_std_2_1.withColumn(\"single_or_formula\", Func.when( col(\"formula\").isin(\n",
    "                                                                ['头孢菌素类+头孢菌素类','青霉素类+青霉素类','其他抗生素+其他抗生素',\n",
    "                                                                   '头孢菌素酶抑制剂+头孢菌素酶抑制剂','四环素类+四环素类',\n",
    "                                                                   '氨基糖甙+氨基糖甙','氟喹诺酮+氟喹诺酮']\n",
    "                                                            ),\"单药\").otherwise( col(\"single_or_formula\") ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "changed-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 院内感染患者\n",
    "df_patient_std_pha = df_patient_std_2.filter( col(\"就诊类型\")==\"住院\")\\\n",
    "                        .withColumn(\"标准处方日期\", col(\"标准处方日期\").cast(\"int\"))\\\n",
    "                        .withColumn(\"标准入院时间\", col(\"标准入院时间\").cast(\"int\"))\n",
    "\n",
    "## 初始未使用 抗菌药的患者\n",
    "# temp_data = data.withColumn(\"初始无抗菌药\", Func.when( ( col(\"标准处方日期\")-col(\"标准入院时间\")<=2 ) & \\\n",
    "#                                               ~col(\"MOLECULE_CATEGORY\").isin(molde_class)\n",
    "#                                             , 1).otherwise(0) )\n",
    "# temp_data = temp_data.select(\"医院ID\",  \"就诊类型\", \"患者ID\",\"就诊序号\", \"标准入院时间\", \"初始无抗菌药\").distinct()\n",
    "\n",
    "# data = df_patient_std_2.join(temp_data, on=[\"医院ID\",  \"就诊类型\", \"患者ID\",\"就诊序号\", \"标准入院时间\" ], how=\"left\")\n",
    "\n",
    "# data = data.withColumn(\"PHA患者\", Func.when(  ((col(\"标准处方日期\")-col(\"标准入院时间\")) >2 ) & \\\n",
    "#                                               (col(\"MOLECULE_CATEGORY\").isin(molde_class) ) & \\\n",
    "#                                               ( col(\"初始无抗菌药\")==1 )\n",
    "#                                             , 1).otherwise(0) )\n",
    "# df_patient_std_5 = df_patient_std_2.join(data)\n",
    "\n",
    "\n",
    "# 筛选最小处方时间\n",
    "win_2 = Window.partitionBy(\"医院ID\",  \"就诊类型\", \"患者ID\",\"就诊序号\",\"标准入院时间\")\n",
    "df_patient_std_pha = df_patient_std_pha.withColumn(\"MIN_标准处方日期\", Func.min( col(\"标准处方日期\") ).over(win_2))\\\n",
    "            .withColumn(\"PHA患者\", Func.when((col(\"MIN_标准处方日期\")-col(\"标准入院时间\")>2),1 ).otherwise(0))\\\n",
    "            .drop(\"MIN_标准处方日期\")\n",
    "\n",
    "# df_patient_std_pha.show(1, vertical=True)\n",
    "# print(df_patient_std_pha.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-slave",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
