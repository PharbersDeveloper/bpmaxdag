{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"check\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "minimum_product_sep = '|'\n",
    "minimum_product_columns = 'Brand, Form, Specifications, Pack_Number, Manufacturer'\n",
    "current_year = '2020'\n",
    "current_month = 'Empty'\n",
    "three = '3'\n",
    "twelve = '12'\n",
    "test = 'False'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ENV\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWPBDTVEAEU44ZAGT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"cn-northwest-1\"\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Jupyter Keep-Alive Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, greatest, least, col\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = '贝达'\n",
    "outdir = '202011'\n",
    "current_month = '11'\n",
    "test = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "current_year = int(current_year)\n",
    "current_month = int(current_month)\n",
    "three = int(three)\n",
    "twelve = int(twelve)\n",
    "\n",
    "if minimum_product_sep == \"kong\":\n",
    "    minimum_product_sep = \"\"\n",
    "minimum_product_columns = minimum_product_columns.replace(\" \",\"\").split(\",\")\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "province_city_mapping_path = max_path + '/' + project_name + '/province_city_mapping'\n",
    "if test == 'True':\n",
    "    raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/raw_data'\n",
    "else:\n",
    "    raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "raw_data_check_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/'\n",
    "\n",
    "# 输出\n",
    "check_result_path = raw_data_check_path + '/check_result.csv'\n",
    "check_1_path = raw_data_check_path + '/check_1_每个月产品个数.csv'\n",
    "check_2_path = raw_data_check_path + '/check_2_各产品历史月份销量.csv'\n",
    "check_3_path = raw_data_check_path + '/check_3_历史医院个数.csv'\n",
    "check_5_path = raw_data_check_path + '/check_5_最近12期每家医院每个月的金额规模.csv'\n",
    "check_8_path = raw_data_check_path + '/check_8_每个医院每个月产品个数.csv'\n",
    "check_9_1_path = raw_data_check_path + '/check_9_1_所有产品每个月金额.csv'\n",
    "check_9_2_path = raw_data_check_path + '/check_9_2_所有产品每个月份额.csv'\n",
    "check_9_3_path = raw_data_check_path + '/check_9_3_所有产品每个月排名.csv'\n",
    "check_10_path = raw_data_check_path + '/check_10_在售产品医院个数.csv'\n",
    "check_11_path = raw_data_check_path + '/check_11_金额_医院贡献率等级.csv'\n",
    "check_12_path = raw_data_check_path + '/check_12_金额_医院分子贡献率等级.csv'\n",
    "check_13_path = raw_data_check_path + '/check_13_数量_医院贡献率等级.csv'\n",
    "check_14_path = raw_data_check_path + '/check_14_数量_医院分子贡献率等级.csv'\n",
    "check_15_path = raw_data_check_path + '/check_15_最近12期每家医院每个月每个产品的价格与倍数.csv'\n",
    "check_16_path = raw_data_check_path + '/check_16_各医院各产品价格与所在地区对比.csv'\n",
    "tmp_1_path  = raw_data_check_path + '/tmp_1'\n",
    "tmp_2_path  = raw_data_check_path + '/tmp_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= 数据执行 ==================\t\n",
    "\n",
    "MTH = current_year*100 + current_month\n",
    "PREMTH = MTH - 1\n",
    "\n",
    "# 当前月的前3个月\n",
    "if three > (current_month - 1):\n",
    "    diff = three - current_month\n",
    "    RQMTH = [i for i in range((current_year - 1)*100 +12 - diff , (current_year - 1)*100 + 12 + 1)] + [i for i in range(MTH - current_month + 1 , MTH)]\n",
    "else:\n",
    "    RQMTH = [i for i in range(MTH - current_month + 1 , MTH)][-three:]\n",
    "\n",
    "# 当前月的前12个月\n",
    "if twelve > (current_month - 1):\n",
    "    diff = twelve - current_month\n",
    "    mat_month = [i for i in range((current_year - 1)*100 + 12 - diff , (current_year - 1)*100 + 12 + 1)] + [i for i in range(MTH - current_month + 1 , MTH)]\n",
    "else:\n",
    "    mat_month = [i for i in range(MTH - current_month + 1 , MTH)][-twelve:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_city_mapping = spark.read.parquet(province_city_mapping_path)\n",
    "province_city_mapping = deal_ID_length(province_city_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 一. 数据准备  ==== \n",
    "\n",
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字。\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df\n",
    "\n",
    "# 1. cpa_pha_mapping，province_city_mapping 文件\n",
    "cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "cpa_pha_mapping = cpa_pha_mapping.where(cpa_pha_mapping[\"推荐版本\"] == 1).select('ID', 'PHA').distinct()\n",
    "cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)\n",
    "\n",
    "province_city_mapping = spark.read.parquet(province_city_mapping_path)\n",
    "province_city_mapping = deal_ID_length(province_city_mapping)\n",
    "\n",
    "# 2. Raw_data 处理\n",
    "Raw_data = spark.read.parquet(raw_data_path)\n",
    "Raw_data = Raw_data.withColumn('Date', Raw_data['Date'].cast(IntegerType())) \\\n",
    "                .withColumn('Units', Raw_data['Units'].cast(DoubleType())) \\\n",
    "                .withColumn('Sales', Raw_data['Sales'].cast(DoubleType()))\n",
    "if 'Pack_ID' in Raw_data.columns:\n",
    "    Raw_data = Raw_data.drop('Pack_ID')\n",
    "\n",
    "# 生成min1\n",
    "if project_name != 'Mylan':\n",
    "    Raw_data = Raw_data.withColumn('Brand_bak', Raw_data.Brand)\n",
    "    Raw_data = Raw_data.withColumn('Brand', func.when((Raw_data.Brand.isNull()) | (Raw_data.Brand == 'NA'), Raw_data.Molecule).\n",
    "                                                otherwise(Raw_data.Brand))\n",
    "    Raw_data = Raw_data.withColumn(\"min1\", func.when(Raw_data[minimum_product_columns[0]].isNull(), func.lit(\"NA\")).\n",
    "                                       otherwise(Raw_data[minimum_product_columns[0]]))\n",
    "    for i in minimum_product_columns[1:]:\n",
    "        Raw_data = Raw_data.withColumn(i, Raw_data[i].cast(StringType()))\n",
    "        Raw_data = Raw_data.withColumn(\"min1\", func.concat(\n",
    "            Raw_data[\"min1\"],\n",
    "            func.lit(minimum_product_sep),\n",
    "            func.when(func.isnull(Raw_data[i]), func.lit(\"NA\")).otherwise(Raw_data[i])))\n",
    "    Raw_data = Raw_data.withColumn('Brand', Raw_data.Brand_bak).drop('Brand_bak')\n",
    "\n",
    "# 3. 产品匹配表处理 \n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "# a. 列名清洗统一\n",
    "if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[21], \"pfc\")\n",
    "if project_name == \"Eisai\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[22], \"pfc\")\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "    if i in [\"剂型_标准\", \"Form_std\", \"S_Dosage\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准剂型\")\n",
    "    if i in [\"规格_标准\", \"Specifications_std\", \"药品规格_标准\", \"S_Pack\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准规格\")\n",
    "    if i in [\"包装数量2\", \"包装数量_标准\", \"Pack_Number_std\", \"S_PackNumber\", \"最小包装数量\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准包装数量\")\n",
    "    if i in [\"标准企业\", \"生产企业_标准\", \"Manufacturer_std\", \"S_CORPORATION\", \"标准生产厂家\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准生产企业\")\n",
    "if project_name == \"Janssen\" or project_name == \"NHWA\":\n",
    "    if \"标准剂型\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"剂型\", \"标准剂型\")\n",
    "    if \"标准规格\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"规格\", \"标准规格\")\n",
    "    if \"标准生产企业\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"生产企业\", \"标准生产企业\")\n",
    "    if \"标准包装数量\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"包装数量\", \"标准包装数量\")\n",
    "\n",
    "# b. 选取需要的列\n",
    "product_map = product_map \\\n",
    "                .select(\"min1\", \"min2\", \"pfc\", \"通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \"标准包装数量\", \"标准生产企业\") \\\n",
    "                .distinct() \\\n",
    "                .withColumnRenamed(\"标准商品名\", \"商品名\") \\\n",
    "                .withColumnRenamed(\"标准剂型\", \"剂型\") \\\n",
    "                .withColumnRenamed(\"标准规格\", \"规格\") \\\n",
    "                .withColumnRenamed(\"标准包装数量\", \"包装数量\") \\\n",
    "                .withColumnRenamed(\"标准生产企业\", \"生产企业\") \\\n",
    "                .withColumnRenamed(\"pfc\", \"Pack_ID\")\n",
    "\n",
    "product_map = product_map.withColumn('Pack_ID', product_map.Pack_ID.cast(IntegerType()))\n",
    "\n",
    "# 4. 匹配产品匹配表，标准化min2通用名商品名\n",
    "Raw_data_1 = Raw_data.join(product_map.select('min1','min2','通用名','商品名','Pack_ID').dropDuplicates(['min1']), on='min1', how='left')\n",
    "Raw_data_1 = Raw_data_1.groupby('ID', 'Date', 'min2', '通用名','商品名','Pack_ID') \\\n",
    "                        .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units')) \\\n",
    "                        .withColumnRenamed('min2', 'Prod_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_1 ==========\n",
    "\n",
    "# 每个月产品个数(min2)\n",
    "check_1 = Raw_data_1.select('Date', 'Prod_Name').distinct() \\\n",
    "                    .groupby('Date').count() \\\n",
    "                    .withColumnRenamed('count', '每月产品个数_min2') \\\n",
    "                    .orderBy('Date').persist()\n",
    "\n",
    "### 判断产品个数与上月相比是否超过 8%\n",
    "MTH_product_num = check_1.where(check_1.Date == MTH).toPandas()['每月产品个数_min2'][0]\n",
    "PREMTH_product_num = check_1.where(check_1.Date == PREMTH).toPandas()['每月产品个数_min2'][0]\n",
    "check_result_1 = (MTH_product_num/PREMTH_product_num < 0.08)\n",
    "\n",
    "check_1 = check_1.repartition(1)\n",
    "check_1.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_2 ==========\n",
    "\n",
    "# 各产品历史月份销量\n",
    "check_2 = Raw_data_1.groupby('Date', 'Prod_Name').agg(func.sum('Sales').alias('Sales'))\n",
    "check_2 = check_2.groupBy(\"Prod_Name\").pivot(\"Date\").agg(func.sum('Sales')).persist()\n",
    "\n",
    "### 判断缺失产品是否在上个月销售金额超过 2%\n",
    "MTH_product_Sales = check_2.where(check_2[str(MTH)].isNull()).groupBy().agg(func.sum(str(PREMTH)).alias('sum')).toPandas()['sum'][0]\n",
    "PREMTH_product_Sales = check_2.groupBy().agg(func.sum(str(PREMTH)).alias('sum')).toPandas()['sum'][0]\n",
    "if MTH_product_Sales == None:\n",
    "    MTH_product_Sales = 0\n",
    "check_result_2 = (MTH_product_Sales/PREMTH_product_Sales < 0.08)\n",
    "\n",
    "check_2 = check_2.repartition(1)\n",
    "check_2.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_3 ==========\n",
    "\n",
    "# 历史医院个数\n",
    "check_3 = Raw_data.select('Date', 'ID').distinct() \\\n",
    "                .groupBy('Date').count() \\\n",
    "                .withColumnRenamed('count', '医院个数') \\\n",
    "                .orderBy('Date').persist()\n",
    "\n",
    "### 判断历史医院个数是否超过1%                \n",
    "MTH_hospital_num = check_3.where(check_3.Date == MTH).toPandas()['医院个数'][0]\n",
    "PREMTH_hospital_num = check_3.where(check_3.Date == PREMTH).toPandas()['医院个数'][0]\n",
    "check_result_3 = (MTH_hospital_num/PREMTH_hospital_num -1 < 0.01)\n",
    "\n",
    "check_3 = check_3.repartition(1)\n",
    "check_3.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_5 ==========\n",
    "\n",
    "# 最近12期每家医院每个月的销量规模\n",
    "check_5_1 = Raw_data.where(Raw_data.Date > (current_year - 1)*100 + current_month - 1) \\\n",
    "                    .groupBy('ID', 'Date').agg(func.sum('Sales').alias('Sales'))\n",
    "check_5_1 = check_5_1.groupBy(\"ID\").pivot(\"Date\").agg(func.sum('Sales')).persist() \\\n",
    "                    .orderBy('ID').persist()\n",
    "\n",
    "### 检查当月缺失医院在上个月的销售额占比\n",
    "MTH_hospital_Sales = check_5_1.where(check_5_1[str(MTH)].isNull()).groupBy().agg(func.sum(str(PREMTH)).alias('sum')).toPandas()['sum'][0]\n",
    "PREMTH_hospital_Sales = check_5_1.groupBy().agg(func.sum(str(PREMTH)).alias('sum')).toPandas()['sum'][0]\n",
    "if MTH_hospital_Sales == None:\n",
    "    MTH_hospital_Sales = 0\n",
    "check_result_5 = (MTH_hospital_Sales/PREMTH_hospital_Sales < 0.01)\n",
    "\n",
    "# 每家医院的月销金额在最近12期的误差范围内（mean+-1.96std），范围内的医院数量占比大于95%；\n",
    "check_5_2 = Raw_data.where((Raw_data.Date > (current_year-1)*100+current_month-1 ) & (Raw_data.Date < current_year*100+current_month)) \\\n",
    "                    .groupBy('ID', 'Date').agg(func.sum('Sales').alias('Sales')) \\\n",
    "                    .groupBy('ID').agg(func.mean('Sales').alias('Mean_Sales'), func.stddev('Sales').alias('Sd_Sales')).persist()\n",
    "\n",
    "check_5_2 = check_5_2.join(Raw_data.where(Raw_data.Date == current_year*100+current_month).groupBy('ID').agg(func.sum('Sales').alias('Sales_newmonth')), \n",
    "                                        on='ID', how='left').persist()\n",
    "check_5_2 = check_5_2.withColumn('Check', func.when(check_5_2.Sales_newmonth < check_5_2.Mean_Sales-1.96*check_5_2.Sd_Sales, func.lit('F')) \\\n",
    "                                            .otherwise(func.when(check_5_2.Sales_newmonth > check_5_2.Mean_Sales+1.96*check_5_2.Sd_Sales, func.lit('F')) \\\n",
    "                                                            .otherwise(func.lit('T'))))\n",
    "check_5_2 = check_5_2.withColumn('Check', func.when(func.isnan(check_5_2.Mean_Sales) | func.isnan(check_5_2.Sd_Sales) | check_5_2.Sales_newmonth.isNull(), func.lit(None)) \\\n",
    "                                                .otherwise(check_5_2.Check))                            \n",
    "\n",
    "check_5 = check_5_1.join(check_5_2, on='ID', how='left').orderBy('ID').persist()\n",
    "\n",
    "check_5 = check_5.repartition(1)\n",
    "check_5.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_6 ==========\n",
    "\n",
    "# 最近12期每家医院每个月的销量规模\n",
    "check_6_1 = Raw_data.where(Raw_data.Date > (current_year-1)*100+current_month-1) \\\n",
    "                    .groupby('ID', 'Date').agg(func.sum('Units').alias('Units')) \\\n",
    "                    .groupBy(\"ID\").pivot(\"Date\").agg(func.sum('Units')).persist()\n",
    "\n",
    "# 每家医院的月销数量在最近12期的误差范围内（mean+-1.96std），范围内的医院数量占比大于95%；\n",
    "check_6_2 = Raw_data.where((Raw_data.Date > (current_year-1)*100+current_month-1 ) & (Raw_data.Date < current_year*100+current_month)) \\\n",
    "                    .groupBy('ID', 'Date').agg(func.sum('Units').alias('Units')) \\\n",
    "                    .groupBy('ID').agg(func.mean('Units').alias('Mean_Units'), func.stddev('Units').alias('Sd_Units'))\n",
    "check_6_2 = check_6_2.join(Raw_data.where(Raw_data.Date == current_year*100+current_month).groupBy('ID').agg(func.sum('Units').alias('Units_newmonth')), \n",
    "                                        on='ID', how='left').persist()\n",
    "check_6_2 = check_6_2.withColumn('Check', func.when(check_6_2.Units_newmonth < check_6_2.Mean_Units-1.96*check_6_2.Sd_Units, func.lit('F')) \\\n",
    "                                            .otherwise(func.when(check_6_2.Units_newmonth > check_6_2.Mean_Units+1.96*check_6_2.Sd_Units, func.lit('F')) \\\n",
    "                                                            .otherwise(func.lit('T'))))\n",
    "check_6_2 = check_6_2.withColumn('Check', func.when(func.isnan(check_6_2.Mean_Units) | func.isnan(check_6_2.Sd_Units) | check_6_2.Units_newmonth.isNull(), func.lit(None)) \\\n",
    "                                                .otherwise(check_6_2.Check)) \n",
    "\n",
    "check_6 = check_6_1.join(check_6_2, on='ID', how='left').orderBy('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_7 ==========\n",
    "\n",
    "# 最近12期每家医院每个月每个产品(Packid)的平均价格\n",
    "check_7_1 = Raw_data_1.where(Raw_data_1.Date > (current_year-1)*100+current_month-1) \\\n",
    "                    .groupBy('ID', 'Date', '通用名','商品名','Pack_ID') \\\n",
    "                    .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units')).persist()\n",
    "check_7_1 = check_7_1.withColumn('Price', check_7_1.Sales/check_7_1.Units)\n",
    "check_7_1 = check_7_1.groupBy('ID', '通用名', '商品名', 'Pack_ID').pivot(\"Date\").agg(func.sum('Price')) \\\n",
    "                    .orderBy('ID', '通用名', '商品名').persist()\n",
    "\n",
    "# 每家医院的每个产品单价在最近12期的误差范围内（mean+-1.96std或gap10%以内），范围内的产品数量占比大于95%；\n",
    "check_7_2 = Raw_data_1.where((Raw_data_1.Date > (current_year-1)*100+current_month-1 ) & (Raw_data_1.Date < current_year*100+current_month)) \\\n",
    "                    .groupBy('ID', 'Date', '通用名', '商品名', 'Pack_ID') \\\n",
    "                    .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units'))\n",
    "check_7_2 = check_7_2.withColumn('Price', check_7_2.Sales/check_7_2.Units)\n",
    "check_7_2 = check_7_2.groupBy('ID', '通用名', '商品名', 'Pack_ID') \\\n",
    "                    .agg(func.mean('Price').alias('Mean_Price'), func.stddev('Price').alias('Sd_Price'))\n",
    "check_7_2 = check_7_2.withColumn('Sd_Price', func.when(func.isnan(check_7_2.Sd_Price), func.lit(0)).otherwise(check_7_2.Sd_Price))\n",
    "Raw_data_1_tmp = Raw_data_1.where(Raw_data_1.Date == current_year*100+current_month) \\\n",
    "                            .groupBy('ID', '通用名', '商品名', 'Pack_ID') \\\n",
    "                            .agg(func.sum('Sales').alias('Sales_newmonth'), func.sum('Units').alias('Units_newmonth'))\n",
    "Raw_data_1_tmp = Raw_data_1_tmp.withColumn('Price_newmonth', Raw_data_1_tmp.Sales_newmonth/Raw_data_1_tmp.Units_newmonth)\n",
    "Raw_data_1_tmp = Raw_data_1_tmp.withColumn('Pack_ID', func.when(Raw_data_1_tmp.Pack_ID.isNull(), func.lit(0)).otherwise(Raw_data_1_tmp.Pack_ID))\n",
    "check_7_2 = check_7_2.withColumn('Pack_ID', func.when(check_7_2.Pack_ID.isNull(), func.lit(0)).otherwise(check_7_2.Pack_ID))\n",
    "check_7_2 = check_7_2.join(Raw_data_1_tmp, on=['ID', '通用名', '商品名', 'Pack_ID'], how='left').persist()\n",
    "\n",
    "check_7_2 = check_7_2.withColumn('Check', \\\n",
    "            func.when((check_7_2.Price_newmonth < check_7_2.Mean_Price-1.96*check_7_2.Sd_Price) & (check_7_2.Price_newmonth < check_7_2.Mean_Price*0.9), func.lit('F')) \\\n",
    "                .otherwise(func.when((check_7_2.Price_newmonth > check_7_2.Mean_Price+1.96*check_7_2.Sd_Price) & (check_7_2.Price_newmonth > check_7_2.Mean_Price*1.1), func.lit('F')) \\\n",
    "                                .otherwise(func.lit('T'))))\n",
    "check_7_2 = check_7_2.withColumn('Check', func.when(check_7_2.Sales_newmonth.isNull() | check_7_2.Units_newmonth.isNull(), func.lit(None)) \\\n",
    "                                                .otherwise(check_7_2.Check)) \n",
    "\n",
    "check_7_1 = check_7_1.withColumn('Pack_ID', func.when(check_7_1.Pack_ID.isNull(), func.lit(0)).otherwise(check_7_1.Pack_ID))\n",
    "check_7 = check_7_1.join(check_7_2, on=['ID', '通用名', '商品名', 'Pack_ID'], how='left').orderBy('ID', '通用名', '商品名').persist()\n",
    "check_7 = check_7.withColumn('Pack_ID', func.when(check_7.Pack_ID == 0, func.lit(None)).otherwise(check_7.Pack_ID))\n",
    "\n",
    "check_7.groupby('Check').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_8 ==========\n",
    "\n",
    "# 每个月产品个数\n",
    "check_8 = Raw_data_1.select('Date', 'ID', 'Prod_Name').distinct() \\\n",
    "                    .groupBy('Date').count() \\\n",
    "                    .withColumnRenamed('count', '每月产品个数_min1') \\\n",
    "                    .orderBy('Date').persist()\n",
    "# 最近三个月全部医院的产品总数\n",
    "check_8_1 = Raw_data_1.where(Raw_data_1.Date.isin(RQMTH)) \\\n",
    "                    .select('Date', 'ID', 'Prod_Name').distinct() \\\n",
    "                    .count()/3\n",
    "# 当月月产品个数                  \n",
    "check_8_2 = Raw_data_1.where(Raw_data_1.Date.isin(MTH)) \\\n",
    "                    .select('Date', 'ID', 'Prod_Name').distinct() \\\n",
    "                    .count()\n",
    "\n",
    "check_result_8 = (check_8_2/check_8_1 < 0.03)\n",
    "\n",
    "check_8 = check_8.repartition(1)\n",
    "check_8.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_8_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_9 ==========\n",
    "\n",
    "# 全部医院的全部产品金额、份额、排名与历史月份对比(含缺失医院)\n",
    "check_9_1 = Raw_data_1.groupBy('Date', '商品名').agg(func.sum(Raw_data_1.Sales).alias('Sales')) \\\n",
    "                    .groupBy('商品名').pivot('Date').agg(func.sum('Sales')).persist()\n",
    "check_9_2 = Raw_data_1.groupBy('Date', '商品名').agg(func.sum(Raw_data_1.Sales).alias('Sales')) \\\n",
    "                    .join(Raw_data_1.groupBy('Date').agg(func.sum('Sales').alias('Sales_month')), on='Date', how='left')\n",
    "check_9_2 = check_9_2.withColumn('share', check_9_2.Sales/check_9_2.Sales_month) \\\n",
    "                    .groupBy('商品名').pivot('Date').agg(func.sum('share')).persist()\n",
    "check_9_3 = Raw_data_1.groupBy('Date', '商品名').agg(func.sum(Raw_data_1.Sales).alias('Sales'))\n",
    "check_9_3 = check_9_3.withColumn('Rank', func.row_number().over(Window.partitionBy('Date').orderBy(check_9_3['Sales'].desc())))\n",
    "check_9_3 = check_9_3.groupBy('商品名').pivot('Date').agg(func.sum('Rank')).persist()\n",
    "\n",
    "check_9_1 = check_9_1.repartition(1)\n",
    "check_9_1.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_9_1_path)\n",
    "\n",
    "check_9_2 = check_9_2.repartition(1)\n",
    "check_9_2.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_9_2_path)\n",
    "\n",
    "check_9_3 = check_9_3.repartition(1)\n",
    "check_9_3.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_9_3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_10 ==========\n",
    "\n",
    "# group by 产品和月份，count 医院ID，省份\n",
    "# 检查是否有退市产品突然有销量；例如（17120906_2019M12,Pfizer_HTN）\n",
    "check_10 = Raw_data_1.select('Date', 'Prod_Name', 'ID').distinct() \\\n",
    "                    .groupBy('Date', 'Prod_Name').count() \\\n",
    "                    .withColumnRenamed('count', '在售产品医院个数') \\\n",
    "                    .groupBy('Prod_Name').pivot('Date').agg(func.sum('在售产品医院个数')).persist()\n",
    "\n",
    "check_10 = check_10.repartition(1)\n",
    "check_10.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_10_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_贡献率等级相关 ==========\n",
    "\n",
    "@udf(DoubleType())\n",
    "def mean_adj(*cols):\n",
    "    # 以行为单位，去掉一个最大值和一个最小值求平均\n",
    "    import numpy as np\n",
    "    row_max = cols[0]\n",
    "    row_min = cols[1]\n",
    "    others = cols[2:]\n",
    "    row_mean_list = [x for x in others if x is not None]\n",
    "    if len(row_mean_list) > 3:\n",
    "        row_mean = (np.sum(row_mean_list) - row_max - row_min)/(len(row_mean_list) -2)\n",
    "    else:\n",
    "        row_mean = 0\n",
    "    return float(row_mean)\n",
    "\n",
    "@udf(DoubleType())\n",
    "def min_diff(row_max, row_min, mean_adj):\n",
    "    # row_min 与 mean_adj 的差值\n",
    "    import numpy as np\n",
    "    if mean_adj is not None:\n",
    "        # diff1 = abs(row_max - mean_adj)\n",
    "        diff2 = abs(row_min - mean_adj)\n",
    "        row_diff = diff2\n",
    "    else:\n",
    "        row_diff = 0\n",
    "    return float(row_diff)\n",
    "\n",
    "def func_pandas_cumsum_level(pdf, grouplist, sumcol):\n",
    "    '''\n",
    "    贡献率等级计算:\n",
    "    分月统计\n",
    "    降序排列累加求和，占总数的比值乘10，取整   \n",
    "    '''\n",
    "    month = pdf['Date'][0]\n",
    "    pdf = pdf.groupby(grouplist)[sumcol].agg('sum').reset_index()\n",
    "    pdf = pdf.sort_values(sumcol, ascending=False)\n",
    "    pdf['cumsum'] = pdf[sumcol].cumsum()\n",
    "    pdf['sum'] = pdf[sumcol].sum()\n",
    "    pdf['con_add'] = pdf['cumsum']/pdf['sum']\n",
    "    pdf['level'] = np.where(pdf['con_add']*10 > 10, 10, np.ceil(pdf['con_add']*10))\n",
    "    pdf ['month'] = str(month)\n",
    "    pdf = pdf[grouplist + ['level', 'month']]\n",
    "    pdf['level'].astype('int')\n",
    "    return pdf\n",
    "\n",
    "def colculate_diff(check_num, grouplist):\n",
    "    '''\n",
    "    去掉最大值和最小值求平均\n",
    "    最小值与平均值的差值min_diff\n",
    "    '''\n",
    "    check_num_cols = check_num.columns\n",
    "    check_num_cols = list(set(check_num_cols) - set(grouplist))\n",
    "    for each in check_num_cols:\n",
    "        check_num = check_num.withColumn(each, check_num[each].cast(IntegerType()))\n",
    "\n",
    "    # 平均值计算\n",
    "    check_num = check_num.withColumn(\"row_max\", func.greatest(*check_num_cols)) \\\n",
    "                    .withColumn(\"row_min\", func.least(*check_num_cols))\n",
    "    check_num = check_num.withColumn(\"mean_adj\", \n",
    "                        mean_adj(func.col('row_max'), func.col('row_min'), *(func.col(x) for x in check_num_cols)))\n",
    "    check_num = check_num.withColumn(\"mean_adj\", func.when(func.col('mean_adj') == 0, func.lit(None)).otherwise(func.col('mean_adj')))\n",
    "    # row_min 与 mean_adj 的差值\n",
    "    check_num = check_num.withColumn(\"min_diff\", min_diff(func.col('row_max'), func.col('row_min'), func.col('mean_adj')))\n",
    "    check_num = check_num.withColumn(\"min_diff\", func.when(func.col('mean_adj').isNull(), func.lit(None)).otherwise(func.col('min_diff')))\n",
    "\n",
    "    # 匹配PHA    \n",
    "    check_num = check_num.join(cpa_pha_mapping, on='ID', how='left')    \n",
    "    # 排序\n",
    "    check_num = check_num.orderBy(col('min_diff').desc(), col('mean_adj').desc())\n",
    "    \n",
    "    return check_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Raw_data = deal_ID_length(Raw_data)\n",
    "\n",
    "#========== check_11 金额==========\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_level_11(pdf):\n",
    "    return func_pandas_cumsum_level(pdf, grouplist=['ID'], sumcol='Sales')\n",
    "\n",
    "check_11 = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_level_11)\n",
    "check_11 = check_11.groupby('ID').pivot('month').agg(func.sum('level')).persist()\n",
    "check_11 = colculate_diff(check_11, grouplist=['ID'])\n",
    "\n",
    "check_11 = check_11.repartition(1)\n",
    "check_11.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_11_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_12 金额==========\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"Molecule\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_level_12(pdf):\n",
    "    return func_pandas_cumsum_level(pdf, grouplist=['ID', \"Molecule\"], sumcol='Sales')\n",
    "\n",
    "check_12 = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_level_12)\n",
    "check_12 = check_12.groupby('ID', \"Molecule\").pivot('month').agg(func.sum('level')).persist()\n",
    "check_12 = colculate_diff(check_12, grouplist=['ID', 'Molecule'])\n",
    "\n",
    "check_12 = check_12.repartition(1)\n",
    "check_12.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_12_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_13 数量==========\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_level_13(pdf):\n",
    "    return func_pandas_cumsum_level(pdf, grouplist=['ID'], sumcol='Units')\n",
    "\n",
    "check_13 = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_level_13)\n",
    "check_13 = check_13.groupby('ID').pivot('month').agg(func.sum('level')).persist()\n",
    "check_13 = colculate_diff(check_13, grouplist=['ID'])\n",
    "\n",
    "check_13 = check_13.repartition(1)\n",
    "check_13.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_13_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_14 数量==========\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"Molecule\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_level_12(pdf):\n",
    "    return func_pandas_cumsum_level(pdf, grouplist=['ID', \"Molecule\"], sumcol='Units')\n",
    "\n",
    "check_14 = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_level_12)\n",
    "check_14 = check_14.groupby('ID', \"Molecule\").pivot('month').agg(func.sum('level')).persist()\n",
    "check_14 = colculate_diff(check_14, grouplist=['ID', 'Molecule'])\n",
    "\n",
    "check_14 = check_14.repartition(1)\n",
    "check_14.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_14_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_15 价格==========\n",
    "\n",
    "check_15_a = Raw_data.where(Raw_data.Date > (current_year-1)*100+current_month-1) \\\n",
    "                    .groupBy('ID', 'Date', 'min1') \\\n",
    "                    .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units')) \\\n",
    "                    .withColumn('Price', col('Sales')/col('Units'))\n",
    "check_15_count = check_15_a.groupBy('ID', 'min1').count()\n",
    "check_15_sum = check_15_a.groupBy('ID', 'min1').agg(func.sum('Price').alias('sum'), func.max('Price').alias('max'),  \n",
    "                                                func.min('Price').alias('min'))\n",
    "\n",
    "check_15_a = check_15_a.distinct() \\\n",
    "                    .join(check_15_count, on=['ID', 'min1'], how='left') \\\n",
    "                    .join(check_15_sum, on=['ID', 'min1'], how='left')\n",
    "\n",
    "check_15_a = check_15_a.withColumn('Mean_Price', func.when(col('count') == 1, col('sum')) \\\n",
    "                                               .otherwise((col('sum') - col('min') - col('max'))/(col('count') - 2 )))\n",
    "\n",
    "check_15_b = check_15_a.withColumn('Mean_times', func.when(func.abs(col('Price')) > col('Mean_Price'), func.abs(col('Price'))/col('Mean_Price')) \\\n",
    "                                                   .otherwise(col('Mean_Price')/func.abs(col('Price'))))\n",
    "\n",
    "check_15 = check_15_b.groupby('ID', 'min1').pivot('Date').agg(func.sum('Mean_times')).persist()\n",
    "\n",
    "check_15 = check_15.withColumn('flag', func.greatest(*[i for i in check_15.columns if \"20\" in i]))\n",
    "\n",
    "check_15 = check_15.repartition(1)\n",
    "check_15.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_15_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['201911',\n",
       " '201912',\n",
       " '202001',\n",
       " '202002',\n",
       " '202003',\n",
       " '202004',\n",
       " '202005',\n",
       " '202006',\n",
       " '202007',\n",
       " '202008',\n",
       " '202009',\n",
       " '202010',\n",
       " '202011']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#========== check_16 价格==========\n",
    "check_15_a = Raw_data.where(Raw_data.Date > (current_year-1)*100+current_month-1) \\\n",
    "                    .groupBy('ID', 'Date', 'min1') \\\n",
    "                    .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units')) \\\n",
    "                    .withColumn('Price', col('Sales')/col('Units'))\n",
    "Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, min1: string, 201911: double, 201912: double, 202001: double, 202002: double, 202003: double, 202004: double, 202005: double, 202006: double, 202007: double, 202008: double, 202009: double, 202010: double, 202011: double, flag: double]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_15.withColumn('flag', func.greatest(*[i for i in check_15.columns if \"20\" in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== 汇总检查结果 ==========\n",
    "\n",
    "# 汇总检查结果\n",
    "check_result = spark.createDataFrame(\n",
    "    [('产品个数与历史相差不超过0.08', str(check_result_1)), \n",
    "    ('缺失产品销售额占比不超过0.02', str(check_result_2)), \n",
    "    ('医院个数和历史相差不超过0.01', str(check_result_3)), \n",
    "    ('缺失医院销售额占比不超过0.01', str(check_result_5)), \n",
    "    ('全部医院的全部产品总个数与最近三个月的均值相差不超过0.03', str(check_result_8))], \n",
    "    ('check', 'result'))\n",
    "\n",
    "check_result = check_result.repartition(1)\n",
    "check_result.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_result_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
