{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"job2_factor_raw\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "model_month_right = 'Empty'\n",
    "model_month_left = 'Empty'\n",
    "all_models = 'Empty'\n",
    "max_file = 'Empty'\n",
    "factor_optimize = 'True'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write factor.job2_factor_raw in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''\n",
    "project_name = 'Sanofi'\n",
    "outdir = '202012'\n",
    "model_month_right = '202012'\n",
    "model_month_left = '202001'\n",
    "all_models = 'SNY15,SNY16,SNY17'\n",
    "max_file = 'MAX_result_201801_202012_city_levelh'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model_month_right = int(model_month_right)\n",
    "model_month_left = int(model_month_left)\n",
    "all_models = all_models.replace(' ','').split(',')\n",
    "\n",
    "mkt_mapping_path = max_path + '/' + project_name + '/mkt_mapping'\n",
    "universe_path = max_path + '/' + project_name + '/universe_base'\n",
    "max_result_path = max_path + '/' + project_name + '/' + outdir + '/MAX_result/' + max_file\n",
    "#panel_result_path = max_path + '/' + project_name + '/' + outdir + '/panel_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =========== 数据执行 ============\n",
    "print(\"job2_factor_raw\")\n",
    "mkt_mapping = spark.read.parquet(mkt_mapping_path)\n",
    "universe = spark.read.parquet(universe_path)\n",
    "\n",
    "max_result = spark.read.parquet(max_result_path)\n",
    "max_result = max_result.where((col('Date') >= model_month_left) & (col('Date') <= model_month_right))\n",
    "\n",
    "#panel_result = spark.read.parquet(panel_result_path)\n",
    "#panel_result = panel_result.where((col('Date') >= model_month_left) & (col('Date') <= model_month_right))\n",
    "\n",
    "# 每个市场算 factor\n",
    "for market in all_models:\n",
    "    print(\"当前market为:\" + str(market))\n",
    "    #market = '固力康'\n",
    "    # 输入\n",
    "    rf_out_path = max_path + '/' + project_name + '/forest/' + market + '_rf_result'\n",
    "    # 输出\n",
    "    if factor_optimize == 'True':\n",
    "        factor1_path = max_path + '/' + project_name + '/forest/' + market + '_factor_1'\n",
    "    else:\n",
    "        factor_out_path = max_path + '/' + project_name + '/factor/factor_' + market\n",
    "\n",
    "    # 样本ID\n",
    "    ID_list = universe.where(col('PANEL') == 1).select('Panel_ID').distinct().toPandas()['Panel_ID'].values.tolist()\n",
    "\n",
    "    # panel 样本\n",
    "    '''\n",
    "    panel = panel_result.where(col('DOI') == market)\n",
    "    panel1 = panel.where(col('HOSP_ID').isin(ID_list)) \\\n",
    "                .drop('Province', 'City') \\\n",
    "                .join(universe.select('Panel_ID', 'Province', 'City'), panel.HOSP_ID == universe.Panel_ID, how='inner')\n",
    "    panel1 = panel1.groupBy('City').agg(func.sum('Sales').alias('panel_sales'))\n",
    "    '''\n",
    "\n",
    "    # rf 非样本\n",
    "    rf_out = spark.read.parquet(rf_out_path)\n",
    "    rf_out = rf_out.select('PHA_ID', 'final_sales') \\\n",
    "                    .join(universe.select('Panel_ID', 'Province', 'City').distinct(), \n",
    "                            rf_out.PHA_ID == universe.Panel_ID, how='left') \\\n",
    "                    .where(~col('PHA_ID').isin(ID_list))\n",
    "    rf_out = rf_out.groupBy('City', 'Province').agg(func.sum('final_sales').alias('Sales_rf'))\n",
    "\n",
    "    # max 非样本\n",
    "    spotfire_out = max_result.where(col('DOI') == market)\n",
    "    spotfire_out = spotfire_out.where(col('PANEL') != 1) \\\n",
    "                            .groupBy('City', 'Province').agg(func.sum('Predict_Sales').alias('Sales'))\n",
    "\n",
    "    # 计算factor 城市层面 ： rf 非样本的Sales 除以  max 非样本 的Sales                \n",
    "    factor_city = spotfire_out.join(rf_out, on=['City', 'Province'], how='left')\n",
    "    factor_city = factor_city.withColumn('factor', col('Sales_rf')/col('Sales'))\n",
    "\n",
    "    # universe join left factor_city 没有的城市factor为1\n",
    "    factor_city1 = universe.select('City', 'Province').distinct() \\\n",
    "                            .join(factor_city, on=['City', 'Province'], how='left')\n",
    "    factor_city1 = factor_city1.withColumn('factor', func.when(((col('factor').isNull()) | (col('factor') <=0)), func.lit(1)) \\\n",
    "                                                        .otherwise(col('factor')))\n",
    "    factor_city1 = factor_city1.withColumn('factor', func.when(col('factor') >4, func.lit(4)) \\\n",
    "                                                        .otherwise(col('factor')))\n",
    "\n",
    "    factor_out = factor_city1.select('City', 'Province', 'factor')\n",
    "\n",
    "\n",
    "    if factor_optimize == 'True':\n",
    "        factor_out = factor_out.repartition(1)\n",
    "        factor_out.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(factor1_path)\n",
    "    else:\n",
    "        factor_out = factor_out.repartition(1)\n",
    "        factor_out.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(factor_out_path)        \n",
    "\n",
    "    \n",
    "    print(\"finish:\" + str(market))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
