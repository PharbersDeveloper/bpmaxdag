{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"load\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 10.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "a = 123\n",
    "b = 456\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write yyw_scripts.load in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "月更新数据上传\n",
    "'''    \n",
    "\n",
    "outdir = '202012'\n",
    "product_name = '贝达'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     min1 Molecule Brand Form Specifications  \\\n",
      "0           泰瑞沙|片剂|80mg|30|AstraZeneca AB     奥希替尼   泰瑞沙   片剂           80mg   \n",
      "1           泰瑞沙|片剂|80mg|10|AstraZeneca AB     奥希替尼   泰瑞沙   片剂           80mg   \n",
      "2  易瑞沙|片剂|0.25g|10|AstraZeneca UK Limited     吉非替尼   易瑞沙   片剂          0.25g   \n",
      "3   易瑞沙|片剂|0.25g|1|AstraZeneca UK Limited     吉非替尼   易瑞沙   片剂          0.25g   \n",
      "\n",
      "  Pack_Number            Manufacturer Corp Route                  min2  ...  \\\n",
      "0          30          AstraZeneca AB  NaN    口服   泰瑞沙|片剂|80MG|30|阿斯利康  ...   \n",
      "1          10          AstraZeneca AB  NaN    口服   泰瑞沙|片剂|80MG|10|阿斯利康  ...   \n",
      "2          10  AstraZeneca UK Limited  NaN    口服  易瑞沙|片剂|250MG|10|阿斯利康  ...   \n",
      "3           1  AstraZeneca UK Limited  NaN    口服   易瑞沙|片剂|250MG|1|阿斯利康  ...   \n",
      "\n",
      "  标准剂型   标准规格 标准包装数量 标准生产企业 标准集团 标准途径 model   version      pfc Prd_desc  \n",
      "0   片剂   80MG     30   阿斯利康  NaN  NaN   BD1  20200729  6672202      NaN  \n",
      "1   片剂   80MG     10   阿斯利康  NaN  NaN   BD1  20200729      NaN      NaN  \n",
      "2   片剂  250MG     10   阿斯利康  NaN  NaN   BD1  20200729  1789502      NaN  \n",
      "3   片剂  250MG      1   阿斯利康  NaN  NaN   BD1  20200729      NaN      NaN  \n",
      "\n",
      "[4 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# 产品匹配表\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(u\"/home/ywyuan/tmp_file/产品匹配表_贝达_20200728.xlsx\", dtype=\"object\")\n",
    "df = df.iloc[:,0:22]\n",
    "print(df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in df.columns.values:\n",
    "  df[each] = df[each].astype(\"str\")\n",
    "  \n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "for each in df_spark.columns:\n",
    "  df_spark = df_spark.withColumn(each, func.when(func.col(each) == 'nan', func.lit(None)).otherwise(func.col(each)))\n",
    "\n",
    "df_spark = df_spark.distinct().repartition(1)\n",
    "df_spark.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\").save(\"s3a://ph-max-auto/v0.0.1-2020-06-08/\" + product_name + \"/\" + outdir + \"/prod_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID         PHA Province City\n",
      "0  230461  PHA0000591      安徽省  蚌埠市\n",
      "1  230631  PHA0025117      安徽省  阜阳市\n",
      "2  230011  PHA0005209      安徽省  合肥市\n",
      "3  230021  PHA0005207      安徽省  合肥市\n"
     ]
    }
   ],
   "source": [
    "# province_city_mapping\n",
    "import pandas as pd\n",
    "df = pd.read_excel(u\"/home/ywyuan/tmp_file/province_city_mapping.xlsx\", dtype=\"object\")\n",
    "print(df.head(4))\n",
    "for each in df.columns.values:\n",
    "  df[each] = df[each].astype(\"str\")\n",
    "  \n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "for each in df_spark.columns:\n",
    "  df_spark = df_spark.withColumn(each, func.when(func.col(each) == 'nan', func.lit(None)).otherwise(func.col(each)))\n",
    "\n",
    "df_spark = df_spark.drop('PHA').distinct().repartition(1)\n",
    "df_spark.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\").save(\"s3a://ph-max-auto/v0.0.1-2020-06-08/\" + product_name + \"/province_city_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '202012'\n",
    "product_name = '神州'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(project='神州', path='s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202011/MAX_result/MAX_result_202001_202011_city_level', time_left='202001', time_right='202011'),\n",
       " Row(project='神州', path='s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202010/MAX_result/MAX_result_201801_202010_city_level', time_left='201801', time_right='201912')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  =========  max_result_path_list 调整  =============\n",
    "max_result_path_list = spark.read.csv(\"s3a://ph-max-auto/v0.0.1-2020-06-08/\" + product_name + \"/max_result_path_list.csv\", header=True)\n",
    "max_result_path_list.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+----------+\n",
      "|project|                path|time_left|time_right|\n",
      "+-------+--------------------+---------+----------+\n",
      "|   神州|s3a://ph-max-auto...|   201801|    201912|\n",
      "|   神州|s3a://ph-max-auto...|   202001|    202012|\n",
      "+-------+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_dict = [\n",
    "  {\"project\":\"神州\",\"path\":\"s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202010/MAX_result/MAX_result_201801_202010_city_level\", \n",
    "  \"time_left\":\"201801\", \"time_right\":\"201912\"},\n",
    "  {\"project\":\"神州\",\"path\":\"s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202012/MAX_result/MAX_result_202001_202012_city_level\", \n",
    "  \"time_left\":\"202001\", \"time_right\":\"202012\"}\n",
    "]\n",
    "\n",
    "max_result_path_list_new = spark.createDataFrame(list_dict)\n",
    "max_result_path_list_new = max_result_path_list_new.select(\"project\", \"path\", \"time_left\", \"time_right\")\n",
    "max_result_path_list_new.show()\n",
    "max_result_path_list_new.toPandas()[\"path\"].values\n",
    "max_result_path_list_new = max_result_path_list_new.repartition(1)\n",
    "max_result_path_list_new.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(\"s3a://ph-max-auto/v0.0.1-2020-06-08/\" + product_name + \"/max_result_path_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad01c035a8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 交付\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_delivery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://ph-max-auto/v0.0.1-2020-06-08/贝达/202012/Delivery/贝达_max_delivery_202001_202012.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmax_delivery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_delivery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_delivery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'金额'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'数量'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'包装数量'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'年月'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmax_delivery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'金额'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'数量'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'包装数量'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'年月'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_delivery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/ywyuan/tmp_file/贝达_max_delivery_202001_202012.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, ID: string, Raw_Hosp_Name: string, Brand: string, Form: string, Specifications: string, Pack_Number: string, Manufacturer: string, Molecule: string, Source: string, Sales: string, Units: string, Units_Box: string, PHA: string, PHA医院名称: string, Province: string, City: string, min1: string, DOI: string, 标准通用名: string, 标准商品名: string, 标准剂型: string, 标准规格: string, 标准包装数量: string, 标准生产企业: string, 标准省份名称: string, 标准城市名称: string, PACK_ID: string, ATC: string, project: string, Date_copy: int]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.parquet('s3a://ph-stream/common/public/max_result/0.0.5/rawdata_standard/神州_rawdata_standard')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.repartition(1)\n",
    "df.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save('s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202012/raw_data_delivery.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b1dcd610142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://ph-stream/common/public/max_result/0.0.5/extract_data_out/out_2021_02_24_test4/out_2021_02_24_test4.csv/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
