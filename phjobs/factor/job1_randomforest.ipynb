{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"job1_randomforest\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "model_month_right = 'Empty'\n",
    "model_month_left = 'Empty'\n",
    "all_models = 'Empty'\n",
    "universe_choice = 'Empty'\n",
    "rf_ntree = '500'\n",
    "rf_minnode = '5'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write factor.job1_randomforest in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import time\n",
    "import re\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer, StringIndexer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "project_name = 'Sanofi'\n",
    "outdir = '202012'\n",
    "model_month_right = '202012'\n",
    "model_month_left = '202001'\n",
    "all_models = 'SNY15,SNY16,SNY17'\n",
    "universe_choice = 'SNY15:universe_az_sanofi_mch,SNY16:universe_az_sanofi_mch,SNY17:universe_az_sanofi_mch'\n",
    "rf_ntree = '500'\n",
    "rf_minnode = '5'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_minnode = int(rf_minnode)\n",
    "rf_ntree = int(rf_ntree)\n",
    "model_month_right = int(model_month_right)\n",
    "model_month_left = int(model_month_left)\n",
    "all_models = all_models.replace(' ','').split(',')\n",
    "# 市场的universe文件\n",
    "universe_choice_dict={}\n",
    "if universe_choice != \"Empty\":\n",
    "    for each in universe_choice.replace(\" \",\"\").split(\",\"):\n",
    "        market_name = each.split(\":\")[0]\n",
    "        universe_name = each.split(\":\")[1]\n",
    "        universe_choice_dict[market_name]=universe_name\n",
    "\n",
    "doctor_path = max_path + '/Common_files/factor_files/doctor.csv' \n",
    "BT_PHA_path = max_path + '/Common_files/factor_files/BT_PHA.csv'\n",
    "ind_path = max_path + '/Common_files/factor_files/ind.csv'\n",
    "cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "mkt_mapping_path = max_path + '/' + project_name + '/mkt_mapping'\n",
    "\n",
    "raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "#raw_data_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/Test/Eisai/raw_data.csv'\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======  数据执行  ============\n",
    "\n",
    "# 1. == 文件准备 ==\n",
    "\n",
    "# 1.1  doctor 文件\n",
    "\n",
    "def unpivot(df, keys):\n",
    "    # 功能：数据宽变长\n",
    "    # 参数说明 df:dataframe,  keys 待转换表中需要保留的主键key，以list[]类型传入\n",
    "    # 转换是为了避免字段类不匹配，统一将数据转换为string类型，如果保证数据类型完全一致，可以省略该句\n",
    "    df = df.select(*[col(_).astype(\"string\") for _ in df.columns])\n",
    "    cols = [_ for _ in df.columns if _ not in keys]\n",
    "    stack_str = ','.join(map(lambda x: \"'%s', `%s`\" % (x, x), cols))\n",
    "    # feature, value 转换后的列名，可自定义\n",
    "    df = df.selectExpr(*keys, \"stack(%s, %s) as (feature, value)\" % (len(cols), stack_str))\n",
    "    return df\n",
    "\n",
    "doctor = spark.read.csv(doctor_path, header='True')\n",
    "doctor = doctor.where(~col('BT_Code').isNull()) \\\n",
    "                .select(['Department', 'Dr_N', 'Dr_N_主任', 'Dr_N_副主任', 'Dr_N_主治', 'Dr_N_住院医', 'BT_Code'])\n",
    "doctor_g = unpivot(doctor, ['BT_Code', 'Department']).persist()\n",
    "doctor_g = doctor_g.withColumn('dr1', func.concat(col('Department'), func.lit('_'), col('feature'))) \\\n",
    "                    .groupBy('BT_Code', 'dr1').agg(func.sum('value'))\n",
    "doctor_g = doctor_g.groupBy('BT_Code').pivot('dr1').sum('sum(value)').persist()\n",
    "\n",
    "# 1.2 BT_PHA 文件\n",
    "BT_PHA = spark.read.csv(BT_PHA_path, header='True')\n",
    "BT_PHA = BT_PHA.select('BT', 'PHA').dropDuplicates(['PHA'])\n",
    "\n",
    "# 1.3 ind 文件\n",
    "# 列名有点的无法识别，把点换成_\n",
    "ind = spark.read.csv(ind_path, header='True')\n",
    "ind = ind.toDF(*(re.sub(r'[\\.\\s]+', '_', c) for c in ind.columns))\n",
    "for each in ind.columns[18:]:\n",
    "    ind = ind.withColumn(each, ind[each].cast(DoubleType()))\n",
    "\n",
    "# 1.4 cpa_pha_mapping 文件\n",
    "hosp_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "\n",
    "# 1.5 product_map 文件\n",
    "molecule_map = spark.read.parquet(product_map_path)\n",
    "molecule_map = molecule_map.select('Molecule','标准通用名').distinct()\n",
    "\n",
    "# 1.6 mkt_mapping 文件\n",
    "mkt_mapping = spark.read.parquet(mkt_mapping_path)\n",
    "mkt_mapping = mkt_mapping.withColumnRenamed('mkt', 'Market')\n",
    "molecule_mkt_map = molecule_map.join(mkt_mapping, on='标准通用名', how='left')\n",
    "# molecule_mkt_map.where(molecule_mkt_map.Market.isNull()).select('标准通用名').show()\n",
    "\n",
    "# 1.7 rawdata 数据\n",
    "rawdata = spark.read.parquet(raw_data_path)\n",
    "rawdata = rawdata.withColumn('Date', col('Date').cast(IntegerType()))\n",
    "rawdata = rawdata.where((col('Date') >= model_month_left) & (col('Date') <= model_month_right))\n",
    "rawdata = rawdata.join(molecule_mkt_map, on='Molecule', how='left') \\\n",
    "                    .join(hosp_mapping, on='ID', how='left').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. == 每个市场进行 randomForest 分析 ==\n",
    "\n",
    "# market = '固力康'\n",
    "\n",
    "for market in all_models:\n",
    "    print(\"当前market为:\" + str(market))\n",
    "    # 输入\n",
    "    if market in universe_choice_dict.keys():\n",
    "        universe_path = max_path + '/' + project_name + '/' + universe_choice_dict[market]\n",
    "        hospital_range =  spark.read.parquet(universe_path)\n",
    "    else:\n",
    "        universe_path = max_path + '/' + project_name + '/universe_base'\n",
    "        hospital_range =  spark.read.parquet(universe_path)\n",
    "    # 输出\n",
    "    df_importances_path = max_path + '/' + project_name + '/forest/' + market + '_importances.csv'\n",
    "    df_nmse_path = max_path + '/' + project_name + '/forest/' + market + '_NMSE.csv'\n",
    "    result_path = max_path + '/' + project_name + '/forest/' + market + '_rf_result'\n",
    "\n",
    "    # 2.1 数据处理    \n",
    "    all_hospital = hospital_range.where(hospital_range.BEDSIZE > 99 ).select('Panel_ID').distinct()\n",
    "    rawdata_mkt = rawdata.where(rawdata.Market == market) \\\n",
    "                            .withColumnRenamed('PHA', 'PHA_ID') \\\n",
    "                            .withColumnRenamed('Market', 'DOI')\n",
    "\n",
    "    hospital_sample = hospital_range.where(hospital_range.PANEL == 1).select('Panel_ID').distinct().toPandas()['Panel_ID'].values.tolist()\n",
    "    rawdata_sample = rawdata_mkt.where(col('PHA_ID').isin(hospital_sample)) \\\n",
    "                            .groupBy('PHA_ID', 'DOI').agg(func.sum('Sales').alias('Sales'))\n",
    "\n",
    "    ind_mkt = ind.join(hospital_range.select('Panel_ID').distinct(), ind['PHA_ID']==hospital_range['Panel_ID'], how='inner')\n",
    "\n",
    "    # 2.2 计算 ind_mkt 符合条件的列\n",
    "    # 去掉不用的列名\n",
    "    drop_cols = [\"Panel_ID\", \"PHA_Hosp_name\", \"PHA_ID\", \"Bayer_ID\", \"If_Panel\", \"Segment\", \"Segment_Description\", \"If_County\"]\n",
    "    all_cols = list(set(ind_mkt.columns)-set(drop_cols))\n",
    "    new_all_cols = []\n",
    "    for each in all_cols:\n",
    "        if len(re.findall('机构|省|市|县|医院级别|医院等次|医院类型|性质|地址|邮编|年诊疗|总收入|门诊药品收入|住院药品收入|总支出', each)) == 0:\n",
    "            new_all_cols.append(each)\n",
    "\n",
    "    # 计算每列非空行数\n",
    "    df_agg = ind_mkt.agg(*[func.count(func.when(~func.isnull(c), c)).alias(c) for c in new_all_cols]).persist()\n",
    "    # 转置为长数据\n",
    "    df_agg_col = df_agg.toPandas().T\n",
    "    df_agg_col.columns = [\"notNULL_Count\"]\n",
    "    df_agg_col_names = df_agg_col[df_agg_col.notNULL_Count >= 15000].index.tolist()\n",
    "\n",
    "    '''\n",
    "    from functools import reduce\n",
    "    df_agg_col = reduce(\n",
    "        lambda a, b: a.union(b),\n",
    "        (df_agg.select(func.lit(c).alias(\"Column_Name\"), func.col(c).alias(\"notNULL_Count\")) \n",
    "            for c in df_agg.columns)\n",
    "    ).persist()\n",
    "    df_agg_col = df_agg_col.where(col('notNULL_Count') >= 15000)\n",
    "    df_agg_col_names = df_agg_col.toPandas()['Column_Name'].values\n",
    "    '''\n",
    "\n",
    "    # 2.3 ind_mkt 文件处理\n",
    "    ind2 = ind_mkt.select('PHA_ID', *df_agg_col_names, *[i for i in ind_mkt.columns if '心血管' in i ])\n",
    "    ind3 = ind2.join(BT_PHA, ind2.PHA_ID==BT_PHA.PHA, how='left') \\\n",
    "                .drop('PHA')\n",
    "    ind4 = ind3.join(doctor_g, ind3.BT==doctor_g.BT_Code, how='left') \\\n",
    "                .drop('BT_Code')\n",
    "    ind5 = ind4.select(*ind2.columns, *[i for i in ind4.columns if '_Dr_N_' in i ])\n",
    "\n",
    "    num_cols = list(set(ind5.columns) -set(['PHA_ID', 'Hosp_level', 'Region', 'respailty', 'Province', 'Prefecture', \n",
    "                                        'City_Tier_2010', 'Specialty_1', 'Specialty_2', 'Re_Speialty', 'Specialty_3']))\n",
    "    ind5 = ind5.fillna(0, subset=num_cols) \n",
    "\n",
    "\n",
    "    # %%\n",
    "    def f1(x):\n",
    "        y=(x+0.001)**(1/2)\n",
    "        return(y)\n",
    "\n",
    "    def f2(x):\n",
    "        y=x**(2)-0.001\n",
    "        return(y)\n",
    "\n",
    "    # 2.4 获得 modeldata \n",
    "    modeldata = ind5.join(rawdata_sample, on='PHA_ID', how='left')\n",
    "    Panel_ID_list = hospital_range.where(hospital_range.PANEL == 1).select('Panel_ID').toPandas()['Panel_ID'].values.tolist()\n",
    "\n",
    "    modeldata = modeldata.withColumn('flag_model', \n",
    "                            func.when(modeldata.PHA_ID.isin(Panel_ID_list), func.lit('TRUE')) \\\n",
    "                                .otherwise(func.lit('FALSE'))).persist()\n",
    "    modeldata = modeldata.withColumn('Sales', func.when((col('Sales').isNull()) & (col('flag_model')=='TRUE'), func.lit(0)) \\\n",
    "                                                 .otherwise(col('Sales')))\n",
    "    modeldata = modeldata.withColumn('v', func.when(col('Sales') > 0, f1(col('Sales'))).otherwise(func.lit(0)))\n",
    "\n",
    "    trn = modeldata.where(modeldata.PHA_ID.isin(Panel_ID_list))\n",
    "\n",
    "    # 2.5 ===  随机森林 ===\n",
    "    # 1. 数据准备\n",
    "    print(\"RandomForest：data prepare\")\n",
    "    not_features_cols = [\"PHA_ID\", \"Province\", \"Prefecture\", \"Specialty_1\", \"Specialty_2\", \"Specialty_3\", \"DOI\", \"Sales\", \"flag_model\"]\n",
    "    features_str_cols = ['Hosp_level', 'Region', 'respailty', 'Re_Speialty', 'City_Tier_2010']\n",
    "    features_cols = list(set(trn.columns) -set(not_features_cols)- set(features_str_cols) - set('v'))\n",
    "\n",
    "    def data_for_forest(data):\n",
    "        # 使用StringIndexer，将features中的字符型变量转为分类数值变量\n",
    "        indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(data) for column in features_str_cols]\n",
    "        pipeline = Pipeline(stages=indexers)\n",
    "        data = pipeline.fit(data).transform(data)\n",
    "        # 使用 VectorAssembler ，将特征合并为features\n",
    "        assembler = VectorAssembler( \\\n",
    "             inputCols = features_cols, \\\n",
    "             outputCol = \"features\")\n",
    "        data = assembler.transform(data)\n",
    "        data = data.withColumnRenamed('v', 'label')\n",
    "        # 识别哪些是分类变量，Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "        featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=15).fit(data)\n",
    "        data = featureIndexer.transform(data)\n",
    "        return data\n",
    "\n",
    "    data = data_for_forest(trn)\n",
    "\n",
    "    # 2. 随机森林模型\n",
    "    print(\"RandomForest：model\")\n",
    "    rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", \n",
    "            numTrees=500, minInstancesPerNode=5, maxDepth=8)\n",
    "    # numTrees=100, minInstancesPerNode=2, maxDepth=8\n",
    "    model = rf.fit(data)\n",
    "\n",
    "    # 特征重要性\n",
    "    print(\"RandomForest：importances\")\n",
    "    dp = model.featureImportances\n",
    "    dendp = DenseVector(dp)\n",
    "    df_importances = pd.DataFrame(dendp.array)\n",
    "    df_importances['feature'] = features_cols\n",
    "    df_importances.columns=['importances','feature']  \n",
    "    df_importances = df_importances.sort_values(by='importances', ascending=False)\n",
    "    df_importances = spark.createDataFrame(df_importances)\n",
    "\n",
    "    df_importances = df_importances.repartition(1)\n",
    "    df_importances.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(df_importances_path)\n",
    "\n",
    "\n",
    "    # 3. 对 modeldata预测\n",
    "    print(\"RandomForest：predict result\")\n",
    "    result = data_for_forest(modeldata)  \n",
    "    result = result.withColumn('DOI', func.lit(market))\n",
    "    result = model.transform(result)\n",
    "    result = result.withColumn('sales_from_model', f2(col('prediction'))) \\\n",
    "            .withColumn('training_set', func.when(col('PHA_ID').isin(trn.select('PHA_ID').distinct().toPandas()['PHA_ID'].values.tolist()), \n",
    "                                                func.lit(1)) \\\n",
    "                                            .otherwise(func.lit(0)))\n",
    "    result = result.withColumn('final_sales', func.when(col('flag_model')== 'TRUE', col('Sales')) \\\n",
    "                                                .otherwise(col('sales_from_model')))\n",
    "    result = result.where(col('PHA_ID').isin(all_hospital.toPandas()['Panel_ID'].values.tolist()))\n",
    "\n",
    "    result = result.repartition(1)\n",
    "    result.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(result_path)\n",
    "\n",
    "    print(\"RandomForest：finish\")\n",
    "\n",
    "    # 4. 评价模型（5次随机森林，计算nmse）\n",
    "    '''\n",
    "    print(\"nmse：RandomForest\")\n",
    "    for i in range(1,6):\n",
    "        # 模型构建\n",
    "        (df_training, df_test) = data.randomSplit([0.7, 0.3])\n",
    "        rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", \n",
    "                numTrees=100, minInstancesPerNode=2, maxDepth=i)\n",
    "        model = rf.fit(df_training)\n",
    "        # 结果预测\n",
    "        # pipeline = Pipeline(stages=[rf])\n",
    "        df_training_pred = model.transform(df_training)\n",
    "        df_training_pred = df_training_pred.withColumn('datatype', func.lit('train'))\n",
    "        df_test_pred = model.transform(df_test)\n",
    "        df_test_pred = df_test_pred.withColumn('datatype', func.lit('test'))\n",
    "        df_test_pred.agg(func.sum('prediction'), func.sum('label')).show()\n",
    "        #rmse1 = evaluator.evaluate(df_training_pred)\n",
    "        #print(rmse1)\n",
    "        #rmse2 = evaluator.evaluate(df_test_pred)\n",
    "        #print(rmse2)\n",
    "\n",
    "        df = df_training_pred.union(df_test_pred.select(df_training_pred.columns))\n",
    "        df = df.withColumn('num', func.lit(i))\n",
    "\n",
    "        if i ==1:\n",
    "            df_all = df\n",
    "        else:\n",
    "            df_all = df_all.union(df)\n",
    "\n",
    "    #df_all = df_all.repartition(1)\n",
    "    #df_all.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    #        .mode(\"overwrite\").save('')\n",
    "\n",
    "    # 计算误差\n",
    "    print(\"nmse：计算\")\n",
    "    schema = StructType([\n",
    "                StructField(\"Province\", StringType(), True), \n",
    "                StructField(\"datatype\", StringType(), True),\n",
    "                StructField(\"NMSE\", StringType(), True),\n",
    "                StructField(\"num\", StringType(), True)\n",
    "                ])\n",
    "\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)   \n",
    "    def nmse_func(pdf):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        Province = pdf['Province'][0]\n",
    "        datatype = pdf['datatype'][0]\n",
    "        num = pdf['num'][0]\n",
    "        pdf['tmp1'] = (pdf['y_prd'] - pdf['y_true']) **2\n",
    "        tmp1_mean = pdf['tmp1'].mean()\n",
    "        y_true_mean = pdf['y_true'].mean()\n",
    "        pdf['tmp2'] = (pdf['y_true'] - y_true_mean) **2\n",
    "        tmp2_mean = pdf['tmp2'].mean()\n",
    "        if tmp2_mean == 0:\n",
    "            NMSE = 'inf'\n",
    "        NMSE = str(tmp1_mean/tmp2_mean)\n",
    "        return pd.DataFrame([[Province] + [datatype] + [NMSE] + [str(num)]], columns=[\"Province\", \"datatype\", \"NMSE\", \"num\"])\n",
    "\n",
    "\n",
    "    df_all = df_all.withColumn('y_prd', f2(col('prediction'))) \\\n",
    "            .withColumn('y_true', f2(col('label'))).persist()\n",
    "    df_nmse = df_all.select('Province', 'y_true', 'y_prd', 'datatype', 'num') \\\n",
    "                    .groupBy('Province', 'datatype', 'num').apply(nmse_func).persist()\n",
    "    df_nmse = df_nmse.withColumn('NMSE', col('NMSE').cast(DoubleType())) \\\n",
    "                    .withColumn('type', func.concat(col('datatype'), col('num')))\n",
    "\n",
    "    # 转置\n",
    "    df_nmse = df_nmse.groupBy('Province').pivot('type').agg(func.sum('NMSE'))\n",
    "\n",
    "    df_nmse = df_nmse.repartition(1)\n",
    "    df_nmse.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(df_nmse_path)\n",
    "\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
