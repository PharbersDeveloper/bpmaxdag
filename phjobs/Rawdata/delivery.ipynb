{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"delivery\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "a = 123\n",
    "b = 456\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ENV\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWPBDTVEAEU44ZAGT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"cn-northwest-1\"\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Jupyter Keep-Alive Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from phcli.ph_logs.ph_logs import phs3logger\n",
    "# from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Qilu\"\n",
    "time_left = \"202001\"\n",
    "time_right = \"202011\"\n",
    "out_dir = \"202011\"\n",
    "extract_path = \"s3a://ph-stream/common/public/max_result/0.0.5/rawdata_standard\"\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "样本数据交付\n",
    "'''\n",
    "# 输入\n",
    "time_left = int(time_left)\n",
    "time_right = int(time_right)\n",
    "\n",
    "CPA_GYC_hospital_map_path = max_path + \"/Common_files/CPA_GYC_hospital_map\"\n",
    "HAIHONG_hospital_map_path = max_path + \"/Common_files/HAIHONG_hospital_map\"\n",
    "\n",
    "product_map_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/prod_mapping\"\n",
    "province_city_mapping_path = max_path + '/' + project_name + '/province_city_mapping'\n",
    "cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "raw_standard_path = extract_path + \"/\" + project_name + \"_rawdata_standard\"\n",
    "\n",
    "# 输出\n",
    "time_range = str(time_left) + '_' + str(time_right)\n",
    "out_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/Delivery/\" + project_name + \"_raw_delivery_\" + time_range + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  数据执行  ============\n",
    "\n",
    "# ====  一. 函数定义  ====\n",
    "\n",
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字。\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====  二. 数据准备  ====  \n",
    "\n",
    "# 1. hospital_map 文件匹配医院名\n",
    "# 1.1 CPA_GYC_hospital_map\n",
    "CPA_GYC_hospital_map0 = spark.read.parquet(CPA_GYC_hospital_map_path)\n",
    "CPA_GYC_hospital_map0 = CPA_GYC_hospital_map0.select(\"医院编码\", \"医院名称\", \"等级\" ,\"标准化省份\", \"标准化城市\", \"新版ID\", \"Source\").distinct() \\\n",
    "                            .withColumnRenamed(\"医院编码\", \"ID\")\n",
    "CPA_GYC_hospital_map0 = CPA_GYC_hospital_map0.withColumn('ID', col('ID').cast(IntegerType()))\n",
    "# 优先选择cpa医院名\n",
    "id_window = Window.partitionBy('新版ID').orderBy(col('ID'))\n",
    "CPA_GYC_hospital_map1 = CPA_GYC_hospital_map0.withColumn('rank',func.row_number().over(id_window))\n",
    "CPA_GYC_hospital_map2 = CPA_GYC_hospital_map1.where(col('rank') == 1).select('新版ID', '医院名称') \\\n",
    "                            .withColumnRenamed(\"医院名称\", \"统一名称\")\n",
    "CPA_GYC_hospital_map3 = CPA_GYC_hospital_map1.join(CPA_GYC_hospital_map2, on='新版ID', how='left')\n",
    "CPA_GYC_hospital_map = CPA_GYC_hospital_map3.select('ID', '统一名称', '等级').distinct()\n",
    "CPA_GYC_hospital_map = deal_ID_length(CPA_GYC_hospital_map)\n",
    "'''\n",
    "CPA_GYC_hospital_map = CPA_GYC_hospital_map3.select('ID', '统一名称', '等级', '标准化省份', '标准化城市') \\\n",
    "                            .withColumn('标准化省份', func.when(col('标准化省份').isin(\"北京\", \"上海\", \"天津\", '重庆'), \n",
    "                                                           func.concat(col('标准化省份'), func.lit(\"市\"))) \\\n",
    "                                                        .otherwise(func.concat(col('标准化省份'), func.lit(\"省\"))))\n",
    "'''\n",
    "\n",
    "# 1.2 HAIHONG_hospital_map\n",
    "HAIHONG_hospital_map = spark.read.parquet(HAIHONG_hospital_map_path)\n",
    "HAIHONG_hospital_map = HAIHONG_hospital_map.select('医院编码', '医院名称', '等级').distinct() \\\n",
    "                                        .withColumnRenamed(\"医院编码\", \"ID\") \\\n",
    "                                        .withColumnRenamed(\"医院名称\", \"统一名称\")\n",
    "\n",
    "# 1.3 合并\n",
    "hospital_map = CPA_GYC_hospital_map.union(HAIHONG_hospital_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. province_city_mapping 文件匹配医院名\n",
    "province_city_mapping = spark.read.parquet(province_city_mapping_path)\n",
    "province_city_mapping = deal_ID_length(province_city_mapping)\n",
    "province_city_mapping = province_city_mapping.select('ID', 'Province', 'City').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. cpa_pha_mapping 文件匹配医院名\n",
    "cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "cpa_pha_mapping = cpa_pha_mapping.where(cpa_pha_mapping[\"推荐版本\"] == 1) \\\n",
    "        .select(\"ID\", \"PHA\").distinct()\n",
    "cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. product_map 文件\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "# a. 列名清洗统一\n",
    "# 有的min2结尾有空格与无空格的是两条不同的匹配\n",
    "if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[21], \"pfc\")\n",
    "if project_name == \"Eisai\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[22], \"pfc\")\n",
    "\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "    if i in [\"剂型_标准\", \"Form_std\", \"S_Dosage\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准剂型\")\n",
    "    if i in [\"规格_标准\", \"Specifications_std\", \"药品规格_标准\", \"S_Pack\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准规格\")\n",
    "    if i in [\"包装数量2\", \"包装数量_标准\", \"Pack_Number_std\", \"S_PackNumber\", \"最小包装数量\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准包装数量\")\n",
    "    if i in [\"标准企业\", \"生产企业_标准\", \"Manufacturer_std\", \"S_CORPORATION\", \"标准生产厂家\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准生产企业\")\n",
    "if project_name == \"Janssen\" or project_name == \"NHWA\":\n",
    "    if \"标准剂型\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"剂型\", \"标准剂型\")\n",
    "    if \"标准规格\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"规格\", \"标准规格\")\n",
    "    if \"标准生产企业\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"生产企业\", \"标准生产企业\")\n",
    "    if \"标准包装数量\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"包装数量\", \"标准包装数量\")\n",
    "\n",
    "# b. min2处理\n",
    "product_map = product_map.withColumn(\"min2\", func.regexp_replace(\"min2\", \"&amp;\", \"&\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&lt;\", \"<\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&gt;\", \">\"))\n",
    "\n",
    "if 'Route' not in product_map.columns:\n",
    "    product_map = product_map.withColumn('Route', func.lit('-'))\n",
    "    \n",
    "# c. 选取需要的列\n",
    "product_map = product_map \\\n",
    "                .select(\"min1\", \"pfc\", \"通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \"标准包装数量\", \"标准生产企业\", 'Route') \\\n",
    "                .withColumn(\"pfc\", product_map[\"pfc\"].cast(IntegerType())) \\\n",
    "                .withColumn(\"标准包装数量\", product_map[\"标准包装数量\"].cast(IntegerType())) \\\n",
    "                .withColumnRenamed(\"pfc\", \"PACK_ID\") \\\n",
    "                .distinct()\n",
    "\n",
    "# d. pfc为0统一替换为null\n",
    "product_map = product_map.withColumn(\"PACK_ID\", func.when(col('PACK_ID') == 0, None).otherwise(col('PACK_ID'))).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[min1: string, PACK_ID: int, 通用名: string, 标准商品名: string, 标准剂型: string, 标准规格: string, 标准包装数量: int, 标准生产企业: string, Route: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====  Raw =====\n",
    "\n",
    "# 读取交付数据\n",
    "monthlist = range(time_left, time_right+1, 1)\n",
    "path_list = [raw_standard_path + '/Date_copy=' + str(i) for i in monthlist]\n",
    "index = 0\n",
    "for eachpath in path_list:\n",
    "    df = spark.read.parquet(eachpath)\n",
    "    if index ==0:\n",
    "        data_standard = df\n",
    "    else:\n",
    "        data_standard = data_standard.union(df)\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 信息匹配，列名处理\n",
    "data_standard = deal_ID_length(data_standard)\n",
    "data_standard_delivery = data_standard.select(\"Date\", \"ID\", \"Brand\", \"Form\", \"Specifications\", \"Pack_Number\", \"Manufacturer\", \"Sales\", \"Units\", 'min1')\n",
    "data_standard_delivery = data_standard_delivery.join(product_map.dropDuplicates(['min1']), on=[\"min1\"], how='left') \\\n",
    "                                                .join(hospital_map, on='ID', how='left') \\\n",
    "                                                .join(province_city_mapping, on='ID', how='left') \\\n",
    "                                                .join(cpa_pha_mapping, on='ID', how='left')\n",
    "\n",
    "\n",
    "data_standard_delivery = data_standard_delivery.select('Date', 'Province', 'City', 'ID', 'Sales', 'Units', '通用名', '标准商品名', '标准剂型',\n",
    "                                                      '标准规格', '标准包装数量', '标准生产企业', '统一名称', '等级', 'PHA', 'Route') \\\n",
    "                                            .withColumnRenamed('Date', '年月') \\\n",
    "                                            .withColumnRenamed('Province', '省份') \\\n",
    "                                            .withColumnRenamed('City', '城市') \\\n",
    "                                            .withColumnRenamed('ID', '医院编码') \\\n",
    "                                            .withColumnRenamed('Sales', '金额') \\\n",
    "                                            .withColumnRenamed('Units', '数量') \\\n",
    "                                            .withColumnRenamed('标准商品名', '商品名') \\\n",
    "                                            .withColumnRenamed('标准剂型', '剂型') \\\n",
    "                                            .withColumnRenamed('标准规格', '规格') \\\n",
    "                                            .withColumnRenamed('标准包装数量', '包装数量') \\\n",
    "                                            .withColumnRenamed('标准生产企业', '生产企业') \\\n",
    "                                            .withColumnRenamed('统一名称', '医院名称') \\\n",
    "                                            .withColumnRenamed('PHA', 'PHA编码') \\\n",
    "                                            .withColumnRenamed('Route', '给药途径')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交付\n",
    "model_1_list = ['贝达', '康哲', '奥鸿', '京新', '海坤', '汇宇', 'Tide']\n",
    "\n",
    "if project_name in model_1_list:\n",
    "    out = data_standard_delivery.select(\"年月\", \"省份\", \"城市\", \"医院编码\", \"医院名称\", \"等级\", \"通用名\", \"商品名\", \"剂型\", \"规格\", \n",
    "                                                       \"包装数量\", \"生产企业\", \"金额\", \"数量\")\n",
    "\n",
    "model_2_list = ['神州', 'Qilu']\n",
    "if project_name in model_2_list:\n",
    "    out = data_standard_delivery.select(\"年月\", \"省份\", \"城市\", \"医院编码\", \"PHA编码\", \"医院名称\", \"等级\", \"通用名\", \"商品名\", \"剂型\", \"规格\", \n",
    "                                                       \"包装数量\", \"生产企业\", \"金额\", \"数量\")\n",
    "\n",
    "model_3_list = ['Gilead']\n",
    "if project_name in model_3_list:\n",
    "    out = data_standard_delivery.select(\"年月\", \"省份\", \"城市\", \"医院编码\", \"医院名称\", \"等级\", \"通用名\", \"商品名\", \"剂型\", \"规格\", \n",
    "                                                       \"包装数量\", \"生产企业\", \"金额\", \"数量\", \"给药途径\")\n",
    "    \n",
    "out = out.repartition(1)\n",
    "out.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .mode(\"overwrite\").save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1264568"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265639"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/Qilu/202011/raw_data.csv/',header=True)\n",
    "df.where(df.Date >202000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(金额)=88355313311.95024, sum(数量)=8312852632.3256)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.agg(func.sum('金额'), func.sum('数量')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------+-------+--------+----+------+------+----+----+--------+--------+----+----+\n",
      "|年月|省份|城市|医院编码|PHA编码|医院名称|等级|通用名|商品名|剂型|规格|包装数量|生产企业|金额|数量|\n",
      "+----+----+----+--------+-------+--------+----+------+------+----+----+--------+--------+----+----+\n",
      "|   0|   0|   0|       0|      0|       0|   0|     0|     0|   0|   0|       0|       0|   0|   0|\n",
      "+----+----+----+--------+-------+--------+----+------+------+----+----+--------+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.agg(*[func.count(func.when(func.isnull(c), c)).alias(c) for c in out.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+--------+----------+--------------+----+----------------------------------+------+------+---------+--------+----------------------------+--------+------+\n",
      "|  年月|省份|  城市|医院编码|   PHA编码|      医院名称|等级|                            通用名|商品名|  剂型|     规格|包装数量|                    生产企业|    金额|  数量|\n",
      "+------+----+------+--------+----------+--------------+----+----------------------------------+------+------+---------+--------+----------------------------+--------+------+\n",
      "|202001|河北|承德市| 1308002|PHA0001796|承德市中心医院|三级|重组人II型肿瘤坏死因子受体-抗体...|  强克|粉针剂|     25MG|       1|    上海赛金生物医药有限公司|125408.6| 192.0|\n",
      "|202001|河北|承德市| 1308002|PHA0001796|承德市中心医院|三级|                          艾司洛尔|  爱络|注射剂|100MG10ML|       1|            齐鲁制药有限公司|  6470.0| 200.0|\n",
      "|202001|河北|承德市| 1308002|PHA0001796|承德市中心医院|三级|                          安罗替尼|福可维|胶囊剂|     12MG|       7|正大天晴药业集团股份有限公司|204540.0| 420.0|\n",
      "|202001|河北|承德市| 1308002|PHA0001796|承德市中心医院|三级|                          塞来昔布|西乐葆|胶囊剂|    200MG|       6|            辉瑞制药有限公司| 16310.0|3000.0|\n",
      "|202001|河北|承德市| 1308002|PHA0001796|承德市中心医院|三级|                          比索洛尔|  康忻|  片剂|      5MG|      10|        Merck KGaA Darmstadt| 25650.0|9000.0|\n",
      "+------+----+------+--------+----------+--------------+----+----------------------------------+------+------+---------+--------+----------------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
