{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"job6_max_city\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "project_name = \"Empty\"\n",
    "time_left = \"Empty\"\n",
    "time_right = \"Empty\"\n",
    "left_models = \"Empty\"\n",
    "left_models_time_left = \"Empty\"\n",
    "right_models = \"Empty\"\n",
    "right_models_time_right = \"Empty\"\n",
    "all_models = \"Empty\"\n",
    "if_others = \"False\"\n",
    "out_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "out_dir = \"Empty\"  \n",
    "need_test = 0\n",
    "minimum_product_columns = \"Brand, Form, Specifications, Pack_Number, Manufacturer\"\n",
    "minimum_product_sep = \"|\"\n",
    "minimum_product_newname = \"min1\"\n",
    "if_two_source = \"False\"\n",
    "hospital_level = \"False\"\n",
    "bedsize = \"True\"\n",
    "id_bedsize_path = \"Empty\"\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write Max.job6_max_city in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import col\n",
    "import boto3\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "project_name = \"NHWA\"\n",
    "time_left = \"202001\"\n",
    "time_right = \"202010\"\n",
    "all_models = \"静麻,丙泊酚\"\n",
    "out_dir = \"202010\" \n",
    "minimum_product_sep = \"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "minimum_product_columns = minimum_product_columns.replace(\" \",\"\").split(\",\")\n",
    "if minimum_product_sep == \"kong\":\n",
    "    minimum_product_sep = \"\"\n",
    "\n",
    "if if_others == \"False\":\n",
    "    if_others = False\n",
    "elif if_others == \"True\":\n",
    "    if_others = True\n",
    "else:\n",
    "    raise ValueError('if_others: False or True')\n",
    "\n",
    "if bedsize != \"False\" and bedsize != \"True\":\n",
    "    raise ValueError('bedsize: False or True')\n",
    "if hospital_level != \"False\" and hospital_level != \"True\":\n",
    "    raise ValueError('hospital_level: False or True')\n",
    "\n",
    "if left_models != \"Empty\":\n",
    "    left_models = left_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    left_models = []\n",
    "\n",
    "if right_models != \"Empty\":\n",
    "    right_models = right_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    right_models = []\n",
    "\n",
    "if left_models_time_left == \"Empty\":\n",
    "    left_models_time_left = 0\n",
    "if right_models_time_right == \"Empty\":\n",
    "    right_models_time_right = 0\n",
    "\n",
    "if all_models != \"Empty\":\n",
    "    all_models = all_models.replace(\", \",\",\").split(\",\")\n",
    "else:\n",
    "    all_models = []\n",
    "\n",
    "time_left = int(time_left)\n",
    "time_right = int(time_right)\n",
    "\n",
    "province_city_mapping_path = max_path + \"/\" + project_name + '/province_city_mapping'\n",
    "hospital_ot_path = max_path + \"/\" + project_name + '/hospital_ot.csv'\n",
    "market_mapping_path = max_path + \"/\" + project_name + '/mkt_mapping'\n",
    "cpa_pha_mapping_path = max_path + \"/\" + project_name + \"/cpa_pha_mapping\"\n",
    "if id_bedsize_path == 'Empty':\n",
    "    ID_Bedsize_path = max_path + \"/Common_files/ID_Bedsize\"\n",
    "else:\n",
    "    ID_Bedsize_path = id_bedsize_path\n",
    "\n",
    "cpa_pha_mapping_common_path = max_path + \"/Common_files/cpa_pha_mapping\"\n",
    "\n",
    "if if_others == True:\n",
    "    out_dir = out_dir + \"/others_box/\"\n",
    "\n",
    "out_path_dir = out_path + \"/\" + project_name + '/' + out_dir\n",
    "product_map_path = out_path_dir + \"/prod_mapping\"\n",
    "if if_two_source == \"False\":\n",
    "    raw_data_std_path = out_path_dir + \"/product_mapping_out\"\n",
    "else:\n",
    "    raw_data_std_path = out_path_dir + \"/raw_data_std\"\n",
    "\n",
    "# 输出\n",
    "time_range = str(time_left) + '_' + str(time_right)\n",
    "tmp_path = out_path_dir + \"/MAX_result/tmp\"\n",
    "if hospital_level == \"True\" and bedsize == \"False\":\n",
    "    max_result_city_csv_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + \"_hospital_level_nobed.csv\"\n",
    "    max_result_city_tmp_path = out_path_dir + \"/MAX_result/tmp_hospital_nobed_\"+ time_range\n",
    "elif hospital_level == \"True\":\n",
    "    max_result_city_csv_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + \"_hospital_level.csv\"\n",
    "    max_result_city_tmp_path = out_path_dir + \"/MAX_result/tmp_hospital_\"+ time_range\n",
    "elif hospital_level == \"False\" and bedsize == \"False\":\n",
    "    max_result_city_csv_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + \"_city_level_nobed.csv\"\n",
    "    max_result_city_tmp_path = out_path_dir + \"/MAX_result/tmp_city_nobed_\"+ time_range\n",
    "else:\n",
    "    max_result_city_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + \"_city_level\"\n",
    "    max_result_city_csv_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + \"_city_level.csv\"\n",
    "    max_result_city_tmp_path = out_path_dir + \"/MAX_result/tmp_city_\"+ time_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== 数据执行 =============\n",
    "'''\n",
    "合并raw_data 和 max 结果\n",
    "'''\n",
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字，其他还有包含字母的ID\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df\n",
    "\n",
    "raw_data = spark.read.parquet(raw_data_std_path)\n",
    "\n",
    "if if_two_source == \"False\":\n",
    "    raw_data = raw_data.where((raw_data.year_month >=time_left) & (raw_data.year_month <=time_right))\n",
    "\n",
    "else:\n",
    "    # job1: raw_data 处理，匹配PHA，部分job1\n",
    "    for i in raw_data.columns:\n",
    "        if i in [\"数量（支/片）\", \"最小制剂单位数量\", \"total_units\", \"SALES_QTY\"]:\n",
    "            raw_data = raw_data.withColumnRenamed(i, \"Units\")\n",
    "        if i in [\"金额（元）\", \"金额\", \"sales_value__rmb_\", \"SALES_VALUE\"]:\n",
    "            raw_data = raw_data.withColumnRenamed(i, \"Sales\")\n",
    "        if i in [\"Yearmonth\", \"YM\", \"Date\"]:\n",
    "            raw_data = raw_data.withColumnRenamed(i, \"year_month\")\n",
    "        if i in [\"医院编码\", \"BI_Code\", \"HOSP_CODE\"]:\n",
    "            raw_data = raw_data.withColumnRenamed(i, \"ID\")\n",
    "\n",
    "    raw_data = raw_data.withColumn(\"year_month\", raw_data[\"year_month\"].cast(IntegerType()))\n",
    "    raw_data = raw_data.where((raw_data.year_month >=time_left) & (raw_data.year_month <=time_right))\n",
    "\n",
    "    cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "    cpa_pha_mapping = cpa_pha_mapping.where(cpa_pha_mapping[\"推荐版本\"] == 1) \\\n",
    "        .select(\"ID\", \"PHA\").distinct()\n",
    "    cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)\n",
    "\n",
    "    raw_data = deal_ID_length(raw_data)    \n",
    "    raw_data = raw_data.join(cpa_pha_mapping, on=\"ID\", how=\"left\")\n",
    "\n",
    "    # job2: raw_data 处理，生成min1，用product_map 匹配获得min2（Prod_Name），同job2\n",
    "    if project_name != \"Mylan\":\n",
    "        raw_data = raw_data.withColumn(\"Brand\", func.when(func.isnull(raw_data.Brand), raw_data.Molecule).\n",
    "                                   otherwise(raw_data.Brand))\n",
    "\n",
    "    for colname, coltype in raw_data.dtypes:\n",
    "        if coltype == \"logical\":\n",
    "            raw_data = raw_data.withColumn(colname, raw_data[colname].cast(StringType()))\n",
    "\n",
    "    raw_data = raw_data.withColumn(\"tmp\", func.when(func.isnull(raw_data[minimum_product_columns[0]]), func.lit(\"NA\")).\n",
    "                                   otherwise(raw_data[minimum_product_columns[0]]))\n",
    "\n",
    "    for col in minimum_product_columns[1:]:\n",
    "        raw_data = raw_data.withColumn(col, raw_data[col].cast(StringType()))\n",
    "        raw_data = raw_data.withColumn(\"tmp\", func.concat(\n",
    "            raw_data[\"tmp\"],\n",
    "            func.lit(minimum_product_sep),\n",
    "            func.when(func.isnull(raw_data[col]), func.lit(\"NA\")).otherwise(raw_data[col])))\n",
    "\n",
    "    # Mylan不重新生成minimum_product_newname: min1，其他项目生成min1\n",
    "    if project_name == \"Mylan\":\n",
    "        raw_data = raw_data.drop(\"tmp\")\n",
    "    else:\n",
    "        if minimum_product_newname in raw_data.columns:\n",
    "            raw_data = raw_data.drop(minimum_product_newname)\n",
    "        raw_data = raw_data.withColumnRenamed(\"tmp\", minimum_product_newname)\n",
    "\n",
    "    # product_map\n",
    "    product_map = spark.read.parquet(product_map_path)\n",
    "    for i in product_map.columns:\n",
    "        if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "            product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "        if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "            product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "        if i in [\"标准途径\"]:\n",
    "            product_map = product_map.withColumnRenamed(i, \"std_route\")\n",
    "        if i in [\"min1_标准\"]:\n",
    "            product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "    if \"std_route\" not in product_map.columns:\n",
    "        product_map = product_map.withColumn(\"std_route\", func.lit(''))\t\n",
    "\n",
    "    product_map_for_rawdata = product_map.select(\"min1\", \"min2\", \"通用名\").distinct()\n",
    "\n",
    "    raw_data = raw_data.join(product_map_for_rawdata, on=\"min1\", how=\"left\") \\\n",
    "        .drop(\"S_Molecule\") \\\n",
    "        .withColumnRenamed(\"通用名\", \"S_Molecule\")\n",
    "\n",
    "raw_data = deal_ID_length(raw_data)\n",
    "\n",
    "# 匹配市场名\n",
    "market_mapping = spark.read.parquet(market_mapping_path)\n",
    "market_mapping = market_mapping.withColumnRenamed(\"标准通用名\", \"通用名\") \\\n",
    "            .withColumnRenamed(\"model\", \"mkt\") \\\n",
    "            .select(\"mkt\", \"通用名\").distinct()\n",
    "raw_data = raw_data.join(market_mapping, raw_data[\"S_Molecule\"] == market_mapping[\"通用名\"], how=\"left\")\n",
    "\n",
    "\n",
    "# 列重命名\n",
    "raw_data = raw_data.withColumnRenamed(\"mkt\", \"DOI\") \\\n",
    "            .withColumnRenamed(\"min2\", \"Prod_Name\") \\\n",
    "            .withColumnRenamed(\"year_month\", \"Date\") \\\n",
    "            .select(\"ID\", \"Date\", \"Prod_Name\", \"Sales\", \"Units\", \"DOI\", \"PHA\", \"S_Molecule\")\n",
    "\n",
    "# 匹配通用cpa_city\n",
    "province_city_mapping = spark.read.parquet(province_city_mapping_path)\n",
    "province_city_mapping = province_city_mapping.distinct()\n",
    "province_city_mapping = deal_ID_length(province_city_mapping)\n",
    "\n",
    "raw_data = raw_data.join(province_city_mapping, on=\"ID\", how=\"left\") \\\n",
    "        .withColumn(\"PANEL\", func.lit(1))\n",
    "\n",
    "# 删除医院\n",
    "# hospital_ot = spark.read.csv(hospital_ot_path, header=True)\n",
    "# raw_data = raw_data.join(hospital_ot, on=\"ID\", how=\"left_anti\")\n",
    "\n",
    "# raw_data PHA是空的重新匹配\n",
    "cpa_pha_mapping_common = spark.read.parquet(cpa_pha_mapping_common_path)\n",
    "cpa_pha_mapping_common = cpa_pha_mapping_common.where(cpa_pha_mapping_common[\"推荐版本\"] == 1) \\\n",
    "        .withColumnRenamed(\"PHA\", \"PHA_common\") \\\n",
    "        .select(\"ID\", \"PHA_common\").distinct()\n",
    "cpa_pha_mapping_common = deal_ID_length(cpa_pha_mapping_common)\n",
    "\n",
    "raw_data = raw_data.join(cpa_pha_mapping_common, on=\"ID\", how=\"left\")\n",
    "raw_data = raw_data.withColumn(\"PHA\", func.when(raw_data.PHA.isNull(), raw_data.PHA_common).otherwise(raw_data.PHA)) \\\n",
    "                .drop(\"PHA_common\")\n",
    "\n",
    "# raw_data 医院列表\n",
    "raw_data_PHA = raw_data.select(\"PHA\", \"Date\").distinct()\n",
    "\n",
    "# ID_Bedsize 匹配\n",
    "ID_Bedsize = spark.read.parquet(ID_Bedsize_path)\n",
    "ID_Bedsize = deal_ID_length(ID_Bedsize)\n",
    "\n",
    "raw_data = raw_data.join(ID_Bedsize, on=\"ID\", how=\"left\")\n",
    "\n",
    "# all_models 筛选\n",
    "if raw_data.select(\"DOI\").dtypes[0][1] == \"double\":\n",
    "    raw_data = raw_data.withColumn(\"DOI\", raw_data[\"DOI\"].cast(IntegerType()))\n",
    "raw_data = raw_data.where(raw_data.DOI.isin(all_models))\n",
    "\n",
    "# 计算\n",
    "if project_name != \"Janssen\":\n",
    "    if bedsize == \"True\":\n",
    "        raw_data = raw_data.where(raw_data.Bedsize > 99)\n",
    "\n",
    "if hospital_level == \"True\":\n",
    "    raw_data_city = raw_data \\\n",
    "        .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"DOI\", \"S_Molecule\", \"PHA\") \\\n",
    "        .agg({\"Sales\":\"sum\", \"Units\":\"sum\"}) \\\n",
    "        .withColumnRenamed(\"sum(Sales)\", \"Predict_Sales\") \\\n",
    "        .withColumnRenamed(\"sum(Units)\", \"Predict_Unit\") \\\n",
    "        .withColumnRenamed(\"S_Molecule\", \"Molecule\") \n",
    "else:\n",
    "    raw_data_city = raw_data \\\n",
    "        .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"DOI\", \"S_Molecule\") \\\n",
    "        .agg({\"Sales\":\"sum\", \"Units\":\"sum\"}) \\\n",
    "        .withColumnRenamed(\"sum(Sales)\", \"Predict_Sales\") \\\n",
    "        .withColumnRenamed(\"sum(Units)\", \"Predict_Unit\") \\\n",
    "        .withColumnRenamed(\"S_Molecule\", \"Molecule\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. max文件处理\n",
    "index = 0\n",
    "for market in all_models:\n",
    "    # market 的 time_left 和 time_right 选择，默认为参数时间\n",
    "    if market in left_models:\n",
    "        time_left_1 = left_models_time_left\n",
    "    else:\n",
    "        time_left_1 = time_left\n",
    "    if market in right_models:\n",
    "        time_right_1 = right_models_time_right\n",
    "    else:\n",
    "        time_right_1 = time_right\n",
    "\n",
    "    time_range = str(time_left_1) + '_' + str(time_right_1)\n",
    "\n",
    "    if if_others:\n",
    "        max_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + '_'  + market + \"_hosp_level_box\"\n",
    "    else:\n",
    "        max_path = out_path_dir + \"/MAX_result/MAX_result_\" + time_range + '_' + market + \"_hosp_level\"\n",
    "\n",
    "    max_result = spark.read.parquet(max_path)\n",
    "\n",
    "    # max_result 筛选 BEDSIZE > 99， 且医院不在raw_data_PHA 中\n",
    "    if bedsize == \"True\":\n",
    "        max_result = max_result.where(max_result.BEDSIZE > 99)\n",
    "    max_result = max_result.join(raw_data_PHA, on=[\"PHA\", \"Date\"], how=\"left_anti\")\n",
    "\n",
    "    if hospital_level == \"True\":\n",
    "        max_result = max_result \\\n",
    "            .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"Molecule\", \"PHA\") \\\n",
    "            .agg({\"Predict_Sales\":\"sum\", \"Predict_Unit\":\"sum\"}) \\\n",
    "            .withColumnRenamed(\"sum(Predict_Sales)\", \"Predict_Sales\") \\\n",
    "            .withColumnRenamed(\"sum(Predict_Unit)\", \"Predict_Unit\")\n",
    "    else:\n",
    "        max_result = max_result \\\n",
    "            .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"Molecule\") \\\n",
    "            .agg({\"Predict_Sales\":\"sum\", \"Predict_Unit\":\"sum\"}) \\\n",
    "            .withColumnRenamed(\"sum(Predict_Sales)\", \"Predict_Sales\") \\\n",
    "            .withColumnRenamed(\"sum(Predict_Unit)\", \"Predict_Unit\")\n",
    "\n",
    "    max_result = max_result.withColumn(\"DOI\", func.lit(market))\n",
    "\n",
    "\n",
    "    if index ==0:\n",
    "        # max_result_all = max_result\n",
    "        max_result = max_result.repartition(1)\n",
    "        max_result.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(max_result_city_tmp_path)\n",
    "\n",
    "    else:\n",
    "        # max_result_all = max_result_all.union(max_result)\n",
    "        max_result = max_result.repartition(1)\n",
    "        max_result.write.format(\"parquet\") \\\n",
    "            .mode(\"append\").save(max_result_city_tmp_path)\n",
    "\n",
    "    index = index + 1\n",
    "\n",
    "\n",
    "max_result_all = spark.read.parquet(max_result_city_tmp_path)\n",
    "\n",
    "# 3. 合并raw_data 和 max文件处理\n",
    "if hospital_level == \"True\":\n",
    "    raw_data_city = raw_data_city.select(\"PHA\", \"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")\n",
    "    max_result_all = max_result_all.select(\"PHA\", \"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")\n",
    "else:\n",
    "    raw_data_city = raw_data_city.select(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")\n",
    "    max_result_all = max_result_all.select(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")\n",
    "\n",
    "max_result_city = max_result_all.union(raw_data_city)\n",
    "\n",
    "# 4. 合并后再进行一次group\n",
    "if hospital_level == \"True\":\n",
    "    max_result_city = max_result_city \\\n",
    "        .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"Molecule\", \"PHA\", \"DOI\") \\\n",
    "        .agg({\"Predict_Sales\":\"sum\", \"Predict_Unit\":\"sum\"}) \\\n",
    "        .withColumnRenamed(\"sum(Predict_Sales)\", \"Predict_Sales\") \\\n",
    "        .withColumnRenamed(\"sum(Predict_Unit)\", \"Predict_Unit\")\n",
    "    max_result_city = max_result_city.select(\"PHA\", \"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")\n",
    "else:\n",
    "    max_result_city = max_result_city \\\n",
    "        .groupBy(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"PANEL\", \"Molecule\", \"DOI\") \\\n",
    "        .agg({\"Predict_Sales\":\"sum\", \"Predict_Unit\":\"sum\"}) \\\n",
    "        .withColumnRenamed(\"sum(Predict_Sales)\", \"Predict_Sales\") \\\n",
    "        .withColumnRenamed(\"sum(Predict_Unit)\", \"Predict_Unit\")\n",
    "    max_result_city = max_result_city.select(\"Province\", \"City\", \"Date\", \"Prod_Name\", \"Molecule\", \"PANEL\", \"DOI\", \"Predict_Sales\", \"Predict_Unit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_result_city_final = max_result_city_final.repartition(1)\\nmax_result_city_final.write.format(\"csv\").option(\"header\", \"true\")     .mode(\"overwrite\").save(max_result_city_csv_path)\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.输出判断是否已有 max_result_city_path 结果\n",
    "'''\n",
    "如果已经存在 max_result_city_path 则用新的结果对已有结果进行DOI替换和补充\n",
    "'''\n",
    "file_name = max_result_city_path.replace('//', '/').split('s3a:/ph-max-auto/')[1]\n",
    "\n",
    "s3 = boto3.resource('s3', region_name='cn-northwest-1',\n",
    "                        aws_access_key_id=\"AKIAWPBDTVEAEU44ZAGT\",\n",
    "                        aws_secret_access_key=\"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\")\n",
    "bucket = s3.Bucket('ph-max-auto')\n",
    "judge = 0\n",
    "for obj in bucket.objects.filter(Prefix = file_name):\n",
    "    path, filename = os.path.split(obj.key)  \n",
    "    if path == file_name:\n",
    "        judge += 1\n",
    "        \n",
    "if judge > 0:\n",
    "    old_max_out = spark.read.parquet(max_result_city_path)   \n",
    "    new_markets = max_result_city.select('DOI').distinct().toPandas()['DOI'].tolist()    \n",
    "    old_max_out_keep = old_max_out.where(~col('DOI').isin(new_markets))    \n",
    "    max_result_city_final = max_result_city.union(old_max_out_keep.select(max_result_city.columns))\n",
    "    # 中间文件读写一下\n",
    "    max_result_city_final = max_result_city_final.repartition(2)\n",
    "    max_result_city_final.write.format(\"parquet\") \\\n",
    "                        .mode(\"overwrite\").save(tmp_path)\n",
    "    max_result_city_final = spark.read.parquet(tmp_path)   \n",
    "else:\n",
    "    max_result_city_final = max_result_city.repartition(2)\n",
    "    \n",
    "# hospital_level 的只输出csv\n",
    "if hospital_level == \"False\" and bedsize == \"True\":     \n",
    "    max_result_city_final = max_result_city_final.repartition(2)\n",
    "    max_result_city_final.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(max_result_city_path)\n",
    "\n",
    "max_result_city_final = max_result_city_final.repartition(1)\n",
    "max_result_city_final.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(max_result_city_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
