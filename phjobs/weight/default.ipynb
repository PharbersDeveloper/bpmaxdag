{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"default\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "project_name = \"Empty\"\n",
    "universe_choice = \"Empty\"\n",
    "all_models = \"Empty\"\n",
    "weight_upper = \"1.25\"\n",
    "job_choice = \"Empty\"\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write weight.default in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, col, udf\n",
    "\n",
    "from scipy.stats import ranksums, mannwhitneyu\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "project_name = \"Gilead\"\n",
    "universe_choice = \"乙肝:universe_传染,乙肝_2:universe_传染,乙肝_3:universe_传染,安必素:universe_传染\"\n",
    "all_models = \"乙肝,乙肝_2,乙肝_3,安必素\"\n",
    "weight_upper = \"1.25\"\n",
    "job_choice = \"weight_default\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否运行此job\n",
    "if job_choice != \"weight_default\":\n",
    "     raise ValueError('不运行weight_default')\n",
    "\n",
    "# 输入\n",
    "universe_choice_dict={}\n",
    "if universe_choice != \"Empty\":\n",
    "    for each in universe_choice.replace(\" \",\"\").split(\",\"):\n",
    "        market_name = each.split(\":\")[0]\n",
    "        universe_name = each.split(\":\")[1]\n",
    "        universe_choice_dict[market_name]=universe_name\n",
    "\n",
    "all_models = all_models.replace(\", \",\",\").split(\",\")\n",
    "weight_upper = float(weight_upper)\n",
    "\n",
    "# 输出\n",
    "project_path = project_path = max_path + \"/\" + project_name\n",
    "weight_default_path = max_path + \"/\" + project_name + '/PHA_weight_default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====  数据分析  ====\n",
    "\n",
    "for index, market in enumerate(all_models):\n",
    "    if market in universe_choice_dict.keys():\n",
    "        universe_path = project_path + '/' + universe_choice_dict[market]\n",
    "    else:\n",
    "        universe_path = project_path + '/universe_base'\n",
    "\n",
    "    universe = spark.read.parquet(universe_path)\n",
    "    universe = universe.fillna(0, 'Est_DrugIncome_RMB')\n",
    "    \n",
    "    # 数据处理\n",
    "    universe_panel = universe.where(col('PANEL') == 1).select('Panel_ID', 'Est_DrugIncome_RMB', 'Seg')\n",
    "    universe_non_panel = universe.where(col('PANEL') == 0).select('Est_DrugIncome_RMB', 'Seg', 'City', 'Province')\n",
    "    \n",
    "    seg_multi_cities = universe.select('Seg', 'City', 'Province').distinct() \\\n",
    "                            .groupby('Seg').count()\n",
    "    seg_multi_cities = seg_multi_cities.where(col('count') > 1).select('Seg').toPandas()['Seg'].tolist()\n",
    "\n",
    "    universe_m = universe_panel.where(col('Seg').isin(seg_multi_cities)) \\\n",
    "                                .withColumnRenamed('Est_DrugIncome_RMB', 'Est_DrugIncome_RMB_x') \\\n",
    "                                .join(universe_non_panel, on='Seg', how='inner')\n",
    "    \n",
    "    # 秩和检验获得p值\n",
    "    schema = StructType([\n",
    "        StructField(\"Panel_ID\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"Province\", StringType(), True),\n",
    "        StructField(\"pvalue\", DoubleType(), True)\n",
    "        ])\n",
    "\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def wilcoxtest(pdf):\n",
    "        # 秩和检验\n",
    "        Panel_ID = pdf['Panel_ID'][0]\n",
    "        City = pdf['City'][0]\n",
    "        Province = pdf['Province'][0]\n",
    "        a = pdf['Est_DrugIncome_RMB_x'].drop_duplicates().values.astype(float)\n",
    "        b = pdf['Est_DrugIncome_RMB'].values.astype(float)\n",
    "        pvalue = round(mannwhitneyu(a, b, alternative=\"two-sided\")[1],6) # 等同于R中的wilcox.test()     \n",
    "        return pd.DataFrame([[Panel_ID] + [City] + [Province] + [pvalue]], columns=[\"Panel_ID\", \"City\", \"Province\", \"pvalue\"])\n",
    "\n",
    "    universe_m_wilcox = universe_m.groupby('Panel_ID', 'City', 'Province') \\\n",
    "                                .apply(wilcoxtest)\n",
    "    \n",
    "    universe_m_maxmin = universe_m_wilcox.groupby('Panel_ID') \\\n",
    "                                        .agg(func.min('pvalue').alias('min'), func.max('pvalue').alias('max'))\n",
    "    \n",
    "    # 计算weight\n",
    "    universe_m_weight = universe_m_wilcox.join(universe_m_maxmin, on='Panel_ID', how='left') \\\n",
    "                                        .withColumn('Weight', \n",
    "                    (col('pvalue') - col('min'))/(col('max') - col('min'))*(weight_upper-1/weight_upper) + 1/weight_upper)\n",
    "\n",
    "    universe_m_weight = universe_m_weight.fillna(1, 'Weight')\n",
    "    \n",
    "    weight_out = universe_m_weight.withColumn('DOI', func.lit(market)) \\\n",
    "                                .withColumnRenamed('Panel_ID', 'PHA') \\\n",
    "                                .select('Province', 'City', 'DOI', 'Weight', 'PHA')\n",
    "    \n",
    "    # 结果输出\n",
    "    if index ==0:\n",
    "        weight_out = weight_out.repartition(1)\n",
    "        weight_out.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(weight_default_path)\n",
    "    else:\n",
    "        weight_out = weight_out.repartition(1)\n",
    "        weight_out.write.format(\"parquet\") \\\n",
    "            .mode(\"append\").save(weight_default_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
