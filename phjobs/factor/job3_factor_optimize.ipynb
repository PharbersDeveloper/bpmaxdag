{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"job3_factor_optimize\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "model_month_right = 'Empty'\n",
    "model_month_left = 'Empty'\n",
    "all_models = 'Empty'\n",
    "max_file = 'Empty'\n",
    "test = 'False'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write factor.job3_factor_optimize in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "project_name = 'Takeda'\n",
    "outdir = '202012'\n",
    "model_month_right = '202012'\n",
    "model_month_left = '202001'\n",
    "all_models = 'TK1'\n",
    "max_file = 'MAX_result_201801_202012_city_level'\n",
    "test = 'True'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job3_factor_optimize\n"
     ]
    }
   ],
   "source": [
    "if test != \"False\" and test != \"True\":\n",
    "    phlogger.error('wrong input: test, False or True') \n",
    "    raise ValueError('wrong input: test, False or True')\n",
    "\n",
    "\n",
    "model_month_right = int(model_month_right)\n",
    "model_month_left = int(model_month_left)\n",
    "all_models = all_models.replace(' ','').split(',')\n",
    "\n",
    "max_result_path = max_path + '/' + project_name + '/' + outdir + '/MAX_result/' + max_file\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "\n",
    "# =========== 数据执行 ============\n",
    "print(\"job3_factor_optimize\")\n",
    "# 1. max 文件处理\n",
    "max_result = spark.read.parquet(max_result_path)\n",
    "max_result = max_result.where((col('Date') >= model_month_left) & (col('Date') <= model_month_right))\n",
    "\n",
    "# 2. product_map 文件处理\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "#if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "#    if \"pfc\" not in product_map.columns:\n",
    "#        product_map = product_map.withColumnRenamed([i for i in product_map.columns if 'pfc' in i][0], \"pfc\")\n",
    "#if project_name == \"Eisai\":\n",
    "#    if \"pfc\" not in product_map.columns:\n",
    "#        product_map = product_map.withColumnRenamed([i for i in product_map.columns if 'pfc' in i][0], \"pfc\")\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "    if i in [\"剂型_标准\", \"Form_std\", \"S_Dosage\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准剂型\")\n",
    "    if i in [\"规格_标准\", \"Specifications_std\", \"药品规格_标准\", \"S_Pack\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准规格\")\n",
    "    if i in [\"包装数量2\", \"包装数量_标准\", \"Pack_Number_std\", \"S_PackNumber\", \"最小包装数量\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准包装数量\")\n",
    "    if i in [\"标准企业\", \"生产企业_标准\", \"Manufacturer_std\", \"S_CORPORATION\", \"标准生产厂家\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准生产企业\")\n",
    "if project_name == \"Janssen\" or project_name == \"NHWA\":\n",
    "    if \"标准剂型\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"剂型\", \"标准剂型\")\n",
    "    if \"标准规格\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"规格\", \"标准规格\")\n",
    "    if \"标准生产企业\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"生产企业\", \"标准生产企业\")\n",
    "    if \"标准包装数量\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"包装数量\", \"标准包装数量\")\n",
    "product_map = product_map.withColumn(\"min2\", func.regexp_replace(\"min2\", \"&amp;\", \"&\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&lt;\", \"<\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&gt;\", \">\"))\n",
    "\n",
    "product_map = product_map.select('通用名', '标准商品名', '标准剂型', '标准规格', '标准包装数量', \n",
    "                                    '标准生产企业', 'min2', 'pfc') \\\n",
    "                        .distinct() \\\n",
    "                        .withColumnRenamed('通用名', 'Molecule') \\\n",
    "                        .withColumnRenamed('标准商品名', 'Brand') \\\n",
    "                        .withColumnRenamed('标准剂型', 'Form') \\\n",
    "                        .withColumnRenamed('标准规格', 'Specifications') \\\n",
    "                        .withColumnRenamed('标准包装数量', 'Pack_Number') \\\n",
    "                        .withColumnRenamed('标准生产企业', 'Manufacturer') \\\n",
    "                        .withColumn('pfc', product_map.pfc.cast(DoubleType())) \\\n",
    "                        .withColumnRenamed('pfc', 'Pack_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前market为:TK1\n",
      "cvxpy优化\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/group_ops.py:73: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish:TK1\n"
     ]
    }
   ],
   "source": [
    "# 3. 对每个市场优化factor\n",
    "# market = '固力康'\n",
    "for market in all_models:\n",
    "    print(\"当前market为:\" + str(market))\n",
    "    # 输入\n",
    "    ims_info_path = max_path + '/' + project_name + '/ims_info/' +  market + '_ims_info'\n",
    "    factor_path = max_path + '/' + project_name + '/forest/' + market + '_factor_1'\n",
    "    # 输出\n",
    "    ims_v1_otherall_path = max_path + '/' + project_name + '/forest/' + market + '_top3_product.csv'\n",
    "    ims_panel_max_out_path = max_path + '/' + project_name + '/forest/' + market + '_factor_gap.csv'\n",
    "    if test == 'True':\n",
    "        factor_out_path = max_path + '/' + project_name + '/forest/factor/factor_' + market\n",
    "    else:\n",
    "        factor_out_path = max_path + '/' + project_name + '/factor/factor_' + market\n",
    "\n",
    "    # 数据执行\n",
    "    ims_info = spark.read.parquet(ims_info_path)\n",
    "    ims_info = ims_info.withColumnRenamed('city', 'City')\n",
    "\n",
    "    factor = spark.read.parquet(factor_path)\n",
    "\n",
    "    @udf(StringType())\n",
    "    def city_change(name):\n",
    "        # 城市名定义\n",
    "        if name in [\"福州市\", \"厦门市\", \"泉州市\"]:\n",
    "            newname = \"福厦泉市\"\n",
    "        elif name in [\"珠海市\", \"东莞市\", \"中山市\", \"佛山市\"]:\n",
    "            newname = \"珠三角市\"\n",
    "        elif name in [\"绍兴市\", \"嘉兴市\", \"台州市\", \"金华市\"]:\n",
    "            newname = \"浙江市\"\n",
    "        elif name in [\"苏州市\", \"无锡市\"]:\n",
    "            newname = \"苏锡市\"\n",
    "        else:\n",
    "            newname = name\n",
    "        return newname\n",
    "\n",
    "    # 3.1 max 数据\n",
    "    max_df = max_result.where(col('DOI') == market)\n",
    "    max_df = max_df.join(factor, on='City', how='left')\n",
    "    max_df = max_df.withColumn('Predict_Sales', func.when(col('PANEL') == 0, col('factor')*col('Predict_Sales')) \\\n",
    "                                                    .otherwise(col('Predict_Sales'))) \\\n",
    "                    .withColumn('Citynew', col('City'))\n",
    "\n",
    "    max_df = max_df.withColumn('Citynew', city_change(col('City')))\n",
    "    max_df = max_df.withColumn('Citynew', func.when(~col('Citynew').isin(ims_info.select('City').distinct().toPandas()['City'].values.tolist()), \n",
    "                                                    func.lit('other')).otherwise(col('Citynew')))\n",
    "\n",
    "    max_df = max_df.join(product_map.dropDuplicates(['min2']), max_df.Prod_Name==product_map.min2, how='left')\n",
    "\n",
    "    # 城市产品层面\n",
    "    max2 = max_df.groupBy('Brand', 'Citynew').agg(func.sum('Predict_Sales').alias('max_Prod')) \\\n",
    "                .withColumn('mkt', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "    # 全国的市场\n",
    "    max3 = max_df.groupBy('Citynew').agg(func.sum('Predict_Sales').alias('max_mkt')) \\\n",
    "                .withColumn('Market', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "\n",
    "    # 3.2 panel数据\n",
    "    bll1 = max_df.where(max_df.PANEL == 1)\n",
    "    # 城市产品层面\n",
    "    bll2 = bll1.groupBy('Brand', 'Citynew').agg(func.sum('Predict_Sales').alias('panel_Sales')) \\\n",
    "                .withColumn('mkt', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "    # 全国的市场\n",
    "    bll3 = bll1.groupBy('Citynew').agg(func.sum('Predict_Sales').alias('panel_mkt')) \\\n",
    "                .withColumn('Market', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "\n",
    "    # 3.3 确定IMS三个最大的产品\n",
    "    ims_df = ims_info.withColumn('Market', func.lit(market))\n",
    "\n",
    "    ims_df = ims_df.withColumn('Brand_en', col('Prd_desc')) \\\n",
    "                        .join(product_map.select('Pack_ID', 'Brand').dropDuplicates(['Pack_ID']), on='Pack_ID', how='left')\n",
    "\n",
    "    ims_df = ims_df.groupBy('Market', 'Brand', 'City').agg(func.sum('ims_poi_vol').alias('Prod')) \\\n",
    "                    .withColumn('Brand', func.when(col('Brand').isNull(), func.lit('none')).otherwise(col('Brand')))\n",
    "\n",
    "    # 全国的商品销量\n",
    "    ims_part1 = ims_df.where(col('City') == 'CHPA')\n",
    "    ims_part1_v1 = ims_part1.groupBy('Market', 'Brand').agg(func.sum('Prod').alias('Prod_CHPA'))\n",
    "    # 非other 省份的商品销量\n",
    "    ims_part2 = ims_df.where(col('City') != 'CHPA')\n",
    "    ims_part2_v1 = ims_part2.groupBy('Market', 'Brand').agg(func.sum('Prod').alias('Prod_nonCHPA'))\n",
    "    # other 省份的商品销量\n",
    "    ims_v1_left = ims_part1_v1.join(ims_part2_v1, on=['Market','Brand'], how='left')\n",
    "    ims_v1_left = ims_v1_left.fillna(0, 'Prod_nonCHPA') \\\n",
    "                            .withColumn('Prod_other', col('Prod_CHPA')-col('Prod_nonCHPA')) \\\n",
    "                            .withColumn('City', func.lit('other'))\n",
    "    ims_v1_left = ims_v1_left.withColumnRenamed('Prod_CHPA', 'CHPA') \\\n",
    "                            .withColumnRenamed('Prod_nonCHPA', 'IMS_City') \\\n",
    "                            .withColumnRenamed('Prod_other', 'Prod') \\\n",
    "                            .select('Market','Brand','City','Prod')\n",
    "    # other + 非other省份 每个城市的销量前三名产品\n",
    "    ims_v1_otherall = ims_part2.union(ims_v1_left)\n",
    "    ims_v1_otherall = ims_v1_otherall.withColumn('n',\n",
    "                            func.row_number().over(Window.partitionBy('Market', 'City').orderBy(col('Prod').desc())))\n",
    "    ims_v1_otherall = ims_v1_otherall.where(col('n') < 4) \\\n",
    "                                    .orderBy(col('Market'), col('City'), col('Prod').desc()) \\\n",
    "                                    .withColumnRenamed('Prod', 'ims_Prod').persist()\n",
    "\n",
    "    ims_v1_mkt =  ims_part2.union(ims_v1_left) \\\n",
    "                            .groupBy('Market','City').agg(func.sum('Prod').alias('ims_mkt'))\n",
    "\n",
    "    def unpivot(df, keys):\n",
    "        # 功能：数据宽变长\n",
    "        # 参数说明 df:dataframe,  keys 待转换表中需要保留的主键key，以list[]类型传入\n",
    "        # 转换是为了避免字段类不匹配，统一将数据转换为string类型，如果保证数据类型完全一致，可以省略该句\n",
    "        df = df.select(*[col(_).astype(\"string\") for _ in df.columns])\n",
    "        cols = [_ for _ in df.columns if _ not in keys]\n",
    "        stack_str = ','.join(map(lambda x: \"'%s', `%s`\" % (x, x), cols))\n",
    "        # feature, value 转换后的列名，可自定义\n",
    "        df = df.selectExpr(*keys, \"stack(%s, %s) as (feature, value)\" % (len(cols), stack_str))\n",
    "        return df\n",
    "\n",
    "    ims_panel_max = ims_v1_otherall.join(max2.withColumnRenamed('mkt', 'Market'), \n",
    "                                        on=['Market', 'Brand', 'City'], how='left') \\\n",
    "                                    .join(bll2.withColumnRenamed('mkt', 'Market'),\n",
    "                                        on=['Market', 'Brand', 'City'], how='left')\n",
    "    ims_panel_max = unpivot(ims_panel_max, ['Market', 'Brand', 'City', 'n'])\n",
    "    ims_panel_max = ims_panel_max.withColumn('value', col('value').cast(DoubleType())) \\\n",
    "                                .withColumn('new', func.concat(col('feature'), func.lit('_'), col('n'))) \\\n",
    "                                .drop('Brand', 'feature', 'n')\n",
    "    ims_panel_max = ims_panel_max.groupBy('Market', 'City').pivot('new').agg(func.sum('value')).fillna(0).persist()\n",
    "    ims_panel_max = ims_panel_max.join(ims_v1_mkt, on=['Market','City'], how='left') \\\n",
    "                                .join(max3, on=['Market','City'], how='left') \\\n",
    "                                .join(bll3, on=['Market','City'], how='left')\n",
    "    ims_panel_max = ims_panel_max.fillna(0, ims_panel_max.columns[2:])\n",
    "\n",
    "    if 'ims_Prod_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('ims_Prod_2', func.lit(0))\n",
    "    if 'max_Prod_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('max_Prod_2', func.lit(0))\n",
    "    if 'panel_Sales_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('panel_Sales_2', func.lit(0))\n",
    "    if 'ims_Prod_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('ims_Prod_3', func.lit(0))\n",
    "    if 'max_Prod_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('max_Prod_3', func.lit(0))\n",
    "    if 'panel_Sales_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('panel_Sales_3', func.lit(0))\n",
    "\n",
    "    # 3.4 优化 \n",
    "    print(\"cvxpy优化\")\n",
    "    schema = deepcopy(ims_panel_max.schema)\n",
    "    schema.add(\"factor\", DoubleType())\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def cvxpy_func(pdf):\n",
    "        import numpy as np\n",
    "        import cvxpy as cp\n",
    "\n",
    "        f = cp.Variable()\n",
    "        prob = cp.Problem(cp.Minimize(cp.maximum(cp.abs((f*(pdf['max_Prod_1'][0]-\n",
    "                                          pdf['panel_Sales_1'][0])+\n",
    "                                       pdf['panel_Sales_1'][0])/pdf['ims_Prod_1'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_Prod_2'][0]-\n",
    "                                          pdf['panel_Sales_2'][0])+\n",
    "                                       pdf['panel_Sales_2'][0])/pdf['ims_Prod_2'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_Prod_3'][0]-\n",
    "                                          pdf['panel_Sales_3'][0])+\n",
    "                                       pdf['panel_Sales_3'][0])/pdf['ims_Prod_3'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_mkt'][0]-\n",
    "                                          pdf['panel_mkt'][0])+\n",
    "                                       pdf['panel_mkt'][0])/pdf['ims_mkt'][0]-1))),\n",
    "                   [cp.abs((f*(pdf['max_mkt'][0]-pdf['panel_mkt'][0])+\n",
    "                               pdf['panel_mkt'][0])/pdf['ims_mkt'][0]-1) <= 0.05])\n",
    "        prob.solve(solver = cp.ECOS)\n",
    "\n",
    "        return pdf.assign(factor = f.value)\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max.groupby('Market','City').apply(cvxpy_func)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('factor', \n",
    "                                            func.when((col('factor').isNull()) | (col('factor') < 0 ), func.lit(0)) \\\n",
    "                                                .otherwise(col('factor')))\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap1', \n",
    "            (col('factor')*(col('max_Prod_1') - col('panel_Sales_1')) + col('panel_Sales_1')) / col('ims_Prod_1') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap2', \n",
    "            (col('factor')*(col('max_Prod_2') - col('panel_Sales_2')) + col('panel_Sales_2')) / col('ims_Prod_2') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap3', \n",
    "            (col('factor')*(col('max_Prod_3') - col('panel_Sales_3')) + col('panel_Sales_3')) / col('ims_Prod_3') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap_mkt', \n",
    "            (col('factor')*(col('max_mkt') - col('panel_mkt')) + col('panel_mkt')) / col('ims_mkt') -1)\n",
    "\n",
    "    # 输出\n",
    "    '''\n",
    "    ims_v1_otherall = ims_v1_otherall.repartition(1)\n",
    "    ims_v1_otherall.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(ims_v1_otherall_path)\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max_out.repartition(1)\n",
    "    ims_panel_max_out.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(ims_panel_max_out_path)\n",
    "    '''\n",
    "    factor1 = spark.read.parquet(factor_path)\n",
    "    factor1 = factor1.withColumnRenamed('factor', 'factor1')\n",
    "    factor2 = ims_panel_max_out.select('City','factor').withColumnRenamed('factor', 'factor2')\n",
    "    factor3 = factor1.join(factor2, on='City', how='left')\n",
    "\n",
    "    factor2_city = factor2.select('City').distinct().toPandas()['City'].values.tolist()\n",
    "\n",
    "    if \"福厦泉市\" in factor2_city:\n",
    "        value = factor2.where(col('City')=='福厦泉市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"福州市\",\"厦门市\",\"泉州市\"), func.lit(value)) \\\n",
    "                                                    .otherwise(col('factor2')))\n",
    "\n",
    "    if \"珠三角市\" in factor2_city:\n",
    "        value2 = factor2.where(col('City')=='珠三角市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"珠海市\",\"东莞市\",\"中山市\",\"佛山市\"), \n",
    "                                                    func.lit(value2)).otherwise(col('factor2')))\n",
    "\n",
    "    if \"浙江市\" in factor2_city:\n",
    "        value3 = factor2.where(col('City')=='浙江市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"绍兴市\",\"嘉兴市\",\"台州市\",\"金华市\"), \n",
    "                                                    func.lit(value3)).otherwise(col('factor2')))\n",
    "\n",
    "    if \"苏锡市\" in factor2_city:\n",
    "        value4 = factor2.where(col('City')=='苏锡市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"苏州市\",\"无锡市\"), \n",
    "                                                    func.lit(value4)).otherwise(col('factor2')))\n",
    "\n",
    "    value_other = factor2.where(col('City')=='other').select('factor2').toPandas()['factor2'][0]    \n",
    "    factor3 = factor3.withColumn('factor2', func.when(col('factor2').isNull(), func.lit(value_other)) \\\n",
    "                                                .otherwise(col('factor2')))\n",
    "\n",
    "    factor3 = factor3.withColumn('factor', col('factor1') * col('factor2'))\n",
    "    factor3 = factor3.withColumn('factor', func.when(col('factor') > 4, func.lit(4)).otherwise(col('factor')))\n",
    "    '''\n",
    "    factor3 = factor3.repartition(1)\n",
    "    factor3.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(factor_out_path)\n",
    "    '''\n",
    "    print(\"finish:\" + str(market))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
