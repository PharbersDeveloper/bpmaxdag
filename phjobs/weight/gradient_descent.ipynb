{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"gradient_descent\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "market_city_brand = 'Empty'\n",
    "lmda = '0.001'\n",
    "learning_rate = '100'\n",
    "max_iteration = '10000'\n",
    "gradient_type = 'both'\n",
    "test = 'False'\n",
    "year_list = 'Empty'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write weight.gradient_descent in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nyear_list = \\'2019,2020\\'\\nproject_name = \"Takeda\"\\noutdir = \"202012\"\\nmarket_city_brand = \"TK1:浙江市_3|温州市_3|珠三角_3|福厦泉_3|重庆市_4|宁波市_4|贵阳市_3|郑州市_3|大连市_1|北京市_3|昆明市_2|西安市_4|合肥市_4|深圳市_3|常州市_4|南宁市_2|太原市_3\"\\n# 2019_邦得清\\n#精神:天津市_3|福厦泉_3|宁波市_3|济南市_3|温州市_2|常州市_3\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "year_list = '2019,2020'\n",
    "project_name = \"Takeda\"\n",
    "outdir = \"202012\"\n",
    "market_city_brand = \"TK1:浙江市_3|温州市_3|珠三角_3|福厦泉_3|重庆市_4|宁波市_4|贵阳市_3|郑州市_3|大连市_1|北京市_3|昆明市_2|西安市_4|合肥市_4|深圳市_3|常州市_4|南宁市_2|太原市_3\"\n",
    "# 2019_邦得清\n",
    "#精神:天津市_3|福厦泉_3|宁波市_3|济南市_3|温州市_2|常州市_3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-725932c1bd49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0myear_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0myear_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0myear_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlmda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlmda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 输入\n",
    "if test != \"False\" and test != \"True\":\n",
    "    logger.info('wrong input: test, False or True') \n",
    "    raise ValueError('wrong input: test, False or True')\n",
    "\n",
    "year_list = year_list.replace(\" \",\"\").split(\",\")\n",
    "year_min = year_list[0]\n",
    "year_max = year_list[1]\n",
    "\n",
    "lmda = float(lmda)\n",
    "learning_rate = int(learning_rate)\n",
    "max_iteration = int(max_iteration)\n",
    "# market_city_brand = json.loads(market_city_brand)\n",
    "universe_path = max_path + '/' + project_name + '/universe_base'\n",
    "\n",
    "market_city_brand_dict={}\n",
    "for each in market_city_brand.replace(\" \",\"\").split(\",\"):\n",
    "    market_name = each.split(\":\")[0]\n",
    "    if market_name not in market_city_brand_dict.keys():\n",
    "        market_city_brand_dict[market_name]={}\n",
    "    city_brand = each.split(\":\")[1]\n",
    "    for each in city_brand.replace(\" \",\"\").split(\"|\"): \n",
    "        city = each.split(\"_\")[0]\n",
    "        brand = each.split(\"_\")[1]\n",
    "        market_city_brand_dict[market_name][city]=brand\n",
    "print(market_city_brand_dict)\n",
    "\n",
    "# 输出\n",
    "if test == \"False\":\n",
    "    weight_path = max_path + '/' + project_name + '/PHA_weight'\n",
    "    weight_tmp_path = max_path + '/' + project_name + '/weight/PHA_weight_tmp'\n",
    "    tmp_path = max_path + '/' + project_name + '/weight/tmp'\n",
    "else:\n",
    "    weight_path = max_path + '/' + project_name + '/weight/PHA_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  数据执行  ============\n",
    "\n",
    "# ====  一. 数据准备  ==== \n",
    "# 1. universe 文件                  \n",
    "universe = spark.read.parquet(universe_path)\n",
    "universe = universe.select(\"Province\", \"City\") \\\n",
    "                    .distinct()\n",
    "\n",
    "\n",
    "# ====  二. 函数定义  ====\n",
    "\n",
    "# 1. 利用 ims_sales_gr，生成城市 top 产品的 'gr','share','share_ly' 字典\n",
    "def func_target_brand(pdf, city_brand_dict):\n",
    "    import json\n",
    "    city_name = pdf['City'][0]\n",
    "    brand_number = city_brand_dict[city_name]\n",
    "    pdf = pdf.sort_values(by='share', ascending=False).reset_index()[0:int(brand_number)]\n",
    "    dict_share = pdf.groupby(['City'])['标准商品名','gr','share','share_ly'].apply(lambda x : x.set_index('标准商品名').to_dict()).to_dict()\n",
    "    dict_share = json.dumps(dict_share)\n",
    "    return pd.DataFrame([[city_name] + [dict_share]], columns=['City', 'dict'])\n",
    "\n",
    "schema= StructType([\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"dict\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def udf_target_brand(pdf):\n",
    "    return func_target_brand(pdf, city_brand_dict)\n",
    "\n",
    "# 2. 算法函数 辅助函数\n",
    "# 计算增长率或者份额\n",
    "def r(W, h_n, h_d):\n",
    "    return h_n.T.dot(W)/h_d.T.dot(W)\n",
    "\n",
    "# 计算和IMS的差距\n",
    "def delta_gr(W, h_n, h_d, g):\n",
    "    return (r(W, h_n, h_d) - g)\n",
    "\n",
    "# 控制W离散\n",
    "def w_discrete_ctrl(W, lmda):\n",
    "    return 2*lmda*(W-np.average(W))\n",
    "\n",
    "# Loss-func求导\n",
    "'''\n",
    "loss function是差距（份额或增长）的平方\n",
    "'''\n",
    "def gradient(W, h_n, h_d, g, lmda):\n",
    "    CrossDiff = (h_d.sum() - W.reshape(-1)*h_d) * h_n - \\\n",
    "              (h_n.sum() - W.reshape(-1)*h_n) * h_d    \n",
    "    dW = delta_gr(W, h_n, h_d, g) * np.power((h_d.T.dot(W)), -2) * CrossDiff \\\n",
    "      + w_discrete_ctrl(W, lmda).reshape(-1, )\n",
    "    return dW.reshape(-1,1)\n",
    "\n",
    "# 梯度下降\n",
    "'''\n",
    "这是一个双任务优化求解，目标是寻找满足IMS Share and Growth的医院参数线性组合 ～ w\n",
    "可能的待优化方向：\n",
    "    1. 自适应学习率\n",
    "    2. 不同任务重要性，不同收敛速度，不同学习率\n",
    "    3. 避免落在局部最优\n",
    "    4. 初始值的选择\n",
    "'''\n",
    "\n",
    "def gradient_descent(W_0, S, G, S_ly, learning_rate, max_iteration, brand_number, H_n, H_share, H_gr, H_ly, lmda, gradient_type):\n",
    "    X = np.array([]); Growth = np.array([]); Share = np.array([])\n",
    "    W = W_0.copy()\n",
    "    W_init = W_0.copy()\n",
    "    w_gd = np.array([0]*H_n.shape[1])\n",
    "    for i in range(max_iteration):\n",
    "        gradient_sum = np.zeros((len(H_n),1))\n",
    "        for k in range(H_n.shape[1]):\n",
    "            gd_share = gradient(W, H_n[:,k], H_share, S[k], lmda)\n",
    "            gd_growth = gradient(W, H_n[:,k], H_gr[:,k], G[k], lmda)\n",
    "            if gradient_type == 'both':\n",
    "                gradient_sum += gd_growth + gd_share\n",
    "            elif gradient_type == 'share':\n",
    "                # 只优化Share\n",
    "                gradient_sum += gd_share\n",
    "            # 优化Share和增长\n",
    "            elif gradient_type == 'gr':\n",
    "                gradient_sum += gd_growth\n",
    "\n",
    "        gradient_sum[(W_init==0)|(W_init==1)] = 0\n",
    "        W -= learning_rate * gradient_sum\n",
    "        W[W<0] = 0\n",
    "        print ('iteration : ', i, '\\n GR : ', r(W, H_n, H_gr),\n",
    "            '\\n Share : ', r(W, H_n, H_share)) \n",
    "        X = np.append(X, [i])\n",
    "        Growth = np.append(Growth, r(W, H_n, H_gr))\n",
    "        Share = np.append(Share, r(W, H_n, H_share))\n",
    "        share_ly_hat = W.T.dot(H_gr).reshape(-1)/W.T.dot(H_ly)\n",
    "        w_gd = abs(share_ly_hat/S_ly -1)\n",
    "    return W, X, Growth.reshape((i+1,-1)), Share.reshape((i+1,-1))\n",
    "\n",
    "# 3. pandas_udf 执行算法，分城市进行优化\n",
    "def func_target_weight(pdf, dict_target_share, l, m, lmda, gradient_type, year_min, year_max):\n",
    "    city_name = pdf['City'][0]\n",
    "    target = list(dict_target_share[city_name]['gr'].keys())\n",
    "    brand_number = len(target)\n",
    "\n",
    "    H = pdf.pivot_table(index=['City','PHA','City_Sample','weight', 'Bedsize>99'], columns=['tmp'], \n",
    "                        values='Sales', fill_value=0, aggfunc='sum')\n",
    "    H = H.reset_index()\n",
    "\n",
    "    # 判断target是否在data中，如果不在给值为0(会有小城市，ims的top—n产品在data中不存在)\n",
    "    for each in target:\n",
    "        if year_min+'_'+each not in H.columns:\n",
    "            H[year_min+'_'+each]=0\n",
    "        if year_max+'_'+each not in H.columns:\n",
    "            H[year_max+'_'+each]=0\n",
    "            \n",
    "    H[year_min+'_total'] = H.loc[:,H.columns.str.contains(year_min)].sum(axis=1)\n",
    "    H[year_max+'_total'] = H.loc[:,H.columns.str.contains(year_max)].sum(axis=1)\n",
    "    H[year_min+'_others'] = H[year_min+'_total'] - H.loc[:,[year_min+'_'+col for col in target]].sum(axis=1)\n",
    "    H[year_max+'_others'] = H[year_max+'_total'] - H.loc[:,[year_max + '_'+col for col in target]].sum(axis=1)\n",
    "\n",
    "    H_18 = H.loc[:,[year_min+'_'+col for col in target]].values\n",
    "    H_19 = H.loc[:,[year_max+'_'+col for col in target]].values\n",
    "    Ht_18 = H.loc[:,year_min+'_total'].values\n",
    "    Ht_19 = H.loc[:,year_max+'_total'].values\n",
    "    G = list(dict_target_share[city_name]['gr'].values())\n",
    "    S = list(dict_target_share[city_name]['share'].values())\n",
    "    S_ly = np.array(list(dict_target_share[city_name]['share_ly'].values()))\n",
    "    W_0 = np.array(H['weight']).reshape(-1,1)\n",
    "\n",
    "    # 梯度下降\n",
    "    # H_n=H_19, H_share=Ht_19, H_gr=H_18, H_ly=Ht_18\n",
    "    W_result, X, Growth, Share= gradient_descent(W_0, S, G, S_ly, l, m, brand_number, H_19, Ht_19, H_18, Ht_18, \n",
    "                                                lmda, gradient_type)\n",
    "    # 首先标准化，使w优化前后的点积相等\n",
    "    W_norm = W_result * Ht_19.T.dot(W_0)[0]/Ht_19.T.dot(W_result)[0]\n",
    "\n",
    "    H2 = deepcopy(H)\n",
    "    H2['weight_factor'] = (W_norm-1)/(np.array(H2['weight']).reshape(-1,1)-1)\n",
    "    H2['weight_factor1'] = (W_norm)/(np.array(H2['weight']).reshape(-1,1))\n",
    "    H2.loc[(H2['Bedsize>99'] == 0), 'weight_factor'] = H2.loc[(H2['Bedsize>99'] == 0), 'weight_factor1']\n",
    "    H2['weight_factor'].fillna(0, inplace=True)\n",
    "    # H2.loc[(H2['weight_factor'] < 0), 'weight_factor'] = 0\n",
    "    H2['W'] = W_norm\n",
    "    # 整理H2便于输出\n",
    "    H2['weight_factor'] = [x if math.isinf(x)==False else 0 for x in H2['weight_factor']]\n",
    "    H2['weight_factor1'] = [x if math.isinf(x)==False else 0 for x in H2['weight_factor1']]\n",
    "    H2[H2.columns] = H2[H2.columns].astype(\"str\")\n",
    "    H2 = H2[['City', 'City_Sample', 'PHA', 'weight', 'Bedsize>99','weight_factor', 'weight_factor1', 'W']]\n",
    "    return H2\n",
    "\n",
    "schema= StructType([\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"PHA\", StringType(), True),\n",
    "        StructField(\"City_Sample\", StringType(), True),\n",
    "        StructField(\"weight\", StringType(), True),\n",
    "        StructField(\"Bedsize>99\", StringType(), True),\n",
    "        StructField(\"weight_factor\", StringType(), True),\n",
    "        StructField(\"weight_factor1\", StringType(), True),\n",
    "        StructField(\"W\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def udf_target_weight(pdf):\n",
    "    return func_target_weight(pdf, dict_target_share, l=learning_rate, m=max_iteration, lmda=lmda, gradient_type=gradient_type, \n",
    "                              year_min=year_min, year_max=year_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====  三. 数据分析  ====\n",
    "\n",
    "# 每个市场 进行分析\n",
    "market_list = list(market_city_brand_dict.keys())\n",
    "index_file = 0\n",
    "for market in market_list:\n",
    "    # 输入文件\n",
    "    # market = '固力康'\n",
    "    data_target_path = max_path + '/' + project_name + '/weight/' + market + '_data_target'\n",
    "    ims_gr_path = max_path + '/' + project_name + '/weight/' + market + '_ims_gr'\n",
    "\n",
    "    # 输出\n",
    "    df_sum_path = max_path + '/' + project_name + '/weight/' + market + '_share_gr_out'\n",
    "    df_weight_path = max_path + '/' + project_name + '/weight/' + market + '_weight_raw_out'\n",
    "\n",
    "    # 1. 该市场所需要的分析的城市\n",
    "    city_brand_dict = market_city_brand_dict[market]\n",
    "    city_list = list(city_brand_dict.keys())\n",
    "\n",
    "    # 2. 利用ims_sales_gr，生成每个城市 top 产品的 'gr','share','share_ly' 字典\n",
    "    ims_sales_gr = spark.read.parquet(ims_gr_path)\n",
    "    ims_sales_gr_city = ims_sales_gr.where(col('City').isin(city_list))\n",
    "    target_share = ims_sales_gr_city.groupBy('City').apply(udf_target_brand)\n",
    "\n",
    "    # 转化为字典格式\n",
    "    df_target_share = target_share.agg(func.collect_list('dict').alias('dict_all')).select(\"dict_all\").toPandas()\n",
    "    df_target_share = df_target_share[\"dict_all\"].values[0]\n",
    "    length_dict = len(df_target_share)\n",
    "    str_target_share = \"\"\n",
    "    for index, each in enumerate(df_target_share):\n",
    "        if index == 0:\n",
    "            str_target_share += \"{\" + each[1:-1]\n",
    "        elif index == length_dict - 1:\n",
    "            str_target_share += \",\" + each[1:-1] + \"}\"\n",
    "        else:\n",
    "            str_target_share += \",\" + each[1:-1]\n",
    "\n",
    "        if length_dict == 1:\n",
    "            str_target_share += \"}\"\n",
    "    dict_target_share  = json.loads(str_target_share)\n",
    "\n",
    "    # 3. 对 data_target 进行weight分析\n",
    "    data_target = spark.read.parquet(data_target_path).where(col('City').isin(city_list))\n",
    "    df_weight_out = data_target.groupBy('City').apply(udf_target_weight).persist()\n",
    "    df_weight_out = df_weight_out.withColumn('weight', col('weight').cast(DoubleType())) \\\n",
    "                       .withColumn('weight_factor', col('weight_factor').cast(DoubleType())) \\\n",
    "                       .withColumn('weight_factor1', col('weight_factor1').cast(DoubleType())) \\\n",
    "                       .withColumn('Bedsize>99', col('Bedsize>99').cast(DoubleType())) \\\n",
    "                       .withColumn('W', col('W').cast(DoubleType())) \n",
    "\n",
    "    # 4. 输出结果整理\n",
    "    # 4.1 原始的weight结果\n",
    "    df_weight_out = df_weight_out.repartition(1)\n",
    "    df_weight_out.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(df_weight_path)\n",
    "\n",
    "    # 4.2 用于生产的weight结果\n",
    "    df_weight_final = df_weight_out.withColumn('weight_factor', func.when(col('Bedsize>99')==0, col('weight_factor1')) \\\n",
    "                                                                    .otherwise((col('W')-1)/(col('weight')-1)))\n",
    "\n",
    "    df_weight_final = df_weight_final.fillna(0, 'weight_factor') \\\n",
    "                                    .withColumn('DOI', func.lit(market)) \\\n",
    "                                    .select('PHA', 'weight_factor', 'DOI', 'City') \\\n",
    "                                    .join(universe, on='City', how='left') \\\n",
    "                                    .withColumnRenamed('weight_factor', 'weight')\n",
    "\n",
    "    df_weight_final = df_weight_final.withColumn('Province', func.when(col('City')=='福厦泉', func.lit('福建省')) \\\n",
    "                                                                .otherwise(col('Province'))) \\\n",
    "                                     .withColumn('Province', func.when(col('City')=='珠三角', func.lit('广东省')) \\\n",
    "                                                                .otherwise(col('Province'))) \\\n",
    "                                     .withColumn('Province', func.when(col('City')=='浙江市', func.lit('浙江省')) \\\n",
    "                                                                .otherwise(col('Province'))) \\\n",
    "                                    .withColumn('Province', func.when(col('City')=='苏锡市', func.lit('江苏省')) \\\n",
    "                                                                .otherwise(col('Province'))) \n",
    "\n",
    "    if index_file == 0:\n",
    "        df_weight_final = df_weight_final.repartition(1)\n",
    "        df_weight_final.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(weight_tmp_path)\n",
    "    else:\n",
    "        df_weight_final = df_weight_final.repartition(1)\n",
    "        df_weight_final.write.format(\"parquet\") \\\n",
    "            .mode(\"append\").save(weight_tmp_path)\n",
    "\n",
    "\n",
    "    # 4.3 share 和 gr 结果\n",
    "    data_final = data_target.join(df_weight_out.select('PHA','W'), on='PHA', how='left')\n",
    "    data_final = data_final.withColumn('MAX_new', col('Sales')*col('W'))\n",
    "\n",
    "    df_sum = data_final.groupBy('City','标准商品名','Year').agg(func.sum('MAX_new').alias('MAX_new')).persist()\n",
    "    df_sum = df_sum.groupBy('City', '标准商品名').pivot('Year').agg(func.sum('MAX_new')).fillna(0).persist()\n",
    "    \n",
    "    df_sum_city = df_sum.groupBy('City').agg(func.sum(year_min).alias('str_sum_'+year_min), func.sum(year_max).alias('str_sum_'+year_max))\n",
    "\n",
    "    df_sum = df_sum.join(df_sum_city, on='City', how='left')\n",
    "    #str_sum_2018 = df_sum.agg(func.sum('2018').alias('sum')).collect()[0][0]\n",
    "    #str_sum_2019 = df_sum.agg(func.sum('2019').alias('sum')).collect()[0][0]\n",
    "    df_sum = df_sum.withColumn('Share_'+year_min, func.bround(col(year_min)/col('str_sum_'+year_min), 3)) \\\n",
    "                   .withColumn('Share_'+year_max, func.bround(col(year_max)/col('str_sum_'+year_max), 3)) \\\n",
    "                   .withColumn('GR', func.bround(col(year_max)/col(year_min)-1, 3)) \\\n",
    "                    .drop('str_sum_'+year_min, 'str_sum_'+year_max)\n",
    "\n",
    "    df_sum = df_sum.repartition(1)\n",
    "    df_sum.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\").save(df_sum_path)\n",
    "\n",
    "    index_file += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====  四. 数据处理  ====\n",
    "\n",
    "# 1、福夏泉，珠三角 城市展开\n",
    "df_weight_final = spark.read.parquet(weight_tmp_path)\n",
    "citys = df_weight_final.select('City').distinct().toPandas()['City'].tolist()\n",
    "\n",
    "if '福厦泉' or '珠三角' or \"浙江市\" or \"苏锡市\" in citys:\n",
    "    df_keep = df_weight_final.where(~col('City').isin('福厦泉', '珠三角', \"浙江市\", \"苏锡市\"))\n",
    "    \n",
    "    if '福厦泉' in citys:\n",
    "        df1 = df_weight_final.where(col('City') == '福厦泉')\n",
    "        df1_1 = df1.withColumn('City', func.lit('福州市'))\n",
    "        df1_2 = df1.withColumn('City', func.lit('厦门市'))\n",
    "        df1_3 = df1.withColumn('City', func.lit('泉州市'))\n",
    "        df1_new = df1_1.union(df1_2).union(df1_3)\n",
    "        # 合并\n",
    "        df_keep = df_keep.union(df1_new)\n",
    "\n",
    "    if '珠三角' in citys:\n",
    "        df2 = df_weight_final.where(col('City') == '珠三角')\n",
    "        df2_1 = df2.withColumn('City', func.lit('珠海市'))\n",
    "        df2_2 = df2.withColumn('City', func.lit('东莞市'))\n",
    "        df2_3 = df2.withColumn('City', func.lit('中山市'))\n",
    "        df2_4 = df1.withColumn('City', func.lit('佛山市'))\n",
    "        df2_new = df2_1.union(df2_2).union(df2_3).union(df2_4)\n",
    "        # 合并\n",
    "        df_keep = df_keep.union(df2_new)\n",
    "        \n",
    "    if '浙江市' in citys:\n",
    "        df3 = df_weight_final.where(col('City') == '浙江市')\n",
    "        df3_1 = df3.withColumn('City', func.lit('绍兴市'))\n",
    "        df3_2 = df3.withColumn('City', func.lit('嘉兴市'))\n",
    "        df3_3 = df3.withColumn('City', func.lit('台州市'))\n",
    "        df3_4 = df3.withColumn('City', func.lit('金华市'))\n",
    "        df3_new = df3_1.union(df3_2).union(df3_3).union(df3_4)\n",
    "        # 合并\n",
    "        df_keep = df_keep.union(df3_new)\n",
    "\n",
    "    if '苏锡市' in citys:\n",
    "        df4 = df_weight_final.where(col('City') == '苏锡市')\n",
    "        df4_1 = df4.withColumn('City', func.lit('苏州市'))\n",
    "        df4_2 = df4.withColumn('City', func.lit('无锡市'))\n",
    "        df4_new = df4_1.union(df4_2)\n",
    "        # 合并\n",
    "        df_keep = df_keep.union(df4_new)\n",
    "\n",
    "    df_weight_final = df_keep     \n",
    "\n",
    "# 2、输出判断是否已有 weight_path 结果，对已有 weight_path 结果替换或者补充\n",
    "'''\n",
    "如果已经存在 weight_path 则用新的结果对已有结果进行(Province,City,DOI)替换和补充\n",
    "'''\n",
    "\n",
    "file_name = weight_path.replace('//', '/').split('s3a:/ph-max-auto/')[1]\n",
    "\n",
    "s3 = boto3.resource('s3', region_name='cn-northwest-1',\n",
    "                        aws_access_key_id=\"AKIAWPBDTVEAEU44ZAGT\",\n",
    "                        aws_secret_access_key=\"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\")\n",
    "bucket = s3.Bucket('ph-max-auto')\n",
    "judge = 0\n",
    "for obj in bucket.objects.filter(Prefix = file_name):\n",
    "    path, filename = os.path.split(obj.key)  \n",
    "    if path == file_name:\n",
    "        judge += 1\n",
    "if judge > 0:\n",
    "    old_out = spark.read.parquet(weight_path)   \n",
    "    new_info = df_weight_final.select('Province', 'City', 'DOI').distinct()\n",
    "    old_out_keep = old_out.join(new_info, on=['Province', 'City', 'DOI'], how='left_anti')\n",
    "    df_weight_final = df_weight_final.union(old_out_keep.select(df_weight_final.columns))           \n",
    "    # 中间文件读写一下\n",
    "    df_weight_final = df_weight_final.repartition(2)\n",
    "    df_weight_final.write.format(\"parquet\") \\\n",
    "                        .mode(\"overwrite\").save(tmp_path)\n",
    "    df_weight_final = spark.read.parquet(tmp_path)   \n",
    "\n",
    "# 3、输出到 weight_path\n",
    "df_weight_final = df_weight_final.repartition(2)\n",
    "df_weight_final.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\").save(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
