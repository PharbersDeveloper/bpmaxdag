{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"readfile\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 10.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "a = 123\n",
    "b = 456\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ENV\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWPBDTVEAEU44ZAGT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"cn-northwest-1\"\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Jupyter Keep-Alive Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.parquet(\"s3a://ph-stream/common/public/universe/0.0.1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inter = spark.read.parquet(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/runs/manual__2021-01-19T05_48_49.058508+00_00/new_inter_0120/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|    MOLE_NAME_LOST|  MOLE_NAME_STANDARD|\n",
      "+------------------+--------------------+\n",
      "|      硝呋太尔胶囊|            硝呋太尔|\n",
      "|          替勃龙片|              替勃龙|\n",
      "| 硫酸钡混悬液(Ⅰ型)|              硫酸钡|\n",
      "|        西地碘含片|              西地碘|\n",
      "|  炔雌醇环丙孕酮片|     炔雌醇,环丙孕酮|\n",
      "|            达那唑|              达那唑|\n",
      "|            白介素|     重组人白介素-11|\n",
      "|  四烯甲萘醌软胶囊|          四烯甲萘醌|\n",
      "|    羧甲淀粉钠溶液|          羧甲淀粉钠|\n",
      "|        咪唑立宾片|            咪唑立宾|\n",
      "|            桉柠蒎|桉油精,柠檬烯,α-蒎烯|\n",
      "|        愈美缓释片|  复方氢溴酸右美沙芬|\n",
      "|          卡莫氟片|              卡莫氟|\n",
      "|    呋喃妥因肠溶片|            呋喃妥因|\n",
      "|  制霉素阴道泡腾片|              制霉素|\n",
      "|胶体果胶铋干混悬剂|          胶体果胶铋|\n",
      "|醋酸甲地孕酮分散片|            甲地孕酮|\n",
      "|        地屈孕酮片|            地屈孕酮|\n",
      "|          利可君片|              利可君|\n",
      "|          赖诺普利|            赖诺普利|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_inter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_inter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inter.repartition(1).write.parquet(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/data/DF_CONF/0.1.1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc = spark.read.parquet(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/data/CHC/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58336"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MOLE_NAME',\n",
       " 'PRODUCT_NAME',\n",
       " 'DOSAGE',\n",
       " 'SPEC',\n",
       " 'MANUFACTURER_NAME',\n",
       " 'PACK_QTY',\n",
       " 'PACK_ID_CHECK',\n",
       " 'code']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc.select(\"SPEC\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc_spec = df_chc.select(\"SPEC\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|                           SPEC|\n",
      "+-------------------------------+\n",
      "|                         3G/8丸|\n",
      "|   15G:曲安奈德15MG,硝酸益康...|\n",
      "|                       1ML:40MG|\n",
      "|                   5G(20%×25ML)|\n",
      "|                      100ML:67G|\n",
      "|0.1G(以沙库巴曲缬沙坦计,沙库...|\n",
      "|                   50ΜG×48片/盒|\n",
      "|                   35MG×30粒/盒|\n",
      "|                   90MG×14片/盒|\n",
      "|    替加氟25MG,吉美嘧啶7.25M...|\n",
      "|                  0.34G×60片/瓶|\n",
      "|                     4MG×1支/支|\n",
      "|                8ML:0.6G×1瓶/盒|\n",
      "|                    0.1G×7枚/盒|\n",
      "|             43MG(含葛根0.417G)|\n",
      "|                    1ML:500万IU|\n",
      "|                          250ΜG|\n",
      "|                 0.1G(10万单位)|\n",
      "|                   5.0MG×1支/支|\n",
      "|                1.6G(1.5G/0.1G)|\n",
      "+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chc_spec.repartition(1).write.mode('overwrite').csv(\"s3a://ph-max-auto/2020-08-11/data_matching/temp/mzhang\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc_spec_ = spark.read.parquet(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/runs/manual__2021-01-25T08_33_42.952864+00_00/cleaning_data_model_predictions/prediction_result/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sid',\n",
       " 'EFFTIVENESS_MOLE_NAME',\n",
       " 'EFFTIVENESS_PRODUCT_NAME',\n",
       " 'EFFTIVENESS_DOSAGE',\n",
       " 'EFFTIVENESS_SPEC',\n",
       " 'EFFTIVENESS_PACK_QTY',\n",
       " 'EFFTIVENESS_MANUFACTURER',\n",
       " 'id',\n",
       " 'PACK_ID_CHECK',\n",
       " 'PACK_ID_STANDARD',\n",
       " 'DOSAGE',\n",
       " 'MOLE_NAME',\n",
       " 'PRODUCT_NAME',\n",
       " 'SPEC',\n",
       " 'PACK_QTY',\n",
       " 'MANUFACTURER_NAME',\n",
       " 'SPEC_ORIGINAL',\n",
       " 'MOLE_NAME_STANDARD',\n",
       " 'PRODUCT_NAME_STANDARD',\n",
       " 'CORP_NAME_STANDARD',\n",
       " 'MANUFACTURER_NAME_STANDARD',\n",
       " 'MANUFACTURER_NAME_EN_STANDARD',\n",
       " 'DOSAGE_STANDARD',\n",
       " 'SPEC_STANDARD',\n",
       " 'PACK_QTY_STANDARD',\n",
       " 'SPEC_valid_digit_STANDARD',\n",
       " 'SPEC_valid_unit_STANDARD',\n",
       " 'SPEC_gross_digit_STANDARD',\n",
       " 'SPEC_gross_unit_STANDARD',\n",
       " 'SPEC_STANDARD_ORIGINAL',\n",
       " 'label',\n",
       " 'indexedLabel',\n",
       " 'prediction',\n",
       " 'SIMILARITY',\n",
       " 'RANK']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc_spec_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc_spec_distinct = df_chc_spec_.select(\"SPEC_STANDARD_ORIGINAL\",\"SPEC_valid_digit_STANDARD\",\"SPEC_valid_unit_STANDARD\",\"SPEC_gross_digit_STANDARD\",\"SPEC_gross_unit_STANDARD\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc_spec_distinct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------------+------------------------+-------------------------+------------------------+\n",
      "|SPEC_STANDARD_ORIGINAL|SPEC_valid_digit_STANDARD|SPEC_valid_unit_STANDARD|SPEC_gross_digit_STANDARD|SPEC_gross_unit_STANDARD|\n",
      "+----------------------+-------------------------+------------------------+-------------------------+------------------------+\n",
      "|                 120MG|                    120.0|                      MG|                         |                        |\n",
      "|                  40MG|                       40|                      MG|                         |                        |\n",
      "|            0.9% 250ML|                   2250.0|                      MG|                    250.0|                      ML|\n",
      "|            64.7% 10ML|                   6470.0|                      MG|                     10.0|                      ML|\n",
      "|           5.6MG 0.4ML|                      5.6|                      MG|                      0.4|                      ML|\n",
      "|               30MG 1G|                     30.0|                      MG|                   1000.0|                      MG|\n",
      "|                 5% 5G|                      250|                      MG|                     5000|                      MG|\n",
      "|              50MG 5ML|                       50|                      MG|                        5|                      ML|\n",
      "|              90MG+4MG|                     90,4|                      MG|                         |                        |\n",
      "|                 1.6MG|                         |                        |                      1.6|                      MG|\n",
      "|             0.5MG 1ML|                      0.5|                      MG|                        1|                      ML|\n",
      "|     17.5MG+2.5MG 10ML|                 17.5,2.5|                      MG|                     10.0|                      ML|\n",
      "|               502.5MG|                      502|                      MG|                         |                        |\n",
      "|              CO 13.7G|                  13700.0|                      MG|                         |                        |\n",
      "|                    5G|                         |                        |                   5000.0|                      MG|\n",
      "|             5000U 2ML|                     5000|                       U|                        2|                      ML|\n",
      "|              7% 250ML|                  17500.0|                      MG|                    250.0|                      ML|\n",
      "|                 215MG|                      215|                      MG|                         |                        |\n",
      "|              10G 50ML|                  10000.0|                      MG|                     50.0|                      ML|\n",
      "|             200MG 20G|                    200.0|                      MG|                  20000.0|                      MG|\n",
      "+----------------------+-------------------------+------------------------+-------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_spec_distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|SPEC_valid_unit_STANDARD|\n",
      "+------------------------+\n",
      "|                       U|\n",
      "|                      MG|\n",
      "|                     PNA|\n",
      "|                      MC|\n",
      "|                      KE|\n",
      "|                      ML|\n",
      "|                      UG|\n",
      "|                        |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_spec_distinct.select(\"SPEC_valid_unit_STANDARD\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|SPEC_gross_unit_STANDARD|\n",
      "+------------------------+\n",
      "|                       U|\n",
      "|                      CM|\n",
      "|                      喷|\n",
      "|                      MG|\n",
      "|                      ML|\n",
      "|                        |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_spec_distinct.select(\"SPEC_gross_unit_STANDARD\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc_pre = spark.read.csv(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/runs/manual__2021-01-25T08_33_42.952864+00_00/data_matching_model_data_collect/prediction_origin_result/\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sid',\n",
       " 'EFFTIVENESS_MOLE_NAME',\n",
       " 'EFFTIVENESS_PRODUCT_NAME',\n",
       " 'EFFTIVENESS_DOSAGE',\n",
       " 'EFFTIVENESS_SPEC',\n",
       " 'EFFTIVENESS_PACK_QTY',\n",
       " 'EFFTIVENESS_MANUFACTURER',\n",
       " 'id',\n",
       " 'PACK_ID_CHECK',\n",
       " 'PACK_ID_STANDARD',\n",
       " 'DOSAGE',\n",
       " 'MOLE_NAME',\n",
       " 'PRODUCT_NAME',\n",
       " 'SPEC',\n",
       " 'PACK_QTY',\n",
       " 'MANUFACTURER_NAME',\n",
       " 'SPEC_ORIGINAL',\n",
       " 'MOLE_NAME_STANDARD',\n",
       " 'PRODUCT_NAME_STANDARD',\n",
       " 'CORP_NAME_STANDARD',\n",
       " 'MANUFACTURER_NAME_STANDARD',\n",
       " 'MANUFACTURER_NAME_EN_STANDARD',\n",
       " 'DOSAGE_STANDARD',\n",
       " 'SPEC_STANDARD',\n",
       " 'PACK_QTY_STANDARD',\n",
       " 'SPEC_valid_digit_STANDARD',\n",
       " 'SPEC_valid_unit_STANDARD',\n",
       " 'SPEC_gross_digit_STANDARD',\n",
       " 'SPEC_gross_unit_STANDARD',\n",
       " 'SPEC_STANDARD_ORIGINAL',\n",
       " 'label',\n",
       " 'indexedLabel',\n",
       " 'prediction',\n",
       " 'SIMILARITY',\n",
       " 'RANK',\n",
       " 'MOLE_NAME_origin',\n",
       " 'PRODUCT_NAME_origin',\n",
       " 'DOSAGE_origin',\n",
       " 'SPEC_origin',\n",
       " 'MANUFACTURER_NAME_origin',\n",
       " 'PACK_QTY_origin',\n",
       " 'PACK_ID_CHECK_origin',\n",
       " 'code_origin',\n",
       " 'id_origin']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chc_pre.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------------------+\n",
      "|SPEC_valid_unit_STANDARD|count(SPEC_valid_unit_STANDARD)|\n",
      "+------------------------+-------------------------------+\n",
      "|                    null|                              0|\n",
      "|                       U|                          11142|\n",
      "|                      MG|                         184211|\n",
      "|                     PNA|                             12|\n",
      "|                      MC|                              9|\n",
      "|                      KE|                             16|\n",
      "|                      ML|                             16|\n",
      "|                      UG|                             23|\n",
      "+------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions  import count\n",
    "df_chc_pre.groupBy(\"SPEC_valid_unit_STANDARD\").agg(count(\"SPEC_valid_unit_STANDARD\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------------------+\n",
      "|SPEC_gross_unit_STANDARD|count(SPEC_valid_unit_STANDARD)|\n",
      "+------------------------+-------------------------------+\n",
      "|                    null|                         116371|\n",
      "|                       U|                            139|\n",
      "|                      CM|                             11|\n",
      "|                      喷|                           1689|\n",
      "|                      MG|                           8988|\n",
      "|                      ML|                          68231|\n",
      "+------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_pre.groupBy(\"SPEC_gross_unit_STANDARD\").agg(count(\"SPEC_valid_unit_STANDARD\")).show()\n",
    "# df_chc_pre.select(\"SPEC_gross_unit_STANDARD\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chc_percent = df_chc_spec.withColumn(\"SPEC_percent\", regexp_extract('SPEC', r'(\\d{1,3}[.]{0,1}\\d+%)', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SPEC: string (nullable = true)\n",
      " |-- SPEC_valid: string (nullable = true)\n",
      " |-- SPEC_valid_: string (nullable = true)\n",
      " |-- SPEC_co: string (nullable = true)\n",
      " |-- SPEC_percent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_chc_percent.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 110, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 99, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b0c51f530c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspec_valid_regex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'([0-9]\\d*\\.?\\d*\\s*[A-Za-z]*/?\\s*[A-Za-z]+)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_chc_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_chc_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SPEC_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SPEC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_valid_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_chc_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 110, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 99, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "spec_valid_regex = r'([0-9]\\d*\\.?\\d*\\s*[A-Za-z]*/?\\s*[A-Za-z]+)'\n",
    "df_chc_spec = df_chc_spec.withColumn(\"SPEC_valid\", regexp_extract('SPEC', spec_valid_regex, 1))\n",
    "df_chc_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@pandas_udf(StringType(), PandasUDFType.SCALAR)\n",
    "def transfer_unit_pandas_udf(value):\n",
    "\tdef unit_trans(value, unit):\n",
    "\t\t# value transform\n",
    "\t\tif unit == \"G\" or unit == \"GM\":\n",
    "\t\t\tvalue = value *1000\n",
    "\t\telif unit == \"UG\" or unit == \"UG/DOS\":\n",
    "\t\t\tvalue = value /1000\n",
    "\t\telif unit == \"L\":\n",
    "\t\t\tvalue = value *1000\n",
    "\t\telif unit == \"TU\" or unit == \"TIU\":\n",
    "\t\t\tvalue = value *10000\n",
    "\t\telif unit == \"MU\" or unit == \"MIU\" or unit == \"M\":\n",
    "\t\t\tvalue = value *1000000\n",
    "\t\telif (unit == \"Y\"):\n",
    "\t\t\tvalue = value /1000\n",
    "\t\tif value >= 1:\n",
    "\t\t\tvalue = round(value, 1)\n",
    "\t\telse:\n",
    "\t\t\tvalue = value\n",
    "\n",
    "\t\t# unit transform\n",
    "\t\tunit_switch = {\n",
    "\t\t\t\t\"G\": \"MG\",\n",
    "\t\t\t\t\"GM\": \"MG\",\n",
    "\t\t\t\t\"MG\": \"MG\",\n",
    "\t\t\t\t\"UG\": \"MG\",\n",
    "\t\t\t\t\"L\": \"ML\",\n",
    "\t\t\t\t\"AXAU\": \"U\",\n",
    "\t\t\t\t\"AXAIU\": \"U\",\n",
    "\t\t\t\t\"IU\": \"U\",\n",
    "\t\t\t\t\"TU\": \"U\",\n",
    "\t\t\t\t\"TIU\": \"U\",\n",
    "\t\t\t\t\"MU\": \"U\",\n",
    "\t\t\t\t\"MIU\": \"U\",\n",
    "\t\t\t\t\"M\": \"U\",\n",
    "\t\t\t\t\"Y\": \"MG\",\n",
    "\t\t\t\t\"MC\": \"MC\",\n",
    "\t\t\t}\n",
    "\t\ttry:\n",
    "\t\t\tunit = unit_switch[unit]\n",
    "\t\texcept KeyError:\n",
    "\t\t\tpass\n",
    "\t\treturn value, unit\n",
    "\n",
    "    \n",
    "    \n",
    "df_chc_spec = df_chc_spec.withColumn(\"SPEC_valid_\", transfer_unit_pandas_udf(df_chc_spec.SPEC_valid))\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 110, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 99, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9ad5c21294e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_chc_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 110, in <lambda>\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File \"/tmp/hadoop/nm-local-dir/usercache/mzhang/appcache/application_1611542550459_0102/container_1611542550459_0102_01_000003/pyspark.zip/pyspark/worker.py\", line 99, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "df_chc_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------------+------------+----------------+-------------------+\n",
      "|               title|数据总数|进入匹配流程条目|机器匹配条目|机器无法匹配条目|             匹配率|\n",
      "+--------------------+--------+----------------+------------+----------------+-------------------+\n",
      "|data_matching_report|  207563|           46087|       31675|          175888|0.15260426954707726|\n",
      "+--------------------+--------+----------------+------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"s3a://ph-max-auto/2020-08-11/data_matching/refactor/results/2021-01-26_07-01-39/Report/\",header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65875"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stand = spark.read.parquet(\"s3a://ph-stream/common/public/prod/0.0.21\")\n",
    "df_stand.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACK_ID',\n",
       " 'MOLE_NAME_EN',\n",
       " 'MOLE_NAME_CH',\n",
       " 'PROD_DESC',\n",
       " 'PROD_NAME_CH',\n",
       " 'CORP_NAME_EN',\n",
       " 'CORP_NAME_CH',\n",
       " 'MNF_NAME_EN',\n",
       " 'MNF_NAME_CH',\n",
       " 'PCK_DESC',\n",
       " 'DOSAGE',\n",
       " 'SPEC',\n",
       " 'SPEC_valid_digit',\n",
       " 'SPEC_valid_unit',\n",
       " 'SPEC_gross_digit',\n",
       " 'SPEC_gross_unit',\n",
       " 'PACK',\n",
       " 'NFC1',\n",
       " 'NFC1_NAME',\n",
       " 'NFC1_NAME_CH',\n",
       " 'NFC12',\n",
       " 'NFC12_NAME',\n",
       " 'NFC12_NAME_CH',\n",
       " 'NFC123',\n",
       " 'NFC123_NAME',\n",
       " 'CORP_ID',\n",
       " 'MNF_TYPE',\n",
       " 'MNF_TYPE_NAME',\n",
       " 'MNF_TYPE_NAME_CH',\n",
       " 'MNF_ID',\n",
       " 'ATC1_CODE',\n",
       " 'ATC1_DESC',\n",
       " 'ATC2_CODE',\n",
       " 'ATC2_DESC',\n",
       " 'ATC3_CODE',\n",
       " 'ATC3_DESC',\n",
       " 'ATC4_CODE',\n",
       " 'ATC4_DESC',\n",
       " 'ATC1',\n",
       " 'ATC2',\n",
       " 'ATC3',\n",
       " 'ATC4',\n",
       " 'PckLaunchDate',\n",
       " 'PrdLaunchDate',\n",
       " 'REMARK']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec = df_stand.select(\"SPEC_valid_digit\",\"SPEC_valid_unit\",\"SPEC_gross_digit\",\"SPEC_gross_unit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+----------------+---------------+\n",
      "|SPEC_valid_digit|SPEC_valid_unit|SPEC_gross_digit|SPEC_gross_unit|\n",
      "+----------------+---------------+----------------+---------------+\n",
      "|         50000.0|             MG|           250.0|             ML|\n",
      "|        100000.0|             MG|           500.0|             ML|\n",
      "|         20000.0|             MG|           100.0|             ML|\n",
      "|         50000.0|             MG|           250.0|             ML|\n",
      "|         10000.0|             MG|           100.0|             ML|\n",
      "|         25000.0|             MG|           250.0|             ML|\n",
      "|         50000.0|             MG|           250.0|             ML|\n",
      "|         50000.0|             MG|           500.0|             ML|\n",
      "|         25000.0|             MG|           250.0|             ML|\n",
      "|         20000.0|             MG|           100.0|             ML|\n",
      "|         50000.0|             MG|           250.0|             ML|\n",
      "|         25000.0|             MG|           250.0|             ML|\n",
      "|         10000.0|             MG|           100.0|             ML|\n",
      "|         20000.0|             MG|           100.0|             ML|\n",
      "|         50000.0|             MG|           250.0|             ML|\n",
      "|         10000.0|             MG|            null|           null|\n",
      "|             1.0|             MG|            null|           null|\n",
      "|             1.0|             MG|            null|           null|\n",
      "|           500.0|             MG|            null|           null|\n",
      "|             1.5|             MG|            null|           null|\n",
      "+----------------+---------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|SPEC_valid_unit|count_valid|\n",
      "+---------------+-----------+\n",
      "|           null|          0|\n",
      "|            PNA|          1|\n",
      "|             UG|          2|\n",
      "|             KE|          4|\n",
      "|             ML|          6|\n",
      "|             MC|         35|\n",
      "|              U|       2303|\n",
      "|             MG|      58514|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spec.groupBy(\"SPEC_valid_unit\").agg(count(\"SPEC_valid_unit\").alias('count_valid')).orderBy(\"count_valid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|SPEC_gross_unit|count_gross|\n",
      "+---------------+-----------+\n",
      "|           null|          0|\n",
      "|            CM2|          8|\n",
      "|              U|         15|\n",
      "|             CM|         58|\n",
      "|             MM|         93|\n",
      "|             喷|        166|\n",
      "|             MG|       2933|\n",
      "|             ML|      19741|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spec.groupBy(\"SPEC_gross_unit\").agg(count(\"SPEC_gross_unit\").alias('count_gross')).orderBy(\"count_gross\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+----------------+---------------+\n",
      "|SPEC_valid_digit|SPEC_valid_unit|SPEC_gross_digit|SPEC_gross_unit|\n",
      "+----------------+---------------+----------------+---------------+\n",
      "|             2.5|             MG|            30.0|             喷|\n",
      "|           200.0|              U|            14.0|             喷|\n",
      "|            0.02|             MG|            28.0|             喷|\n",
      "|            0.02|             MG|            16.0|             喷|\n",
      "|            50.0|              U|            20.0|             喷|\n",
      "|            50.0|              U|            40.0|             喷|\n",
      "|            50.0|              U|            60.0|             喷|\n",
      "|            0.05|             MG|            60.0|             喷|\n",
      "|            0.25|             MG|            80.0|             喷|\n",
      "|            0.01|             MG|           200.0|             喷|\n",
      "|            0.05|             MG|             200|             喷|\n",
      "|            0.05|             MG|             200|             喷|\n",
      "|            0.05|             MG|             100|             喷|\n",
      "|             0.1|             MG|             120|             喷|\n",
      "|             0.2|             MG|             120|             喷|\n",
      "|            0.05|             MG|             200|             喷|\n",
      "|            0.25|             MG|             200|             喷|\n",
      "|            0.25|             MG|             200|             喷|\n",
      "|            0.25|             MG|             400|             喷|\n",
      "|            1.25|             MG|             200|             喷|\n",
      "+----------------+---------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spec.filter(df_spec.SPEC_gross_unit==\"喷\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
