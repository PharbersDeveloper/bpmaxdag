{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Python Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"job3_factor_optimize\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = 'Empty'\n",
    "outdir = 'Empty'\n",
    "model_month_right = 'Empty'\n",
    "model_month_left = 'Empty'\n",
    "all_models = 'Empty'\n",
    "max_file = 'Empty'\n",
    "test = 'False'\n",
    "ims_info_auto = 'True'\n",
    "ims_version = 'Empty'\n",
    "add_imsinfo_path = 'Empty'\n",
    "geo_map_path = 'Empty'\n",
    "factor_optimize = 'True'\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "# YARN URL: http://161.189.223.227:8088/cluster\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"ywyuan write factor.job3_factor_optimize in jupyter using python3\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"AKIAWPBDTVEAEU44ZAGT\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"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\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nproject_name = '神州'\\noutdir = '201912'\\nmodel_month_right = '201912'\\nmodel_month_left = '201901'\\nall_models = 'SZ1'\\nmax_file = 'MAX_result_201801_201912_city_level'\\ntest = 'True'\\nims_version = '202010'\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "'''\n",
    "project_name = 'Takeda'\n",
    "outdir = '202012'\n",
    "model_month_right = '202012'\n",
    "model_month_left = '202001'\n",
    "all_models = 'TK1'\n",
    "max_file = 'MAX_result_201801_202012_city_level'\n",
    "test = 'True'\n",
    "ims_version = '202012'\n",
    "add_imsinfo_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/Takeda/add_ims_info.csv'\n",
    "geo_map_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/Takeda/geo_map.csv'\n",
    "\n",
    "project_name = '神州'\n",
    "outdir = '201912'\n",
    "model_month_right = '201912'\n",
    "model_month_left = '201901'\n",
    "all_models = 'SZ1'\n",
    "max_file = 'MAX_result_201801_201912_city_level'\n",
    "test = 'True'\n",
    "ims_version = '202010'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 输入\n",
    "if factor_optimize != \"True\":\n",
    "     raise ValueError('不进行优化')\n",
    "        \n",
    "if test != \"False\" and test != \"True\":\n",
    "    print('wrong input: test, False or True') \n",
    "    raise ValueError('wrong input: test, False or True')\n",
    "if ims_info_auto != \"False\" and ims_info_auto != \"True\":\n",
    "    print('wrong input: test, False or True') \n",
    "    raise ValueError('wrong input: test, False or True')\n",
    "\n",
    "model_month_right = int(model_month_right)\n",
    "model_month_left = int(model_month_left)\n",
    "all_models = all_models.replace(' ','').split(',')\n",
    "\n",
    "max_result_path = max_path + '/' + project_name + '/' + outdir + '/MAX_result/' + max_file\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "mkt_map_path = max_path + '/' + project_name + '/mkt_mapping'\n",
    "\n",
    "if ims_info_auto == 'True':\n",
    "    ims_mapping_path = max_path + '/' + '/Common_files/IMS_flat_files/' + ims_version + '/ims_mapping_' + ims_version + '.csv'\n",
    "    ims_sales_path = max_path + '/Common_files/IMS_flat_files/' + ims_version + '/cn_IMS_Sales_Fdata_' + ims_version + '_1.csv'\n",
    "    if geo_map_path == 'Empty':\n",
    "        geo_map_path = max_path + '/' + '/Common_files/IMS_flat_files/' + ims_version + '/cn_geog_dimn_' + ims_version + '_1.csv'\n",
    "    # 输出检查文件\n",
    "    check_path = max_path + '/' + project_name + '/ims_info/ims_info_check'\n",
    "    ims_sales_all_path = max_path + '/' + project_name + '/ims_info/ims_info_all'\n",
    "        \n",
    "# 输出在每个循环下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job3_factor_optimize\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =========== 数据执行 ============\n",
    "print(\"job3_factor_optimize\")\n",
    "# 1. max 文件处理\n",
    "max_result = spark.read.parquet(max_result_path)\n",
    "max_result = max_result.where((col('Date') >= model_month_left) & (col('Date') <= model_month_right))\n",
    "\n",
    "# 2. product_map 文件处理\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "#if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "#    if \"pfc\" not in product_map.columns:\n",
    "#        product_map = product_map.withColumnRenamed([i for i in product_map.columns if 'pfc' in i][0], \"pfc\")\n",
    "#if project_name == \"Eisai\":\n",
    "#    if \"pfc\" not in product_map.columns:\n",
    "#        product_map = product_map.withColumnRenamed([i for i in product_map.columns if 'pfc' in i][0], \"pfc\")\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准商品名\")\n",
    "    if i in [\"剂型_标准\", \"Form_std\", \"S_Dosage\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准剂型\")\n",
    "    if i in [\"规格_标准\", \"Specifications_std\", \"药品规格_标准\", \"S_Pack\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准规格\")\n",
    "    if i in [\"包装数量2\", \"包装数量_标准\", \"Pack_Number_std\", \"S_PackNumber\", \"最小包装数量\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准包装数量\")\n",
    "    if i in [\"标准企业\", \"生产企业_标准\", \"Manufacturer_std\", \"S_CORPORATION\", \"标准生产厂家\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"标准生产企业\")\n",
    "if project_name == \"Janssen\" or project_name == \"NHWA\":\n",
    "    if \"标准剂型\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"剂型\", \"标准剂型\")\n",
    "    if \"标准规格\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"规格\", \"标准规格\")\n",
    "    if \"标准生产企业\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"生产企业\", \"标准生产企业\")\n",
    "    if \"标准包装数量\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"包装数量\", \"标准包装数量\")\n",
    "product_map = product_map.withColumn(\"min2\", func.regexp_replace(\"min2\", \"&amp;\", \"&\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&lt;\", \"<\")) \\\n",
    "                        .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&gt;\", \">\"))\n",
    "\n",
    "product_map = product_map.select('通用名', '标准商品名', '标准剂型', '标准规格', '标准包装数量', \n",
    "                                    '标准生产企业', 'min2', 'pfc') \\\n",
    "                        .distinct() \\\n",
    "                        .withColumnRenamed('通用名', 'Molecule') \\\n",
    "                        .withColumnRenamed('标准商品名', 'Brand') \\\n",
    "                        .withColumnRenamed('标准剂型', 'Form') \\\n",
    "                        .withColumnRenamed('标准规格', 'Specifications') \\\n",
    "                        .withColumnRenamed('标准包装数量', 'Pack_Number') \\\n",
    "                        .withColumnRenamed('标准生产企业', 'Manufacturer') \\\n",
    "                        .withColumn('pfc', product_map.pfc.cast(IntegerType())) \\\n",
    "                        .withColumnRenamed('pfc', 'Pack_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 3. mkt_map_path\n",
    "mkt_map = spark.read.parquet(mkt_map_path)\n",
    "mkt_map = mkt_map.withColumnRenamed(\"标准通用名\", \"通用名\") \\\n",
    "    .withColumnRenamed(\"model\", \"mkt\") \\\n",
    "    .select(\"mkt\", \"通用名\").distinct()\n",
    "model_pfc = product_map.where(~col('Pack_ID').isNull()).select('Molecule', 'Pack_ID').distinct() \\\n",
    "                    .join(mkt_map, product_map['Molecule']==mkt_map['通用名'], how='left')\n",
    "model_pfc = model_pfc.select('Pack_ID', 'mkt').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4. ims文件\n",
    "if ims_info_auto == 'True':\n",
    "    @udf(StringType())\n",
    "    def city_change(name):\n",
    "        # 城市名定义\n",
    "        if name in [\"苏锡城市群\"]:\n",
    "            newname = \"苏锡市\"\n",
    "        elif name in [\"全国\"]:\n",
    "            newname = \"CHPA\"\n",
    "        else:\n",
    "            newname = name + '市'\n",
    "        return newname\n",
    "\n",
    "    # geo_map_path 匹配城市中文名\n",
    "    geo_map = spark.read.csv(geo_map_path, header=True)\n",
    "    geo_map = geo_map.select('GEO_CD', 'GEO_DESC_CN').distinct() \\\n",
    "                    .withColumnRenamed('GEO_CD', 'Geography_id')\n",
    "    # ims_mapping 匹配产品英文信息\n",
    "    ims_mapping = spark.read.csv(ims_mapping_path, header=True)\n",
    "    ims_mapping = ims_mapping.select('Molecule_Composition', 'Prd_desc', 'Pack_Id0').distinct() \\\n",
    "                            .withColumnRenamed('Pack_Id0', 'PACK_ID')\n",
    "    # ims 销量数据\n",
    "    ims_sales = spark.read.csv(ims_sales_path, header=True)\n",
    "    ims_sales = ims_sales.select('Geography_id', 'Pack_ID', 'Period_Code', 'LC')\n",
    "\n",
    "    # 是否补充ims 销量数据\n",
    "    if add_imsinfo_path != 'Empty':\n",
    "        add_imsinfo_file = spark.read.csv(add_imsinfo_path, header=True)\n",
    "        add_imsinfo_file = add_imsinfo_file.select('Geography_id', 'Pack_ID', 'Period_Code', 'LC')\n",
    "        # 去掉add_imsinfo_file中有的\n",
    "        ims_sales_keep = ims_sales.join(add_imsinfo_file, on=[\"Pack_ID\", \"Geography_id\"], how='left_anti')\n",
    "        ims_sales = ims_sales_keep.union(add_imsinfo_file.select(ims_sales_keep.columns))\n",
    "\n",
    "    # 信息匹配\n",
    "    ims_sales = ims_sales.join(ims_mapping, on='Pack_ID', how='left') \\\n",
    "                        .join(model_pfc, on='Pack_ID', how='left') \\\n",
    "                        .join(geo_map, on='Geography_id', how='left')\n",
    "\n",
    "    ims_sales = ims_sales.withColumn('Date', func.regexp_replace('Period_Code', 'M', '')) \\\n",
    "                        .withColumn('City', city_change(col('GEO_DESC_CN'))) \\\n",
    "                        .where(col('Date').between(model_month_left, model_month_right))\n",
    "\n",
    "    # 检查文件同一个分子是否有没匹配上的packid\n",
    "    check = ims_sales.where(col('City') == 'CHPA') \\\n",
    "                    .groupby('Molecule_Composition', 'mkt').agg(func.sum('LC').alias('Sales'))\n",
    "    check_mol = check.groupby('Molecule_Composition').agg(func.sum('Sales').alias('Sales_mol'))\n",
    "    check = check.join(check_mol, on='Molecule_Composition', how='left')\n",
    "    check = check.withColumn('share', col('Sales')/col('Sales_mol'))\n",
    "    molecules_in_model = check.where(~col('mkt').isNull()).select('Molecule_Composition').distinct()\n",
    "    check = check.join(molecules_in_model, on='Molecule_Composition', how='inner')\n",
    "\n",
    "    # 根据分子名重新匹配市场名，补充上缺失的数据\n",
    "    ims_sales_all = ims_sales.drop('mkt') \\\n",
    "                            .join(check.where(~col('mkt').isNull()).select(\"Molecule_Composition\",\"mkt\").distinct(), \n",
    "                                  on='Molecule_Composition', how='left')\n",
    "    # 计算share\n",
    "    ims_sales_all = ims_sales_all.groupby('City', 'Pack_ID', 'Prd_desc', 'Molecule_Composition', 'mkt') \\\n",
    "                                .agg(func.sum('LC').alias('ims_poi_vol'))\n",
    "    ims_sales_tmp = ims_sales_all.groupby('City', 'mkt') \\\n",
    "                                .agg(func.sum('ims_poi_vol').alias('ims_poi_vol_all'))\n",
    "    ims_sales_all = ims_sales_all.join(ims_sales_tmp, on=['City', 'mkt'], how='left') \\\n",
    "                                .withColumn('ims_share', col('ims_poi_vol')/col('ims_poi_vol_all'))\n",
    "    # 整理\n",
    "    ims_sales_all = ims_sales_all.drop(\"ims_poi_vol_all\") \\\n",
    "                                .withColumnRenamed(\"mkt\", \"model\") \\\n",
    "                                .where(~col('model').isNull())\n",
    "\n",
    "    # 输出检查文件\n",
    "    check = check.repartition(1)\n",
    "    check.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(check_path)\n",
    "    \n",
    "    # 输出 ims_sales_all\n",
    "    ims_sales_all = ims_sales_all.repartition(2)\n",
    "    ims_sales_all.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(ims_sales_all_path)\n",
    "    \n",
    "    ims_sales_all = spark.read.parquet(ims_sales_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前market为:TK1\n",
      "cvxpy优化\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/pandas/group_ops.py:73: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish:TK1\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. 对每个市场优化factor\n",
    "@udf(StringType())\n",
    "def city_change(name):\n",
    "    # 城市名定义\n",
    "    if name in [\"福州市\", \"厦门市\", \"泉州市\"]:\n",
    "        newname = \"福厦泉市\"\n",
    "    elif name in [\"珠海市\", \"东莞市\", \"中山市\", \"佛山市\"]:\n",
    "        newname = \"珠三角市\"\n",
    "    elif name in [\"绍兴市\", \"嘉兴市\", \"台州市\", \"金华市\"]:\n",
    "        newname = \"浙江市\"\n",
    "    elif name in [\"苏州市\", \"无锡市\"]:\n",
    "        newname = \"苏锡市\"\n",
    "    else:\n",
    "        newname = name\n",
    "    return newname\n",
    "\n",
    "def unpivot(df, keys):\n",
    "    # 功能：数据宽变长\n",
    "    # 参数说明 df:dataframe,  keys 待转换表中需要保留的主键key，以list[]类型传入\n",
    "    # 转换是为了避免字段类不匹配，统一将数据转换为string类型，如果保证数据类型完全一致，可以省略该句\n",
    "    df = df.select(*[col(_).astype(\"string\") for _ in df.columns])\n",
    "    cols = [_ for _ in df.columns if _ not in keys]\n",
    "    stack_str = ','.join(map(lambda x: \"'%s', `%s`\" % (x, x), cols))\n",
    "    # feature, value 转换后的列名，可自定义\n",
    "    df = df.selectExpr(*keys, \"stack(%s, %s) as (feature, value)\" % (len(cols), stack_str))\n",
    "    return df\n",
    "\n",
    "# market = '固力康'\n",
    "for market in all_models:\n",
    "    print(\"当前market为:\" + str(market))\n",
    "    # 输入\n",
    "    if ims_info_auto != 'True':     \n",
    "        ims_info_path = max_path + '/' + project_name + '/ims_info/' +  market + '_ims_info'\n",
    "    factor_path = max_path + '/' + project_name + '/forest/' + market + '_factor_1'\n",
    "    # 输出\n",
    "    ims_v1_otherall_path = max_path + '/' + project_name + '/forest/' + market + '_top3_product.csv'\n",
    "    ims_panel_max_out_path = max_path + '/' + project_name + '/forest/' + market + '_factor_gap.csv'\n",
    "    if test == 'True':\n",
    "        factor_out_path = max_path + '/' + project_name + '/forest/factor/factor_' + market\n",
    "    else:\n",
    "        factor_out_path = max_path + '/' + project_name + '/factor/factor_' + market\n",
    "\n",
    "    # 数据执行\n",
    "    if ims_info_auto != 'True':\n",
    "        ims_info = spark.read.parquet(ims_info_path)\n",
    "        ims_info = ims_info.withColumnRenamed('city', 'City')\n",
    "    else:\n",
    "        ims_info = ims_sales_all.where(col('model') == market)\n",
    "\n",
    "    factor = spark.read.parquet(factor_path)\n",
    "\n",
    "    # 3.1 max 数据\n",
    "    max_df = max_result.where(col('DOI') == market)\n",
    "    max_df = max_df.join(factor, on='City', how='left')\n",
    "    max_df = max_df.withColumn('Predict_Sales', func.when(col('PANEL') == 0, col('factor')*col('Predict_Sales')) \\\n",
    "                                                    .otherwise(col('Predict_Sales'))) \\\n",
    "                    .withColumn('Citynew', col('City'))\n",
    "\n",
    "    max_df = max_df.withColumn('Citynew', city_change(col('City')))\n",
    "    max_df = max_df.withColumn('Citynew', func.when(~col('Citynew').isin(ims_info.select('City').distinct().toPandas()['City'].values.tolist()), \n",
    "                                                    func.lit('other')).otherwise(col('Citynew')))\n",
    "\n",
    "    max_df = max_df.join(product_map.dropDuplicates(['min2']), max_df.Prod_Name==product_map.min2, how='left')\n",
    "\n",
    "    # 城市产品层面\n",
    "    max2 = max_df.groupBy('Brand', 'Citynew').agg(func.sum('Predict_Sales').alias('max_Prod')) \\\n",
    "                .withColumn('mkt', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "    # 全国的市场\n",
    "    max3 = max_df.groupBy('Citynew').agg(func.sum('Predict_Sales').alias('max_mkt')) \\\n",
    "                .withColumn('Market', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "\n",
    "    # 3.2 panel数据\n",
    "    bll1 = max_df.where(max_df.PANEL == 1)\n",
    "    # 城市产品层面\n",
    "    bll2 = bll1.groupBy('Brand', 'Citynew').agg(func.sum('Predict_Sales').alias('panel_Sales')) \\\n",
    "                .withColumn('mkt', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "    # 全国的市场\n",
    "    bll3 = bll1.groupBy('Citynew').agg(func.sum('Predict_Sales').alias('panel_mkt')) \\\n",
    "                .withColumn('Market', func.lit(market)) \\\n",
    "                .withColumnRenamed('Citynew', 'City')\n",
    "\n",
    "    # 3.3 确定IMS三个最大的产品\n",
    "    ims_df = ims_info.withColumn('Market', func.lit(market))\n",
    "\n",
    "    ims_df = ims_df.withColumn('Brand_en', col('Prd_desc')) \\\n",
    "                        .join(product_map.select('Pack_ID', 'Brand').dropDuplicates(['Pack_ID']), on='Pack_ID', how='left')\n",
    "\n",
    "    ims_df = ims_df.groupBy('Market', 'Brand', 'City').agg(func.sum('ims_poi_vol').alias('Prod')) \\\n",
    "                    .withColumn('Brand', func.when(col('Brand').isNull(), func.lit('none')).otherwise(col('Brand')))\n",
    "\n",
    "    # 全国的商品销量\n",
    "    ims_part1 = ims_df.where(col('City') == 'CHPA')\n",
    "    ims_part1_v1 = ims_part1.groupBy('Market', 'Brand').agg(func.sum('Prod').alias('Prod_CHPA'))\n",
    "    # 非other 省份的商品销量\n",
    "    ims_part2 = ims_df.where(col('City') != 'CHPA')\n",
    "    ims_part2_v1 = ims_part2.groupBy('Market', 'Brand').agg(func.sum('Prod').alias('Prod_nonCHPA'))\n",
    "    # other 省份的商品销量\n",
    "    ims_v1_left = ims_part1_v1.join(ims_part2_v1, on=['Market','Brand'], how='left')\n",
    "    ims_v1_left = ims_v1_left.fillna(0, 'Prod_nonCHPA') \\\n",
    "                            .withColumn('Prod_other', col('Prod_CHPA')-col('Prod_nonCHPA')) \\\n",
    "                            .withColumn('City', func.lit('other'))\n",
    "    ims_v1_left = ims_v1_left.withColumnRenamed('Prod_CHPA', 'CHPA') \\\n",
    "                            .withColumnRenamed('Prod_nonCHPA', 'IMS_City') \\\n",
    "                            .withColumnRenamed('Prod_other', 'Prod') \\\n",
    "                            .select('Market','Brand','City','Prod')\n",
    "    # other + 非other省份 每个城市的销量前三名产品\n",
    "    ims_v1_otherall = ims_part2.union(ims_v1_left)\n",
    "    ims_v1_otherall = ims_v1_otherall.withColumn('n',\n",
    "                            func.row_number().over(Window.partitionBy('Market', 'City').orderBy(col('Prod').desc())))\n",
    "    ims_v1_otherall = ims_v1_otherall.where(col('n') < 4) \\\n",
    "                                    .orderBy(col('Market'), col('City'), col('Prod').desc()) \\\n",
    "                                    .withColumnRenamed('Prod', 'ims_Prod').persist()\n",
    "\n",
    "    ims_v1_mkt =  ims_part2.union(ims_v1_left) \\\n",
    "                            .groupBy('Market','City').agg(func.sum('Prod').alias('ims_mkt'))\n",
    "\n",
    "    ims_panel_max = ims_v1_otherall.join(max2.withColumnRenamed('mkt', 'Market'), \n",
    "                                        on=['Market', 'Brand', 'City'], how='left') \\\n",
    "                                    .join(bll2.withColumnRenamed('mkt', 'Market'),\n",
    "                                        on=['Market', 'Brand', 'City'], how='left')\n",
    "    ims_panel_max = unpivot(ims_panel_max, ['Market', 'Brand', 'City', 'n'])\n",
    "    ims_panel_max = ims_panel_max.withColumn('value', col('value').cast(DoubleType())) \\\n",
    "                                .withColumn('new', func.concat(col('feature'), func.lit('_'), col('n'))) \\\n",
    "                                .drop('Brand', 'feature', 'n')\n",
    "    ims_panel_max = ims_panel_max.groupBy('Market', 'City').pivot('new').agg(func.sum('value')).fillna(0).persist()\n",
    "    ims_panel_max = ims_panel_max.join(ims_v1_mkt, on=['Market','City'], how='left') \\\n",
    "                                .join(max3, on=['Market','City'], how='left') \\\n",
    "                                .join(bll3, on=['Market','City'], how='left')\n",
    "    ims_panel_max = ims_panel_max.fillna(0, ims_panel_max.columns[2:])\n",
    "\n",
    "    if 'ims_Prod_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('ims_Prod_2', func.lit(0))\n",
    "    if 'max_Prod_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('max_Prod_2', func.lit(0))\n",
    "    if 'panel_Sales_2' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('panel_Sales_2', func.lit(0))\n",
    "    if 'ims_Prod_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('ims_Prod_3', func.lit(0))\n",
    "    if 'max_Prod_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('max_Prod_3', func.lit(0))\n",
    "    if 'panel_Sales_3' not in ims_panel_max.columns:\n",
    "        ims_panel_max = ims_panel_max.withColumn('panel_Sales_3', func.lit(0))\n",
    "\n",
    "    # 3.4 优化 \n",
    "    print(\"cvxpy优化\")\n",
    "    schema = deepcopy(ims_panel_max.schema)\n",
    "    schema.add(\"factor\", DoubleType())\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def cvxpy_func(pdf):\n",
    "        import numpy as np\n",
    "        import cvxpy as cp\n",
    "\n",
    "        f = cp.Variable()\n",
    "        prob = cp.Problem(cp.Minimize(cp.maximum(cp.abs((f*(pdf['max_Prod_1'][0]-\n",
    "                                          pdf['panel_Sales_1'][0])+\n",
    "                                       pdf['panel_Sales_1'][0])/pdf['ims_Prod_1'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_Prod_2'][0]-\n",
    "                                          pdf['panel_Sales_2'][0])+\n",
    "                                       pdf['panel_Sales_2'][0])/pdf['ims_Prod_2'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_Prod_3'][0]-\n",
    "                                          pdf['panel_Sales_3'][0])+\n",
    "                                       pdf['panel_Sales_3'][0])/pdf['ims_Prod_3'][0]-1),\n",
    "                                cp.abs((f*(pdf['max_mkt'][0]-\n",
    "                                          pdf['panel_mkt'][0])+\n",
    "                                       pdf['panel_mkt'][0])/pdf['ims_mkt'][0]-1))),\n",
    "                   [cp.abs((f*(pdf['max_mkt'][0]-pdf['panel_mkt'][0])+\n",
    "                               pdf['panel_mkt'][0])/pdf['ims_mkt'][0]-1) <= 0.05])\n",
    "        prob.solve(solver = cp.ECOS)\n",
    "\n",
    "        return pdf.assign(factor = f.value)\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max.groupby('Market','City').apply(cvxpy_func)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('factor', \n",
    "                                            func.when((col('factor').isNull()) | (col('factor') < 0 ), func.lit(0)) \\\n",
    "                                                .otherwise(col('factor')))\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap1', \n",
    "            (col('factor')*(col('max_Prod_1') - col('panel_Sales_1')) + col('panel_Sales_1')) / col('ims_Prod_1') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap2', \n",
    "            (col('factor')*(col('max_Prod_2') - col('panel_Sales_2')) + col('panel_Sales_2')) / col('ims_Prod_2') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap3', \n",
    "            (col('factor')*(col('max_Prod_3') - col('panel_Sales_3')) + col('panel_Sales_3')) / col('ims_Prod_3') -1)\n",
    "    ims_panel_max_out = ims_panel_max_out.withColumn('gap_mkt', \n",
    "            (col('factor')*(col('max_mkt') - col('panel_mkt')) + col('panel_mkt')) / col('ims_mkt') -1)\n",
    "\n",
    "    # 输出\n",
    "    \n",
    "    ims_v1_otherall = ims_v1_otherall.repartition(1)\n",
    "    ims_v1_otherall.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(ims_v1_otherall_path)\n",
    "\n",
    "    ims_panel_max_out = ims_panel_max_out.repartition(1)\n",
    "    ims_panel_max_out.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\").save(ims_panel_max_out_path)\n",
    "    \n",
    "    factor1 = spark.read.parquet(factor_path)\n",
    "    factor1 = factor1.withColumnRenamed('factor', 'factor1')\n",
    "    factor2 = ims_panel_max_out.select('City','factor').withColumnRenamed('factor', 'factor2')\n",
    "    factor3 = factor1.join(factor2, on='City', how='left')\n",
    "\n",
    "    factor2_city = factor2.select('City').distinct().toPandas()['City'].values.tolist()\n",
    "\n",
    "    if \"福厦泉市\" in factor2_city:\n",
    "        value = factor2.where(col('City')=='福厦泉市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"福州市\",\"厦门市\",\"泉州市\"), func.lit(value)) \\\n",
    "                                                    .otherwise(col('factor2')))\n",
    "\n",
    "    if \"珠三角市\" in factor2_city:\n",
    "        value2 = factor2.where(col('City')=='珠三角市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"珠海市\",\"东莞市\",\"中山市\",\"佛山市\"), \n",
    "                                                    func.lit(value2)).otherwise(col('factor2')))\n",
    "\n",
    "    if \"浙江市\" in factor2_city:\n",
    "        value3 = factor2.where(col('City')=='浙江市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"绍兴市\",\"嘉兴市\",\"台州市\",\"金华市\"), \n",
    "                                                    func.lit(value3)).otherwise(col('factor2')))\n",
    "\n",
    "    if \"苏锡市\" in factor2_city:\n",
    "        value4 = factor2.where(col('City')=='苏锡市').select('factor2').toPandas()['factor2'][0]\n",
    "        factor3 = factor3.withColumn('factor2', func.when(col('City').isin(\"苏州市\",\"无锡市\"), \n",
    "                                                    func.lit(value4)).otherwise(col('factor2')))\n",
    "\n",
    "    value_other = factor2.where(col('City')=='other').select('factor2').toPandas()['factor2'][0]    \n",
    "    factor3 = factor3.withColumn('factor2', func.when(col('factor2').isNull(), func.lit(value_other)) \\\n",
    "                                                .otherwise(col('factor2')))\n",
    "\n",
    "    factor3 = factor3.withColumn('factor', col('factor1') * col('factor2'))\n",
    "    factor3 = factor3.withColumn('factor', func.when(col('factor') > 4, func.lit(4)).otherwise(col('factor')))\n",
    "    \n",
    "    factor3 = factor3.repartition(1)\n",
    "    factor3.write.format(\"parquet\") \\\n",
    "            .mode(\"overwrite\").save(factor_out_path)\n",
    "    \n",
    "    print(\"finish:\" + str(market))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
